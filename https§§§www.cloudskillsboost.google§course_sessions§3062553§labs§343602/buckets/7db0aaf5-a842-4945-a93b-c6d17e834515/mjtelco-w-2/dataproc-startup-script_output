+ run_with_logger --tag startup-script
+ local tag=
+ local pid=1131
+ [[ --tag == \-\-\t\a\g ]]
+ tag=startup-script
+ shift 2
+ [[ 0 -eq 0 ]]
+ exec
++ logger -s -t 'startup-script[1131]'
<13>Jun 28 16:02:23 startup-script[1131]: + update_spark_bq_connector
<13>Jun 28 16:02:23 startup-script[1131]: + local connector_version
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_spark_bq_connector_version
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_SPARK_BQ_CONNECTOR_VERSION attributes/SPARK_BQ_CONNECTOR_VERSION
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + connector_version=
<13>Jun 28 16:02:23 startup-script[1131]: + local connector_url
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_spark_bq_connector_url
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_SPARK_BQ_CONNECTOR_URL attributes/dataproc_spark_bq_connector_url
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + connector_url=
<13>Jun 28 16:02:23 startup-script[1131]: + local jar_name
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z '' ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -n '' ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -n '' ]]
<13>Jun 28 16:02:23 startup-script[1131]: + cd /tmp
<13>Jun 28 16:02:23 startup-script[1131]: + trap logstacktrace ERR
<13>Jun 28 16:02:23 startup-script[1131]: + loginfo 'Starting Dataproc startup script'
<13>Jun 28 16:02:23 startup-script[1131]: + echo 'Starting Dataproc startup script'
<13>Jun 28 16:02:23 startup-script[1131]: Starting Dataproc startup script
<13>Jun 28 16:02:23 startup-script[1131]: + set -a
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_project_id
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_PROJECT_ID ../project/project-id
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + PROJECT=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_dataproc_region
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_REGION attributes/dataproc-region
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + REGION=us-east1
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_zone
<13>Jun 28 16:02:23 startup-script[1131]: ++ local zone_uri
<13>Jun 28 16:02:23 startup-script[1131]: +++ get_dataproc_metadata DATAPROC_METADATA_ZONE zone
<13>Jun 28 16:02:23 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: ++ zone_uri=projects/746779145865/zones/us-east1-b
<13>Jun 28 16:02:23 startup-script[1131]: ++ echo us-east1-b
<13>Jun 28 16:02:23 startup-script[1131]: + ZONE=us-east1-b
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_bucket
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + CONFIGBUCKET=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_temp_bucket
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_TEMP_BUCKET attributes/dataproc-temp-bucket
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + TEMP_BUCKET=dataproc-temp-us-east1-746779145865-aaezsxz3
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_role
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + ROLE=Worker
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_cluster_name
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + CLUSTER_NAME=mjtelco
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_cluster_uuid
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_UUID attributes/dataproc-cluster-uuid
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + CLUSTER_UUID=7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_worker_count
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + WORKER_COUNT=5
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_master
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_metadata_master_additional
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + DATAPROC_MASTER_ADDITIONAL=
<13>Jun 28 16:02:23 startup-script[1131]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:23 startup-script[1131]: + MASTER_COUNT=1
<13>Jun 28 16:02:23 startup-script[1131]: ++ hostname -s
<13>Jun 28 16:02:23 startup-script[1131]: + MY_HOSTNAME=mjtelco-w-2
<13>Jun 28 16:02:23 startup-script[1131]: ++ hostname -f
<13>Jun 28 16:02:23 startup-script[1131]: + MY_FULL_HOSTNAME=mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:23 startup-script[1131]: ++ dnsdomainname
<13>Jun 28 16:02:23 startup-script[1131]: + DOMAIN=us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:23 startup-script[1131]: + DATAPROC_MASTER_FQDN=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:23 startup-script[1131]: ++ echo mjtelco-w-2
<13>Jun 28 16:02:23 startup-script[1131]: ++ sed -r 's/-([mw](-[0-9]*)?)$//'
<13>Jun 28 16:02:23 startup-script[1131]: + PREFIX=mjtelco
<13>Jun 28 16:02:23 startup-script[1131]: + DATAPROC_ETC_DIR=/etc/google-dataproc
<13>Jun 28 16:02:23 startup-script[1131]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Jun 28 16:02:23 startup-script[1131]: + UNINSTALL_TMP_DIR=/tmp/dataproc/uninstall
<13>Jun 28 16:02:23 startup-script[1131]: + UNINSTALL_PRE_ACTIVATE_TMP_DIR=/tmp/dataproc/uninstall-pre-activate
<13>Jun 28 16:02:23 startup-script[1131]: + KEYTAB_DIR=/etc/security/keytab
<13>Jun 28 16:02:23 startup-script[1131]: + CLUSTER_STAGING_FOLDER=gs://qwiklabs-gcp-03-e04e71dd72c2/google-cloud-dataproc-metainfo/7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:23 startup-script[1131]: + CLUSTER_TEMP_FOLDER=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:23 startup-script[1131]: + COMPONENT_SERVICES=(hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server)
<13>Jun 28 16:02:23 startup-script[1131]: + readonly COMPONENT_SERVICES
<13>Jun 28 16:02:23 startup-script[1131]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:23 startup-script[1131]: + set +a
<13>Jun 28 16:02:23 startup-script[1131]: + mkdir -p /tmp/dataproc
<13>Jun 28 16:02:23 startup-script[1131]: + mkdir -p /tmp/dataproc/commands
<13>Jun 28 16:02:23 startup-script[1131]: + mkdir -p /tmp/dataproc/components
<13>Jun 28 16:02:23 startup-script[1131]: + mkdir -p /tmp/dataproc/uninstall
<13>Jun 28 16:02:23 startup-script[1131]: + mkdir -p /tmp/dataproc/uninstall-pre-activate
<13>Jun 28 16:02:23 startup-script[1131]: + merge_java_properties /tmp/cluster/properties/dataproc.properties /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + local -r src=/tmp/cluster/properties/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + local -r dest=/etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + local -r 'header=\n# User-supplied properties.'
<13>Jun 28 16:02:23 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/dataproc.properties ]]
<13>Jun 28 16:02:23 startup-script[1131]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:23 startup-script[1131]: + cat /tmp/cluster/properties/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Jun 28 16:02:23 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/dataproc.properties.'
<13>Jun 28 16:02:23 startup-script[1131]: Merged /tmp/cluster/properties/dataproc.properties.
<13>Jun 28 16:02:23 startup-script[1131]: + merge_java_properties /etc/google-dataproc/dataproc.custom.properties /etc/google-dataproc/dataproc.properties '\n# Custom image supplied properties'
<13>Jun 28 16:02:23 startup-script[1131]: + local -r src=/etc/google-dataproc/dataproc.custom.properties
<13>Jun 28 16:02:23 startup-script[1131]: + local -r dest=/etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + local -r 'header=\n# Custom image supplied properties'
<13>Jun 28 16:02:23 startup-script[1131]: + [[ ! -f /etc/google-dataproc/dataproc.custom.properties ]]
<13>Jun 28 16:02:23 startup-script[1131]: + loginfo 'Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.'
<13>Jun 28 16:02:23 startup-script[1131]: + echo 'Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.'
<13>Jun 28 16:02:23 startup-script[1131]: Skipping merging /etc/google-dataproc/dataproc.custom.properties, file does not exist.
<13>Jun 28 16:02:23 startup-script[1131]: + return 0
<13>Jun 28 16:02:23 startup-script[1131]: + determine_selected_components
<13>Jun 28 16:02:23 startup-script[1131]: + local -a default_components
<13>Jun 28 16:02:23 startup-script[1131]: + default_components=($(get_default_components))
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_default_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property_or_default dataproc.components.default 'hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom'
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE=
<13>Jun 28 16:02:23 startup-script[1131]: + add_default_components hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom
<13>Jun 28 16:02:23 startup-script[1131]: + default_components=("$@")
<13>Jun 28 16:02:23 startup-script[1131]: + local default_components
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected hdfs
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=hdfs
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *hdfs* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component hdfs
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=hdfs
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *hdfs* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z '' ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE=hdfs
<13>Jun 28 16:02:23 startup-script[1131]: + echo dataproc.components.activate=hdfs
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected yarn
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=yarn
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *yarn* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component yarn
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=yarn
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs == *yarn* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' yarn'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected mapreduce
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=mapreduce
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *mapreduce* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component mapreduce
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=mapreduce
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn == *mapreduce* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' mapreduce'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected mysql
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=mysql
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *mysql* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component mysql
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=mysql
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce == *mysql* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn mapreduce ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' mysql'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected pig
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=pig
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *pig* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component pig
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=pig
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql == *pig* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn mapreduce mysql ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' pig'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql pig/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected tez
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=tez
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *tez* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component tez
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=tez
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig == *tez* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn mapreduce mysql pig ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' tez'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql pig tez/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected hive-metastore
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=hive-metastore
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *hive-metastore* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component hive-metastore
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=hive-metastore
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez == *hive-metastore* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn mapreduce mysql pig tez ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' hive-metastore'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql pig tez hive-metastore/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected hive-server2
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=hive-server2
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *hive-server2* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component hive-server2
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=hive-server2
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore == *hive-server2* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn mapreduce mysql pig tez hive-metastore ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' hive-server2'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected spark
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=spark
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *spark* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component spark
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=spark
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 == *spark* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' spark'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + for component in "${default_components[@]}"
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_explicitly_unselected earlyoom
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=earlyoom
<13>Jun 28 16:02:23 startup-script[1131]: + local deactivated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.deactivate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + deactivated_components=
<13>Jun 28 16:02:23 startup-script[1131]: + [[ '' == *earlyoom* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component earlyoom
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=earlyoom
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark == *earlyoom* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' earlyoom'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:23 startup-script[1131]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:23 startup-script[1131]: + set +x
<13>Jun 28 16:02:23 startup-script[1131]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:23 startup-script[1131]: + case ${compare_versions_result} in
<13>Jun 28 16:02:23 startup-script[1131]: + return 0
<13>Jun 28 16:02:23 startup-script[1131]: + add_optional_component miniconda3
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=miniconda3
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom == *miniconda3* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + [[ -z hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom ]]
<13>Jun 28 16:02:23 startup-script[1131]: + OPTIONAL_COMPONENTS_VALUE+=' miniconda3'
<13>Jun 28 16:02:23 startup-script[1131]: + sed -i 's/dataproc.components.activate=.*/dataproc.components.activate=hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3/g' /etc/google-dataproc/dataproc.properties
<13>Jun 28 16:02:23 startup-script[1131]: + (( 1 > 1 ))
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_selected kafka-server
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=kafka-server
<13>Jun 28 16:02:23 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kafka-server* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + local is_caching_enabled
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property_or_default dataproc.cluster.caching.enabled false
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + is_caching_enabled=false
<13>Jun 28 16:02:23 startup-script[1131]: + local -r is_caching_enabled
<13>Jun 28 16:02:23 startup-script[1131]: + [[ false == \t\r\u\e ]]
<13>Jun 28 16:02:23 startup-script[1131]: + SELECTED_COMPONENTS=(${OPTIONAL_COMPONENTS_VALUE})
<13>Jun 28 16:02:23 startup-script[1131]: + is_component_selected kerberos
<13>Jun 28 16:02:23 startup-script[1131]: + local -r component=kerberos
<13>Jun 28 16:02:23 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:23 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:23 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:23 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:23 startup-script[1131]: + HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:23 startup-script[1131]: + HBASE_CONF_DIR=/etc/hbase/conf
<13>Jun 28 16:02:23 startup-script[1131]: + SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:23 startup-script[1131]: + HADOOP_2_PORTS=(50010 50020 50070 50090)
<13>Jun 28 16:02:23 startup-script[1131]: + export_hcfs_root_uri
<13>Jun 28 16:02:23 startup-script[1131]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:23 startup-script[1131]: ++ get_property_in_xml /tmp/cluster/properties/core.xml fs.defaultFS
<13>Jun 28 16:02:23 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:24 startup-script[1131]: + HCFS_ROOT_URI=
<13>Jun 28 16:02:24 startup-script[1131]: + [[ -z '' ]]
<13>Jun 28 16:02:24 startup-script[1131]: + is_component_selected hdfs
<13>Jun 28 16:02:24 startup-script[1131]: + local -r component=hdfs
<13>Jun 28 16:02:24 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:24 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:24 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:24 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:24 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:24 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:24 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *hdfs* ]]
<13>Jun 28 16:02:24 startup-script[1131]: + (( 1 > 1 ))
<13>Jun 28 16:02:24 startup-script[1131]: + HCFS_ROOT_URI=hdfs://mjtelco-m
<13>Jun 28 16:02:24 startup-script[1131]: + readonly HCFS_ROOT_URI
<13>Jun 28 16:02:24 startup-script[1131]: + export HCFS_ROOT_URI
<13>Jun 28 16:02:24 startup-script[1131]: + [[ Worker == \M\a\s\t\e\r ]]
<13>Jun 28 16:02:24 startup-script[1131]: + SERVICES=()
<13>Jun 28 16:02:24 startup-script[1131]: + ARTIFACTS_TO_UNINSTALL=(${DATAPROC_MASTER_SERVICES} ${DATAPROC_MASTER_EXCLUSIVE_SERVICES} ${DATAPROC_MASTER_STANDALONE_SERVICES} ${DATAPROC_MASTER_HA_SERVICES})
<13>Jun 28 16:02:24 startup-script[1131]: + enable_worker_services
<13>Jun 28 16:02:24 startup-script[1131]: + for service in ${DATAPROC_WORKER_SERVICES}
<13>Jun 28 16:02:24 startup-script[1131]: + in_array hadoop-hdfs-datanode DATAPROC_COMPONENTS
<13>Jun 28 16:02:24 startup-script[1131]: + local -r value=hadoop-hdfs-datanode
<13>Jun 28 16:02:24 startup-script[1131]: + local -n values=DATAPROC_COMPONENTS
<13>Jun 28 16:02:24 startup-script[1131]: + [[ !    == *\ \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e\ * ]]
<13>Jun 28 16:02:24 startup-script[1131]: + return 1
<13>Jun 28 16:02:24 startup-script[1131]: + in_array hadoop-hdfs-datanode COMPONENT_SERVICES
<13>Jun 28 16:02:24 startup-script[1131]: + local -r value=hadoop-hdfs-datanode
<13>Jun 28 16:02:24 startup-script[1131]: + local -n values=COMPONENT_SERVICES
<13>Jun 28 16:02:24 startup-script[1131]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e\ * ]]
<13>Jun 28 16:02:24 startup-script[1131]: + continue
<13>Jun 28 16:02:24 startup-script[1131]: + for service in ${DATAPROC_WORKER_SERVICES}
<13>Jun 28 16:02:24 startup-script[1131]: + in_array hadoop-yarn-nodemanager DATAPROC_COMPONENTS
<13>Jun 28 16:02:24 startup-script[1131]: + local -r value=hadoop-yarn-nodemanager
<13>Jun 28 16:02:24 startup-script[1131]: + local -n values=DATAPROC_COMPONENTS
<13>Jun 28 16:02:24 startup-script[1131]: + [[ !    == *\ \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r\ * ]]
<13>Jun 28 16:02:24 startup-script[1131]: + return 1
<13>Jun 28 16:02:24 startup-script[1131]: + in_array hadoop-yarn-nodemanager COMPONENT_SERVICES
<13>Jun 28 16:02:24 startup-script[1131]: + local -r value=hadoop-yarn-nodemanager
<13>Jun 28 16:02:24 startup-script[1131]: + local -n values=COMPONENT_SERVICES
<13>Jun 28 16:02:24 startup-script[1131]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r\ * ]]
<13>Jun 28 16:02:24 startup-script[1131]: + return 1
<13>Jun 28 16:02:24 startup-script[1131]: + enable_service hadoop-yarn-nodemanager
<13>Jun 28 16:02:24 startup-script[1131]: + local -r service=hadoop-yarn-nodemanager
<13>Jun 28 16:02:24 startup-script[1131]: + local -r unit=hadoop-yarn-nodemanager.service
<13>Jun 28 16:02:24 startup-script[1131]: + retry_constant_short systemctl enable hadoop-yarn-nodemanager.service
<13>Jun 28 16:02:24 startup-script[1131]: + retry_constant_custom 30 1 systemctl enable hadoop-yarn-nodemanager.service
<13>Jun 28 16:02:24 startup-script[1131]: + local -r max_retry_time=30
<13>Jun 28 16:02:24 startup-script[1131]: + local -r retry_delay=1
<13>Jun 28 16:02:24 startup-script[1131]: + cmd=("${@:3}")
<13>Jun 28 16:02:24 startup-script[1131]: + local -r cmd
<13>Jun 28 16:02:24 startup-script[1131]: + local -r max_retries=30
<13>Jun 28 16:02:24 startup-script[1131]: + local reenable_x=false
<13>Jun 28 16:02:24 startup-script[1131]: + [[ -o xtrace ]]
<13>Jun 28 16:02:24 startup-script[1131]: + set +x
<13>Jun 28 16:02:24 startup-script[1131]: About to run 'systemctl enable hadoop-yarn-nodemanager.service' with retries...
<13>Jun 28 16:02:24 startup-script[1131]: hadoop-yarn-nodemanager.service is not a native service, redirecting to systemd-sysv-install.
<13>Jun 28 16:02:24 startup-script[1131]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-yarn-nodemanager
<13>Jun 28 16:02:24 startup-script[1131]: 'systemctl enable hadoop-yarn-nodemanager.service' succeeded after 1 execution(s).
<13>Jun 28 16:02:24 startup-script[1131]: + return 0
<13>Jun 28 16:02:24 startup-script[1131]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Jun 28 16:02:24 startup-script[1131]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Jun 28 16:02:24 startup-script[1131]: ++ dirname /etc/systemd/system/common/restart.conf
<13>Jun 28 16:02:24 startup-script[1131]: + mkdir -p /etc/systemd/system/common
<13>Jun 28 16:02:24 startup-script[1131]: + cat
<13>Jun 28 16:02:24 startup-script[1131]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Jun 28 16:02:24 startup-script[1131]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Jun 28 16:02:24 startup-script[1131]: ++ dirname /etc/systemd/system/common/worker-restart.conf
<13>Jun 28 16:02:24 startup-script[1131]: + mkdir -p /etc/systemd/system/common
<13>Jun 28 16:02:24 startup-script[1131]: + cat
<13>Jun 28 16:02:24 startup-script[1131]: + local -r drop_in_dir=/etc/systemd/system/hadoop-yarn-nodemanager.service.d
<13>Jun 28 16:02:24 startup-script[1131]: + mkdir -p /etc/systemd/system/hadoop-yarn-nodemanager.service.d
<13>Jun 28 16:02:24 startup-script[1131]: + local props
<13>Jun 28 16:02:24 startup-script[1131]: ++ retry_constant_short systemctl show hadoop-yarn-nodemanager.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:24 startup-script[1131]: ++ retry_constant_custom 30 1 systemctl show hadoop-yarn-nodemanager.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:24 startup-script[1131]: ++ local -r max_retry_time=30
<13>Jun 28 16:02:24 startup-script[1131]: ++ local -r retry_delay=1
<13>Jun 28 16:02:24 startup-script[1131]: ++ cmd=("${@:3}")
<13>Jun 28 16:02:24 startup-script[1131]: ++ local -r cmd
<13>Jun 28 16:02:24 startup-script[1131]: ++ local -r max_retries=30
<13>Jun 28 16:02:24 startup-script[1131]: ++ local reenable_x=false
<13>Jun 28 16:02:24 startup-script[1131]: ++ [[ -o xtrace ]]
<13>Jun 28 16:02:24 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:24 startup-script[1131]: About to run 'systemctl show hadoop-yarn-nodemanager.service -p Restart,RemainAfterExit' with retries...
<13>Jun 28 16:02:24 startup-script[1131]: 'systemctl show hadoop-yarn-nodemanager.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Jun 28 16:02:24 startup-script[1131]: ++ return 0
<13>Jun 28 16:02:24 startup-script[1131]: + props='Restart=no
<13>Jun 28 16:02:24 startup-script[1131]: RemainAfterExit=no'
<13>Jun 28 16:02:24 startup-script[1131]: + [[ hadoop-yarn-nodemanager != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:24 startup-script[1131]: + [[ hadoop-yarn-nodemanager != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:24 startup-script[1131]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Jun 28 16:02:24 startup-script[1131]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Jun 28 16:02:24 startup-script[1131]: ++ dirname /etc/systemd/system/common/agent-gate.conf
<13>Jun 28 16:02:24 startup-script[1131]: + mkdir -p /etc/systemd/system/common
<13>Jun 28 16:02:24 startup-script[1131]: + cat
<13>Jun 28 16:02:24 startup-script[1131]: + [[ hadoop-yarn-nodemanager == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:24 startup-script[1131]: + [[ hadoop-yarn-nodemanager == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:24 startup-script[1131]: ++ get_metadata_worker_count
<13>Jun 28 16:02:24 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Jun 28 16:02:24 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:24 startup-script[1131]: + local -r worker_count=5
<13>Jun 28 16:02:24 startup-script[1131]: + [[ 5 != 0 ]]
<13>Jun 28 16:02:24 startup-script[1131]: + ln -s -f /etc/systemd/system/common/agent-gate.conf /etc/systemd/system/hadoop-yarn-nodemanager.service.d
<13>Jun 28 16:02:24 startup-script[1131]: + ln -s -f /etc/systemd/system/common/worker-restart.conf /etc/systemd/system/hadoop-yarn-nodemanager.service.d
<13>Jun 28 16:02:24 startup-script[1131]: + loginfo 'Generating helper scripts'
<13>Jun 28 16:02:24 startup-script[1131]: + echo 'Generating helper scripts'
<13>Jun 28 16:02:24 startup-script[1131]: Generating helper scripts
<13>Jun 28 16:02:24 startup-script[1131]: + cat
<13>Jun 28 16:02:24 startup-script[1131]: ++ (( i = 0 ))
<13>Jun 28 16:02:24 startup-script[1131]: ++ (( i < 1 ))
<13>Jun 28 16:02:24 startup-script[1131]: ++ echo MASTER_HOSTNAME_0=mjtelco-m
<13>Jun 28 16:02:24 startup-script[1131]: ++ (( i++ ))
<13>Jun 28 16:02:24 startup-script[1131]: ++ (( i < 1 ))
<13>Jun 28 16:02:24 startup-script[1131]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:24 startup-script[1131]: + cat
<13>Jun 28 16:02:24 startup-script[1131]: ++ cat /usr/local/share/google/dataproc/bdutil/configure_keys.sh /usr/local/share/google/dataproc/bdutil/configure_hadoop.sh /usr/local/share/google/dataproc/bdutil/configure_connectors.sh /usr/local/share/google/dataproc/bdutil/configure_docker.sh /usr/local/share/google/dataproc/bdutil/configure_metadata_proxy.sh
<13>Jun 28 16:02:24 startup-script[1131]: + cp -r /usr/local/share/google/dataproc/bdutil/conf/bq-mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/capacity-scheduler-template.xml /usr/local/share/google/dataproc/bdutil/conf/collectd /usr/local/share/google/dataproc/bdutil/conf/collectd_default_filtered_write.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hdfs_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hivemetastore_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_hiveserver2_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_load_jmx_plugin.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_processes_default_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_shs_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_spark_default_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_spark_yarn_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_w
<13>Jun 28 16:02:24 startup-script[1131]: ithout_monitoring_agent_defaults.conf /usr/local/share/google/dataproc/bdutil/conf/collectd_yarn_jmx_metrics.conf /usr/local/share/google/dataproc/bdutil/conf/core-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/core-template.xml /usr/local/share/google/dataproc/bdutil/conf/distcp-template.xml /usr/local/share/google/dataproc/bdutil/conf/gcs-core-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-ha-template.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hdfs-template.xml /usr/local/share/google/dataproc/bdutil/conf/hive-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/hive-template.xml /usr/local/share/google/dataproc/bdutil/conf/knox /usr/local/share/google/dataproc/bdutil/conf/mapred-template.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-ha-template.xml /usr/local/shar
<13>Jun 28 16:02:24 startup-script[1131]: e/google/dataproc/bdutil/conf/yarn-simplification-ha-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-template.xml /tmp
<13>Jun 28 16:02:25 startup-script[1131]: + cp /usr/local/share/google/dataproc/bdutil/configure_mrv2_mem.py /tmp
<13>Jun 28 16:02:25 startup-script[1131]: + chmod +x configure_mrv2_mem.py
<13>Jun 28 16:02:25 startup-script[1131]: + loginfo 'Running helper scripts'
<13>Jun 28 16:02:25 startup-script[1131]: + echo 'Running helper scripts'
<13>Jun 28 16:02:25 startup-script[1131]: Running helper scripts
<13>Jun 28 16:02:25 startup-script[1131]: ++ get_dataproc_property dataproc.localssd.mount.enable
<13>Jun 28 16:02:25 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:25 startup-script[1131]: + MOUNT_DISKS_ENABLED=
<13>Jun 28 16:02:25 startup-script[1131]: + [[ '' == \f\a\l\s\e ]]
<13>Jun 28 16:02:25 startup-script[1131]: + DATAPROC_MOUNT_SERVICE_FILE=/usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Jun 28 16:02:25 startup-script[1131]: + cat
<13>Jun 28 16:02:25 startup-script[1131]: + chmod +x /usr/local/share/google/dataproc/bdutil/mount_disks.sh
<13>Jun 28 16:02:25 startup-script[1131]: + chmod 644 /usr/lib/systemd/system/google-dataproc-disk-mount.service
<13>Jun 28 16:02:25 startup-script[1131]: + systemctl enable google-dataproc-disk-mount
<13>Jun 28 16:02:25 startup-script[1131]: Created symlink /etc/systemd/system/multi-user.target.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Jun 28 16:02:25 startup-script[1131]: Created symlink /etc/systemd/system/hadoop-hdfs-namenode.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Jun 28 16:02:25 startup-script[1131]: Created symlink /etc/systemd/system/hadoop-hdfs-datanode.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Jun 28 16:02:25 startup-script[1131]: Created symlink /etc/systemd/system/hadoop-yarn-resourcemanager.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Jun 28 16:02:25 startup-script[1131]: Created symlink /etc/systemd/system/hadoop-yarn-nodemanager.service.wants/google-dataproc-disk-mount.service → /lib/systemd/system/google-dataproc-disk-mount.service.
<13>Jun 28 16:02:25 startup-script[1131]: + systemctl start google-dataproc-disk-mount
<13>Jun 28 16:02:25 startup-script[1131]: + bash configuration_script.sh
<13>Jun 28 16:02:25 startup-script[1131]: + source /usr/local/share/google/dataproc/dataproc_env.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ CLUSTER_NAME=mjtelco
<13>Jun 28 16:02:25 startup-script[1131]: ++ CLUSTER_UUID=7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:25 startup-script[1131]: ++ CONFIGBUCKET=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:25 startup-script[1131]: ++ TEMP_BUCKET=dataproc-temp-us-east1-746779145865-aaezsxz3
<13>Jun 28 16:02:25 startup-script[1131]: ++ HCFS_ROOT_URI=hdfs://mjtelco-m
<13>Jun 28 16:02:25 startup-script[1131]: ++ MASTER_HOSTNAME_0=mjtelco-m
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_MASTER_FQDN=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:25 startup-script[1131]: ++ MASTER_HOSTNAMES=(mjtelco-m)
<13>Jun 28 16:02:25 startup-script[1131]: ++ NUM_MASTERS=1
<13>Jun 28 16:02:25 startup-script[1131]: ++ NUM_WORKERS=5
<13>Jun 28 16:02:25 startup-script[1131]: ++ PREFIX=mjtelco
<13>Jun 28 16:02:25 startup-script[1131]: ++ PROJECT=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:25 startup-script[1131]: ++ ROLE=Worker
<13>Jun 28 16:02:25 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:25 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:25 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:25 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:25 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:25 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:25 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:25 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:25 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:25 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:25 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:25 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:25 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:25 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:25 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:25 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:25 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:25 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:25 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:25 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:25 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:25 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:25 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:25 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:25 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:25 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:25 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:25 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:25 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:25 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:25 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:25 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:25 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:25 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:25 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:25 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:25 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:25 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:25 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:25 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:25 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:25 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:25 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:25 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:25 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:25 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:25 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:25 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:25 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:25 startup-script[1131]: + set +a
<13>Jun 28 16:02:25 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:25 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:25 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:25 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:25 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:25 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:25 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:25 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:25 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:25 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:25 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:25 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:25 startup-script[1131]: + readonly METADATA_HOST=metadata.google.internal
<13>Jun 28 16:02:25 startup-script[1131]: + METADATA_HOST=metadata.google.internal
<13>Jun 28 16:02:25 startup-script[1131]: + readonly INSTANCE_METADATA_PATH=computeMetadata/v1/instance
<13>Jun 28 16:02:25 startup-script[1131]: + INSTANCE_METADATA_PATH=computeMetadata/v1/instance
<13>Jun 28 16:02:25 startup-script[1131]: + readonly IDENTITY_KEY=service-accounts/default/identity
<13>Jun 28 16:02:25 startup-script[1131]: + IDENTITY_KEY=service-accounts/default/identity
<13>Jun 28 16:02:25 startup-script[1131]: + readonly GUEST_ATTRIBUTE_PATH=computeMetadata/v1/instance/guest-attributes/dataproc-signed-keys
<13>Jun 28 16:02:25 startup-script[1131]: + GUEST_ATTRIBUTE_PATH=computeMetadata/v1/instance/guest-attributes/dataproc-signed-keys
<13>Jun 28 16:02:25 startup-script[1131]: + readonly SERIAL_DEVICE=/dev/ttyS3
<13>Jun 28 16:02:25 startup-script[1131]: + SERIAL_DEVICE=/dev/ttyS3
<13>Jun 28 16:02:25 startup-script[1131]: + readonly GENERATE_KEYS_SCRIPT=/usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Jun 28 16:02:25 startup-script[1131]: + GENERATE_KEYS_SCRIPT=/usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Jun 28 16:02:25 startup-script[1131]: + readonly TINKEY_BINARY_PATH=/usr/local/bin/tinkey
<13>Jun 28 16:02:25 startup-script[1131]: + TINKEY_BINARY_PATH=/usr/local/bin/tinkey
<13>Jun 28 16:02:25 startup-script[1131]: ++ get_dataproc_property dataproc.encryption.keygen.enabled
<13>Jun 28 16:02:25 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:25 startup-script[1131]: + cluster_keys_enabled=
<13>Jun 28 16:02:25 startup-script[1131]: ++ get_dataproc_property_or_default dataproc.encryption.keygen.rotation_hours 6
<13>Jun 28 16:02:25 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:25 startup-script[1131]: + cluster_keys_rotation_hours=6
<13>Jun 28 16:02:25 startup-script[1131]: + cat
<13>Jun 28 16:02:25 startup-script[1131]: + chmod u+rwx /usr/bin/google-dataproc-generate-cluster-keys.sh
<13>Jun 28 16:02:25 startup-script[1131]: + cat
<13>Jun 28 16:02:25 startup-script[1131]: + cat
<13>Jun 28 16:02:25 startup-script[1131]: + [[ '' == \t\r\u\e ]]
<13>Jun 28 16:02:25 startup-script[1131]: + rm /etc/udev/rules.d/80-ttyS3.rules
<13>Jun 28 16:02:25 startup-script[1131]: + udevadm trigger
<13>Jun 28 16:02:25 startup-script[1131]: + chown root:dialout /dev/ttyS3
<13>Jun 28 16:02:25 startup-script[1131]: + set -e
<13>Jun 28 16:02:25 startup-script[1131]: + loginfo 'Running configure_hadoop.sh'
<13>Jun 28 16:02:25 startup-script[1131]: + echo 'Running configure_hadoop.sh'
<13>Jun 28 16:02:25 startup-script[1131]: Running configure_hadoop.sh
<13>Jun 28 16:02:25 startup-script[1131]: + export HADOOP_TMP_DIR=/hadoop/tmp
<13>Jun 28 16:02:25 startup-script[1131]: + HADOOP_TMP_DIR=/hadoop/tmp
<13>Jun 28 16:02:25 startup-script[1131]: + mkdir -p /hadoop/tmp
<13>Jun 28 16:02:25 startup-script[1131]: + export DEFAULT_NUM_MAPS=100
<13>Jun 28 16:02:25 startup-script[1131]: + DEFAULT_NUM_MAPS=100
<13>Jun 28 16:02:25 startup-script[1131]: + export DEFAULT_NUM_REDUCES=40
<13>Jun 28 16:02:25 startup-script[1131]: + DEFAULT_NUM_REDUCES=40
<13>Jun 28 16:02:25 startup-script[1131]: ++ grep -c processor /proc/cpuinfo
<13>Jun 28 16:02:25 startup-script[1131]: + NUM_CORES=2
<13>Jun 28 16:02:25 startup-script[1131]: + export NUM_CORES
<13>Jun 28 16:02:25 startup-script[1131]: ++ python -c 'print(int(2 // 1.0))'
<13>Jun 28 16:02:25 startup-script[1131]: + MAP_SLOTS=2
<13>Jun 28 16:02:25 startup-script[1131]: + export MAP_SLOTS
<13>Jun 28 16:02:25 startup-script[1131]: ++ python -c 'print(int(2 // 2.0))'
<13>Jun 28 16:02:25 startup-script[1131]: + REDUCE_SLOTS=1
<13>Jun 28 16:02:25 startup-script[1131]: + export REDUCE_SLOTS
<13>Jun 28 16:02:25 startup-script[1131]: ++ free -m
<13>Jun 28 16:02:25 startup-script[1131]: ++ awk '/^Mem:/{print $2}'
<13>Jun 28 16:02:25 startup-script[1131]: + TOTAL_MEM=7452
<13>Jun 28 16:02:25 startup-script[1131]: ++ python -c 'print(int(7452 * 0.4))'
<13>Jun 28 16:02:25 startup-script[1131]: + HADOOP_MR_MASTER_MEM_MB=2980
<13>Jun 28 16:02:25 startup-script[1131]: + [[ -x configure_mrv2_mem.py ]]
<13>Jun 28 16:02:25 startup-script[1131]: ++ mktemp /tmp/mrv2_XXX_tmp_env.sh
<13>Jun 28 16:02:25 startup-script[1131]: + TEMP_ENV_FILE=/tmp/mrv2_8Ww_tmp_env.sh
<13>Jun 28 16:02:25 startup-script[1131]: + ./configure_mrv2_mem.py --output_file /tmp/mrv2_8Ww_tmp_env.sh --total_memory 7452 --available_memory_ratio 0.8 --total_cores 2 --cores_per_map 1.0 --cores_per_reduce 2.0 --cores_per_app_master 2.0
<13>Jun 28 16:02:25 startup-script[1131]: + source /tmp/mrv2_8Ww_tmp_env.sh
<13>Jun 28 16:02:25 startup-script[1131]: ++ export YARN_MIN_MEM_MB=512
<13>Jun 28 16:02:25 startup-script[1131]: ++ YARN_MIN_MEM_MB=512
<13>Jun 28 16:02:25 startup-script[1131]: ++ export YARN_MAX_MEM_MB=5632
<13>Jun 28 16:02:25 startup-script[1131]: ++ YARN_MAX_MEM_MB=5632
<13>Jun 28 16:02:25 startup-script[1131]: ++ export NODEMANAGER_MEM_MB=5632
<13>Jun 28 16:02:25 startup-script[1131]: ++ NODEMANAGER_MEM_MB=5632
<13>Jun 28 16:02:25 startup-script[1131]: ++ export APP_MASTER_MEM_MB=5632
<13>Jun 28 16:02:25 startup-script[1131]: ++ APP_MASTER_MEM_MB=5632
<13>Jun 28 16:02:25 startup-script[1131]: ++ export CORES_PER_APP_MASTER_ROUNDED=2
<13>Jun 28 16:02:25 startup-script[1131]: ++ CORES_PER_APP_MASTER_ROUNDED=2
<13>Jun 28 16:02:25 startup-script[1131]: ++ export APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Jun 28 16:02:25 startup-script[1131]: ++ APP_MASTER_JAVA_OPTS=-Xmx4505m
<13>Jun 28 16:02:25 startup-script[1131]: ++ export MAP_MEM_MB=2560
<13>Jun 28 16:02:25 startup-script[1131]: ++ MAP_MEM_MB=2560
<13>Jun 28 16:02:25 startup-script[1131]: ++ export CORES_PER_MAP_ROUNDED=1
<13>Jun 28 16:02:25 startup-script[1131]: ++ CORES_PER_MAP_ROUNDED=1
<13>Jun 28 16:02:25 startup-script[1131]: ++ export MAP_JAVA_OPTS=-Xmx2048m
<13>Jun 28 16:02:25 startup-script[1131]: ++ MAP_JAVA_OPTS=-Xmx2048m
<13>Jun 28 16:02:25 startup-script[1131]: ++ export REDUCE_MEM_MB=5632
<13>Jun 28 16:02:25 startup-script[1131]: ++ REDUCE_MEM_MB=5632
<13>Jun 28 16:02:25 startup-script[1131]: ++ export CORES_PER_REDUCE_ROUNDED=2
<13>Jun 28 16:02:25 startup-script[1131]: ++ CORES_PER_REDUCE_ROUNDED=2
<13>Jun 28 16:02:25 startup-script[1131]: ++ export REDUCE_JAVA_OPTS=-Xmx4505m
<13>Jun 28 16:02:25 startup-script[1131]: ++ REDUCE_JAVA_OPTS=-Xmx4505m
<13>Jun 28 16:02:25 startup-script[1131]: ++ python -c 'print(int(7452 / 4))'
<13>Jun 28 16:02:25 startup-script[1131]: + HADOOP_CLIENT_MEM_MB=1863
<13>Jun 28 16:02:25 startup-script[1131]: + cat
<13>Jun 28 16:02:25 startup-script[1131]: + is_version_at_least 2.0 2.1
<13>Jun 28 16:02:25 startup-script[1131]: + set +x
<13>Jun 28 16:02:25 startup-script[1131]: Comparing if version 2.0 is at least version 2.1 
<13>Jun 28 16:02:25 startup-script[1131]: + case ${compare_versions_result} in
<13>Jun 28 16:02:25 startup-script[1131]: + return 1
<13>Jun 28 16:02:25 startup-script[1131]: + GC_LOG_OPTS='-XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails'
<13>Jun 28 16:02:25 startup-script[1131]: + cat
<13>Jun 28 16:02:25 startup-script[1131]: + cat
<13>Jun 28 16:02:25 startup-script[1131]: + DATA_DIRS_ARRAY=($(get_data_dirs))
<13>Jun 28 16:02:25 startup-script[1131]: ++ get_data_dirs
<13>Jun 28 16:02:25 startup-script[1131]: ++ local -a mount_points
<13>Jun 28 16:02:25 startup-script[1131]: ++ mapfile -t mount_points
<13>Jun 28 16:02:25 startup-script[1131]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Jun 28 16:02:25 startup-script[1131]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Jun 28 16:02:25 startup-script[1131]: +++ true
<13>Jun 28 16:02:25 startup-script[1131]: ++ (( 0 ))
<13>Jun 28 16:02:25 startup-script[1131]: ++ echo /
<13>Jun 28 16:02:25 startup-script[1131]: ++ return
<13>Jun 28 16:02:25 startup-script[1131]: + MAPRED_DIRS=/hadoop/mapred
<13>Jun 28 16:02:25 startup-script[1131]: + MAPRED_DIRS_ARRAY=(${MAPRED_DIRS})
<13>Jun 28 16:02:25 startup-script[1131]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Jun 28 16:02:25 startup-script[1131]: + MAPRED_LOCAL_DIRS_ARRAY=(${MAPRED_LOCAL_DIRS})
<13>Jun 28 16:02:25 startup-script[1131]: + YARN_DIRS=/hadoop/yarn
<13>Jun 28 16:02:25 startup-script[1131]: + YARN_DIRS_ARRAY=(${YARN_DIRS})
<13>Jun 28 16:02:25 startup-script[1131]: ++ get_yarn_nm_local_dirs
<13>Jun 28 16:02:25 startup-script[1131]: ++ data_dirs=($(get_data_dirs))
<13>Jun 28 16:02:25 startup-script[1131]: +++ get_data_dirs
<13>Jun 28 16:02:25 startup-script[1131]: +++ local -a mount_points
<13>Jun 28 16:02:25 startup-script[1131]: +++ mapfile -t mount_points
<13>Jun 28 16:02:25 startup-script[1131]: ++++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Jun 28 16:02:25 startup-script[1131]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Jun 28 16:02:25 startup-script[1131]: ++++ true
<13>Jun 28 16:02:25 startup-script[1131]: +++ (( 0 ))
<13>Jun 28 16:02:25 startup-script[1131]: +++ echo /
<13>Jun 28 16:02:25 startup-script[1131]: +++ return
<13>Jun 28 16:02:25 startup-script[1131]: ++ local -r data_dirs
<13>Jun 28 16:02:25 startup-script[1131]: ++ echo /hadoop/yarn/nm-local-dir
<13>Jun 28 16:02:25 startup-script[1131]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Jun 28 16:02:25 startup-script[1131]: + NODEMANAGER_LOCAL_DIRS_ARRAY=(${NODEMANAGER_LOCAL_DIRS})
<13>Jun 28 16:02:25 startup-script[1131]: + mkdir -p /hadoop/mapred/local /hadoop/yarn/nm-local-dir
<13>Jun 28 16:02:25 startup-script[1131]: + chgrp hadoop -L -R /hadoop /hadoop/tmp
<13>Jun 28 16:02:25 startup-script[1131]: + chown -L -R mapred:hadoop /hadoop/mapred
<13>Jun 28 16:02:26 startup-script[1131]: + chown -L -R yarn:hadoop /hadoop/yarn
<13>Jun 28 16:02:26 startup-script[1131]: + chmod g+rwx -R /hadoop /hadoop/mapred /hadoop/yarn
<13>Jun 28 16:02:26 startup-script[1131]: + chmod 777 -R /hadoop/tmp
<13>Jun 28 16:02:26 startup-script[1131]: + export MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Jun 28 16:02:26 startup-script[1131]: + MAPRED_LOCAL_DIRS=/hadoop/mapred/local
<13>Jun 28 16:02:26 startup-script[1131]: + export NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Jun 28 16:02:26 startup-script[1131]: + NODEMANAGER_LOCAL_DIRS=/hadoop/yarn/nm-local-dir
<13>Jun 28 16:02:26 startup-script[1131]: + YARN_ENV_FILE=/etc/hadoop/conf/yarn-env.sh
<13>Jun 28 16:02:26 startup-script[1131]: + [[ -f /etc/hadoop/conf/yarn-env.sh ]]
<13>Jun 28 16:02:26 startup-script[1131]: + cat
<13>Jun 28 16:02:26 startup-script[1131]: + [[ 1 -gt 1 ]]
<13>Jun 28 16:02:26 startup-script[1131]: + readonly CORE_TEMPLATE=core-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + CORE_TEMPLATE=core-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + readonly YARN_TEMPLATE=yarn-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + YARN_TEMPLATE=yarn-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + merge_hadoop_configurations core-site.xml core-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r config=core-site.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r template=core-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file core-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:26 startup-script[1131]: + merge_hadoop_configurations mapred-site.xml mapred-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r config=mapred-site.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r template=mapred-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file mapred-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:26 startup-script[1131]: + merge_hadoop_configurations yarn-site.xml yarn-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r template=yarn-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file yarn-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:26 startup-script[1131]: + merge_hadoop_configurations capacity-scheduler.xml capacity-scheduler-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r config=capacity-scheduler.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r template=capacity-scheduler-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file capacity-scheduler-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:26 startup-script[1131]: + merge_hadoop_configurations distcp-default.xml distcp-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r config=distcp-default.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r template=distcp-template.xml
<13>Jun 28 16:02:26 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file distcp-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:26 startup-script[1131]: + set_hadoop_property mapred-site.xml mapreduce.application.classpath '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     /usr/local/share/google/dataproc/lib/*'
<13>Jun 28 16:02:26 startup-script[1131]: + local -r config=mapred-site.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r property_name=mapreduce.application.classpath
<13>Jun 28 16:02:26 startup-script[1131]: + local -r 'property_value=$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     /usr/local/share/google/dataproc/lib/*'
<13>Jun 28 16:02:26 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.application.classpath --value '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     /usr/local/share/google/dataproc/lib/*' --clobber
<13>Jun 28 16:02:26 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.application.classpath '$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     /usr/local/share/google/dataproc/lib/*'
<13>Jun 28 16:02:26 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:26 startup-script[1131]: + local -r property_name=yarn.application.classpath
<13>Jun 28 16:02:26 startup-script[1131]: + local -r 'property_value=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     /usr/local/share/google/dataproc/lib/*'
<13>Jun 28 16:02:26 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.application.classpath --value '$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,
<13>Jun 28 16:02:26 startup-script[1131]:     $HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,
<13>Jun 28 16:02:26 startup-script[1131]:     /usr/local/share/google/dataproc/lib/*' --clobber
<13>Jun 28 16:02:27 startup-script[1131]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:27 startup-script[1131]: + set +x
<13>Jun 28 16:02:27 startup-script[1131]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:27 startup-script[1131]: + case ${compare_versions_result} in
<13>Jun 28 16:02:27 startup-script[1131]: + return 0
<13>Jun 28 16:02:27 startup-script[1131]: + cat
<13>Jun 28 16:02:27 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.nodemanager.env-whitelist PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ
<13>Jun 28 16:02:27 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_name=yarn.nodemanager.env-whitelist
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_value=PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ
<13>Jun 28 16:02:27 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nodemanager.env-whitelist --value PATH,JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME,LD_LIBRARY_PATH,LANG,TZ --clobber
<13>Jun 28 16:02:27 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.nm.liveness-monitor.expiry-interval-ms 15000
<13>Jun 28 16:02:27 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_name=yarn.nm.liveness-monitor.expiry-interval-ms
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_value=15000
<13>Jun 28 16:02:27 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nm.liveness-monitor.expiry-interval-ms --value 15000 --clobber
<13>Jun 28 16:02:27 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.log-aggregation-enable false
<13>Jun 28 16:02:27 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_name=yarn.log-aggregation-enable
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_value=false
<13>Jun 28 16:02:27 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value false --clobber
<13>Jun 28 16:02:27 startup-script[1131]: + ZK_QUORUM=mjtelco-m:2181,:2181,:2181
<13>Jun 28 16:02:27 startup-script[1131]: + [[ 1 -gt 1 ]]
<13>Jun 28 16:02:27 startup-script[1131]: + set_hadoop_property hdfs-site.xml dfs.namenode.file.close.num-committed-allowed 1
<13>Jun 28 16:02:27 startup-script[1131]: + local -r config=hdfs-site.xml
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_name=dfs.namenode.file.close.num-committed-allowed
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_value=1
<13>Jun 28 16:02:27 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.file.close.num-committed-allowed --value 1 --clobber
<13>Jun 28 16:02:27 startup-script[1131]: + set_hadoop_property core-site.xml hadoop.http.filter.initializers org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
<13>Jun 28 16:02:27 startup-script[1131]: + local -r config=core-site.xml
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_name=hadoop.http.filter.initializers
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_value=org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter
<13>Jun 28 16:02:27 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/core-site.xml --name hadoop.http.filter.initializers --value org.apache.hadoop.security.HttpCrossOriginFilterInitializer,org.apache.hadoop.http.lib.StaticUserWebFilter --clobber
<13>Jun 28 16:02:27 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.webapp.cross-origin.enabled true
<13>Jun 28 16:02:27 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_name=yarn.resourcemanager.webapp.cross-origin.enabled
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_value=true
<13>Jun 28 16:02:27 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.webapp.cross-origin.enabled --value true --clobber
<13>Jun 28 16:02:27 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.timeline-service.http-cross-origin.enabled true
<13>Jun 28 16:02:27 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_name=yarn.timeline-service.http-cross-origin.enabled
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_value=true
<13>Jun 28 16:02:27 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.http-cross-origin.enabled --value true --clobber
<13>Jun 28 16:02:27 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.timeline-service.enabled true
<13>Jun 28 16:02:27 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_name=yarn.timeline-service.enabled
<13>Jun 28 16:02:27 startup-script[1131]: + local -r property_value=true
<13>Jun 28 16:02:27 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.enabled --value true --clobber
<13>Jun 28 16:02:28 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.timeline-service.hostname mjtelco-m
<13>Jun 28 16:02:28 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_name=yarn.timeline-service.hostname
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_value=mjtelco-m
<13>Jun 28 16:02:28 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.hostname --value mjtelco-m --clobber
<13>Jun 28 16:02:28 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.timeline-service.bind-host 0.0.0.0
<13>Jun 28 16:02:28 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_name=yarn.timeline-service.bind-host
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_value=0.0.0.0
<13>Jun 28 16:02:28 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.bind-host --value 0.0.0.0 --clobber
<13>Jun 28 16:02:28 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.system-metrics-publisher.enabled true
<13>Jun 28 16:02:28 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_name=yarn.resourcemanager.system-metrics-publisher.enabled
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_value=true
<13>Jun 28 16:02:28 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.system-metrics-publisher.enabled --value true --clobber
<13>Jun 28 16:02:28 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.timeline-service.generic-application-history.enabled true
<13>Jun 28 16:02:28 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_name=yarn.timeline-service.generic-application-history.enabled
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_value=true
<13>Jun 28 16:02:28 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.generic-application-history.enabled --value true --clobber
<13>Jun 28 16:02:28 startup-script[1131]: + is_version_at_least 2.0 2.1
<13>Jun 28 16:02:28 startup-script[1131]: + set +x
<13>Jun 28 16:02:28 startup-script[1131]: Comparing if version 2.0 is at least version 2.1 
<13>Jun 28 16:02:28 startup-script[1131]: + case ${compare_versions_result} in
<13>Jun 28 16:02:28 startup-script[1131]: + return 1
<13>Jun 28 16:02:28 startup-script[1131]: ++ get_dataproc_property am.primary_only
<13>Jun 28 16:02:28 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:28 startup-script[1131]: + AM_ON_PRIMARY_WORKER_ENABLED=false
<13>Jun 28 16:02:28 startup-script[1131]: ++ get_metadata_datanode_enabled
<13>Jun 28 16:02:28 startup-script[1131]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Jun 28 16:02:28 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:28 startup-script[1131]: + DATAPROC_DATANODE_ENABLED=true
<13>Jun 28 16:02:28 startup-script[1131]: + [[ false == \t\r\u\e ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++ create_or_validate_include_file_path
<13>Jun 28 16:02:28 startup-script[1131]: ++ local include_path
<13>Jun 28 16:02:28 startup-script[1131]: +++ get_include_file_path
<13>Jun 28 16:02:28 startup-script[1131]: +++ local include_path
<13>Jun 28 16:02:28 startup-script[1131]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-include-file-location
<13>Jun 28 16:02:28 startup-script[1131]: ++++ echo ''
<13>Jun 28 16:02:28 startup-script[1131]: +++ include_path=
<13>Jun 28 16:02:28 startup-script[1131]: +++ [[ -z '' ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++++ get_include_file_path_deprecated
<13>Jun 28 16:02:28 startup-script[1131]: ++++ local include_path
<13>Jun 28 16:02:28 startup-script[1131]: ++++ [[ 1 -eq 1 ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++++ include_path=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: ++++ echo /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: +++ include_path=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: +++ echo /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: ++ include_path=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: ++ [[ /etc/hadoop/conf/nodes_include == gs://* ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++ [[ ! -f /etc/hadoop/conf/nodes_include ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++ loginfo 'Creating local membership files: /etc/hadoop/conf/nodes_include'
<13>Jun 28 16:02:28 startup-script[1131]: ++ echo 'Creating local membership files: /etc/hadoop/conf/nodes_include'
<13>Jun 28 16:02:28 startup-script[1131]: Creating local membership files: /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: ++ touch /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: ++ chown root:hadoop /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: ++ chmod ugo+rw /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: ++ echo /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: + INCLUDE_PATH=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: ++ create_or_validate_exclude_file_path
<13>Jun 28 16:02:28 startup-script[1131]: ++ local exclude_path
<13>Jun 28 16:02:28 startup-script[1131]: +++ get_exclude_file_path
<13>Jun 28 16:02:28 startup-script[1131]: +++ local exclude_path
<13>Jun 28 16:02:28 startup-script[1131]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-exclude-file-location
<13>Jun 28 16:02:28 startup-script[1131]: ++++ echo ''
<13>Jun 28 16:02:28 startup-script[1131]: +++ exclude_path=
<13>Jun 28 16:02:28 startup-script[1131]: +++ [[ -z '' ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++++ get_exclude_file_path_deprecated
<13>Jun 28 16:02:28 startup-script[1131]: ++++ local exclude_path
<13>Jun 28 16:02:28 startup-script[1131]: +++++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Jun 28 16:02:28 startup-script[1131]: ++++ MASTER_RUN_DRIVER_LOCATION=LOCAL
<13>Jun 28 16:02:28 startup-script[1131]: ++++ [[ LOCAL == \Y\A\R\N ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++++ [[ 1 -eq 1 ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++++ exclude_path=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: ++++ echo /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: +++ exclude_path=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: +++ echo /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: ++ exclude_path=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: ++ [[ /etc/hadoop/conf/nodes_exclude == gs://* ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++ [[ ! -f /etc/hadoop/conf/nodes_exclude ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++ loginfo 'Creating local membership files: /etc/hadoop/conf/nodes_exclude'
<13>Jun 28 16:02:28 startup-script[1131]: ++ echo 'Creating local membership files: /etc/hadoop/conf/nodes_exclude'
<13>Jun 28 16:02:28 startup-script[1131]: Creating local membership files: /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: ++ touch /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: ++ [[ /etc/hadoop/conf/nodes_exclude == *\.\x\m\l ]]
<13>Jun 28 16:02:28 startup-script[1131]: ++ chown root:hadoop /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: ++ chmod uro+rw /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: chmod: invalid mode: ‘uro+rw’
<13>Jun 28 16:02:28 startup-script[1131]: Try 'chmod --help' for more information.
<13>Jun 28 16:02:28 startup-script[1131]: ++ echo /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: + EXCLUDE_PATH=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.nodes.include-path /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_name=yarn.resourcemanager.nodes.include-path
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_value=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:28 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.nodes.include-path --value /etc/hadoop/conf/nodes_include --clobber
<13>Jun 28 16:02:28 startup-script[1131]: + set_hadoop_property yarn-site.xml yarn.resourcemanager.nodes.exclude-path /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_name=yarn.resourcemanager.nodes.exclude-path
<13>Jun 28 16:02:28 startup-script[1131]: + local -r property_value=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:28 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.resourcemanager.nodes.exclude-path --value /etc/hadoop/conf/nodes_exclude --clobber
<13>Jun 28 16:02:29 startup-script[1131]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Jun 28 16:02:29 startup-script[1131]: + MASTER_RUN_DRIVER_LOCATION=LOCAL
<13>Jun 28 16:02:29 startup-script[1131]: + [[ LOCAL == \Y\A\R\N ]]
<13>Jun 28 16:02:29 startup-script[1131]: + [[ 1 -gt 1 ]]
<13>Jun 28 16:02:29 startup-script[1131]: + readonly YARN_SIMPLIFICATION_MIXINS=yarn-simplification-mixins.xml
<13>Jun 28 16:02:29 startup-script[1131]: + YARN_SIMPLIFICATION_MIXINS=yarn-simplification-mixins.xml
<13>Jun 28 16:02:29 startup-script[1131]: + merge_hadoop_configurations yarn-site.xml /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml
<13>Jun 28 16:02:29 startup-script[1131]: + local -r config=yarn-site.xml
<13>Jun 28 16:02:29 startup-script[1131]: + local -r template=/usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml
<13>Jun 28 16:02:29 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /usr/local/share/google/dataproc/bdutil/conf/yarn-simplification-mixins.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:29 startup-script[1131]: + is_component_selected kerberos
<13>Jun 28 16:02:29 startup-script[1131]: + local -r component=kerberos
<13>Jun 28 16:02:29 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:29 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:29 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property yarn.docker.enable
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + ENABLE_DOCKER_YARN=
<13>Jun 28 16:02:29 startup-script[1131]: + [[ -z '' ]]
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property docker.yarn.enable
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + ENABLE_DOCKER_YARN=
<13>Jun 28 16:02:29 startup-script[1131]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:29 startup-script[1131]: + set +x
<13>Jun 28 16:02:29 startup-script[1131]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:29 startup-script[1131]: + case ${compare_versions_result} in
<13>Jun 28 16:02:29 startup-script[1131]: + return 0
<13>Jun 28 16:02:29 startup-script[1131]: + is_component_selected docker-ce
<13>Jun 28 16:02:29 startup-script[1131]: + local -r component=docker-ce
<13>Jun 28 16:02:29 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:29 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *docker-ce* ]]
<13>Jun 28 16:02:29 startup-script[1131]: + set -e
<13>Jun 28 16:02:29 startup-script[1131]: + loginfo 'Running configure_connectors.sh'
<13>Jun 28 16:02:29 startup-script[1131]: + echo 'Running configure_connectors.sh'
<13>Jun 28 16:02:29 startup-script[1131]: Running configure_connectors.sh
<13>Jun 28 16:02:29 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file gcs-core-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Jun 28 16:02:29 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file bq-mapred-template.xml --resolve_environment_variables --create_if_absent --noclobber
<13>Jun 28 16:02:29 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:29 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh
<13>Jun 28 16:02:29 startup-script[1131]: ++ readonly DOCKER_PATH=/var/lib/docker
<13>Jun 28 16:02:29 startup-script[1131]: ++ DOCKER_PATH=/var/lib/docker
<13>Jun 28 16:02:29 startup-script[1131]: ++ readonly GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Jun 28 16:02:29 startup-script[1131]: ++ GCR_CREDENTIAL_HELPER_VERSION=2.0.0
<13>Jun 28 16:02:29 startup-script[1131]: + is_component_selected docker-ce
<13>Jun 28 16:02:29 startup-script[1131]: + local -r component=docker-ce
<13>Jun 28 16:02:29 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:29 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *docker-ce* ]]
<13>Jun 28 16:02:29 startup-script[1131]: + readonly RUN_SCRIPT_PATH=/usr/local/share/google/dataproc/metadata-proxy.sh
<13>Jun 28 16:02:29 startup-script[1131]: + RUN_SCRIPT_PATH=/usr/local/share/google/dataproc/metadata-proxy.sh
<13>Jun 28 16:02:29 startup-script[1131]: + readonly POLLING_SCRIPT_PATH=/usr/local/share/google/dataproc/poll-metadata.sh
<13>Jun 28 16:02:29 startup-script[1131]: + POLLING_SCRIPT_PATH=/usr/local/share/google/dataproc/poll-metadata.sh
<13>Jun 28 16:02:29 startup-script[1131]: + readonly PROXY_SERVICE_CONF=/usr/lib/systemd/system/metadata-proxy.service
<13>Jun 28 16:02:29 startup-script[1131]: + PROXY_SERVICE_CONF=/usr/lib/systemd/system/metadata-proxy.service
<13>Jun 28 16:02:29 startup-script[1131]: + readonly POLLING_SERVICE_CONF=/usr/lib/systemd/system/metadata-credentials-polling.service
<13>Jun 28 16:02:29 startup-script[1131]: + POLLING_SERVICE_CONF=/usr/lib/systemd/system/metadata-credentials-polling.service
<13>Jun 28 16:02:29 startup-script[1131]: + readonly METADATA_ADDRESS=169.254.169.254
<13>Jun 28 16:02:29 startup-script[1131]: + METADATA_ADDRESS=169.254.169.254
<13>Jun 28 16:02:29 startup-script[1131]: + readonly METADATA_PORT=80
<13>Jun 28 16:02:29 startup-script[1131]: + METADATA_PORT=80
<13>Jun 28 16:02:29 startup-script[1131]: + readonly METADATA_PASSTHROUGH_PORT=987
<13>Jun 28 16:02:29 startup-script[1131]: + METADATA_PASSTHROUGH_PORT=987
<13>Jun 28 16:02:29 startup-script[1131]: + readonly METADATA_PROXY_PORT=988
<13>Jun 28 16:02:29 startup-script[1131]: + METADATA_PROXY_PORT=988
<13>Jun 28 16:02:29 startup-script[1131]: + readonly 'TOKEN_SOURCE_METADATA=TOKEN FROM METADATA ATTRIBUTES'
<13>Jun 28 16:02:29 startup-script[1131]: + TOKEN_SOURCE_METADATA='TOKEN FROM METADATA ATTRIBUTES'
<13>Jun 28 16:02:29 startup-script[1131]: + readonly 'TOKEN_SOURCE_GCS=TOKEN FROM GCS'
<13>Jun 28 16:02:29 startup-script[1131]: + TOKEN_SOURCE_GCS='TOKEN FROM GCS'
<13>Jun 28 16:02:29 startup-script[1131]: ++ /usr/share/google/get_metadata_value attributes/dataproc-bucket
<13>Jun 28 16:02:29 startup-script[1131]: + CONFIGBUCKET=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:29 startup-script[1131]: + readonly CONFIGBUCKET
<13>Jun 28 16:02:29 startup-script[1131]: ++ /usr/share/google/get_metadata_value attributes/dataproc-cluster-uuid
<13>Jun 28 16:02:29 startup-script[1131]: + CLUSTER_UUID=7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:29 startup-script[1131]: + readonly CLUSTER_UUID
<13>Jun 28 16:02:29 startup-script[1131]: ++ /usr/share/google/get_metadata_value id
<13>Jun 28 16:02:29 startup-script[1131]: + INSTANCE_UUID=8131998013723895706
<13>Jun 28 16:02:29 startup-script[1131]: + readonly INSTANCE_UUID
<13>Jun 28 16:02:29 startup-script[1131]: + readonly TOKEN_METADATA_ATTRIBUTE=attributes/dataproc-injected-credentials
<13>Jun 28 16:02:29 startup-script[1131]: + TOKEN_METADATA_ATTRIBUTE=attributes/dataproc-injected-credentials
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property dataproc.metadata.proxy.enabled
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + METADATA_PROXY_ENABLED=
<13>Jun 28 16:02:29 startup-script[1131]: + readonly METADATA_PROXY_ENABLED
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property dataproc.exclusive.user
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + DATAPROC_EXCLUSIVE_USER=
<13>Jun 28 16:02:29 startup-script[1131]: + readonly DATAPROC_EXCLUSIVE_USER
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property dataproc.personal-auth.override-user-display-name
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + PERSONAL_AUTH_OVERRIDE_USER_DISPLAY_NAME=
<13>Jun 28 16:02:29 startup-script[1131]: + readonly PERSONAL_AUTH_OVERRIDE_USER_DISPLAY_NAME
<13>Jun 28 16:02:29 startup-script[1131]: + [[ '' == \t\r\u\e ]]
<13>Jun 28 16:02:29 startup-script[1131]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:29 startup-script[1131]: + WORKER_COUNT=5
<13>Jun 28 16:02:29 startup-script[1131]: + is_component_selected yarn
<13>Jun 28 16:02:29 startup-script[1131]: + local -r component=yarn
<13>Jun 28 16:02:29 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:29 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *yarn* ]]
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property_or_default yarn.log-aggregation.enabled false
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: + LOG_AGGREGATION_ENABLED=true
<13>Jun 28 16:02:29 startup-script[1131]: + readonly LOG_AGGREGATION_ENABLED
<13>Jun 28 16:02:29 startup-script[1131]: + [[ true == \t\r\u\e ]]
<13>Jun 28 16:02:29 startup-script[1131]: + loginfo 'Enabling YARN log aggregation.'
<13>Jun 28 16:02:29 startup-script[1131]: + echo 'Enabling YARN log aggregation.'
<13>Jun 28 16:02:29 startup-script[1131]: Enabling YARN log aggregation.
<13>Jun 28 16:02:29 startup-script[1131]: + set_property_yarn_site yarn.log-aggregation-enable true
<13>Jun 28 16:02:29 startup-script[1131]: + local -r name=yarn.log-aggregation-enable
<13>Jun 28 16:02:29 startup-script[1131]: + local -r value=true
<13>Jun 28 16:02:29 startup-script[1131]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation-enable true
<13>Jun 28 16:02:29 startup-script[1131]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Jun 28 16:02:29 startup-script[1131]: + local -r name=yarn.log-aggregation-enable
<13>Jun 28 16:02:29 startup-script[1131]: + local -r value=true
<13>Jun 28 16:02:29 startup-script[1131]: + local -r mode=overwrite
<13>Jun 28 16:02:29 startup-script[1131]: + local -r delimiter=,
<13>Jun 28 16:02:29 startup-script[1131]: + local skip=false
<13>Jun 28 16:02:29 startup-script[1131]: + local old_value
<13>Jun 28 16:02:29 startup-script[1131]: + local new_value
<13>Jun 28 16:02:29 startup-script[1131]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Jun 28 16:02:29 startup-script[1131]: + new_value=true
<13>Jun 28 16:02:29 startup-script[1131]: + [[ false == \f\a\l\s\e ]]
<13>Jun 28 16:02:29 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation-enable --value true --create_if_absent --clobber
<13>Jun 28 16:02:29 startup-script[1131]: + is_component_selected kerberos
<13>Jun 28 16:02:29 startup-script[1131]: + local -r component=kerberos
<13>Jun 28 16:02:29 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:29 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:29 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:29 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:29 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:29 startup-script[1131]: + readonly YARN_LOG_SERVER_SCHEMA=http
<13>Jun 28 16:02:29 startup-script[1131]: + YARN_LOG_SERVER_SCHEMA=http
<13>Jun 28 16:02:29 startup-script[1131]: + readonly YARN_LOG_SERVER_PORT=19888
<13>Jun 28 16:02:29 startup-script[1131]: + YARN_LOG_SERVER_PORT=19888
<13>Jun 28 16:02:29 startup-script[1131]: + readonly YARN_LOG_SERVER_URL=http://mjtelco-m:19888/jobhistory/logs
<13>Jun 28 16:02:29 startup-script[1131]: + YARN_LOG_SERVER_URL=http://mjtelco-m:19888/jobhistory/logs
<13>Jun 28 16:02:29 startup-script[1131]: + set_property_yarn_site yarn.log.server.url http://mjtelco-m:19888/jobhistory/logs
<13>Jun 28 16:02:29 startup-script[1131]: + local -r name=yarn.log.server.url
<13>Jun 28 16:02:29 startup-script[1131]: + local -r value=http://mjtelco-m:19888/jobhistory/logs
<13>Jun 28 16:02:29 startup-script[1131]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log.server.url http://mjtelco-m:19888/jobhistory/logs
<13>Jun 28 16:02:29 startup-script[1131]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Jun 28 16:02:29 startup-script[1131]: + local -r name=yarn.log.server.url
<13>Jun 28 16:02:29 startup-script[1131]: + local -r value=http://mjtelco-m:19888/jobhistory/logs
<13>Jun 28 16:02:29 startup-script[1131]: + local -r mode=overwrite
<13>Jun 28 16:02:29 startup-script[1131]: + local -r delimiter=,
<13>Jun 28 16:02:29 startup-script[1131]: + local skip=false
<13>Jun 28 16:02:29 startup-script[1131]: + local old_value
<13>Jun 28 16:02:29 startup-script[1131]: + local new_value
<13>Jun 28 16:02:29 startup-script[1131]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Jun 28 16:02:29 startup-script[1131]: + new_value=http://mjtelco-m:19888/jobhistory/logs
<13>Jun 28 16:02:29 startup-script[1131]: + [[ false == \f\a\l\s\e ]]
<13>Jun 28 16:02:29 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log.server.url --value http://mjtelco-m:19888/jobhistory/logs --create_if_absent --clobber
<13>Jun 28 16:02:30 startup-script[1131]: + [[ -n dataproc-temp-us-east1-746779145865-aaezsxz3 ]]
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_yarn_site yarn.nodemanager.remote-app-log-dir gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/yarn-logs
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=yarn.nodemanager.remote-app-log-dir
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/yarn-logs
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.nodemanager.remote-app-log-dir gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/yarn-logs
<13>Jun 28 16:02:30 startup-script[1131]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=yarn.nodemanager.remote-app-log-dir
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/yarn-logs
<13>Jun 28 16:02:30 startup-script[1131]: + local -r mode=overwrite
<13>Jun 28 16:02:30 startup-script[1131]: + local -r delimiter=,
<13>Jun 28 16:02:30 startup-script[1131]: + local skip=false
<13>Jun 28 16:02:30 startup-script[1131]: + local old_value
<13>Jun 28 16:02:30 startup-script[1131]: + local new_value
<13>Jun 28 16:02:30 startup-script[1131]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + new_value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/yarn-logs
<13>Jun 28 16:02:30 startup-script[1131]: + [[ false == \f\a\l\s\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.nodemanager.remote-app-log-dir --value gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/yarn-logs --create_if_absent --clobber
<13>Jun 28 16:02:30 startup-script[1131]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:30 startup-script[1131]: + set +x
<13>Jun 28 16:02:30 startup-script[1131]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:30 startup-script[1131]: + case ${compare_versions_result} in
<13>Jun 28 16:02:30 startup-script[1131]: + return 0
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_yarn_site yarn.log-aggregation.file-formats IFile,TFile
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=yarn.log-aggregation.file-formats
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=IFile,TFile
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation.file-formats IFile,TFile
<13>Jun 28 16:02:30 startup-script[1131]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=yarn.log-aggregation.file-formats
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=IFile,TFile
<13>Jun 28 16:02:30 startup-script[1131]: + local -r mode=overwrite
<13>Jun 28 16:02:30 startup-script[1131]: + local -r delimiter=,
<13>Jun 28 16:02:30 startup-script[1131]: + local skip=false
<13>Jun 28 16:02:30 startup-script[1131]: + local old_value
<13>Jun 28 16:02:30 startup-script[1131]: + local new_value
<13>Jun 28 16:02:30 startup-script[1131]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + new_value=IFile,TFile
<13>Jun 28 16:02:30 startup-script[1131]: + [[ false == \f\a\l\s\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation.file-formats --value IFile,TFile --create_if_absent --clobber
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_yarn_site yarn.log-aggregation.file-controller.IFile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=yarn.log-aggregation.file-controller.IFile.class
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_in_xml /etc/hadoop/conf/yarn-site.xml yarn.log-aggregation.file-controller.IFile.class org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Jun 28 16:02:30 startup-script[1131]: + local -r xml_file=/etc/hadoop/conf/yarn-site.xml
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=yarn.log-aggregation.file-controller.IFile.class
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Jun 28 16:02:30 startup-script[1131]: + local -r mode=overwrite
<13>Jun 28 16:02:30 startup-script[1131]: + local -r delimiter=,
<13>Jun 28 16:02:30 startup-script[1131]: + local skip=false
<13>Jun 28 16:02:30 startup-script[1131]: + local old_value
<13>Jun 28 16:02:30 startup-script[1131]: + local new_value
<13>Jun 28 16:02:30 startup-script[1131]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + new_value=org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController
<13>Jun 28 16:02:30 startup-script[1131]: + [[ false == \f\a\l\s\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.log-aggregation.file-controller.IFile.class --value org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController --create_if_absent --clobber
<13>Jun 28 16:02:30 startup-script[1131]: ++ get_dataproc_property_or_default job.history.to-gcs.enabled false
<13>Jun 28 16:02:30 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:30 startup-script[1131]: + persist_history_to_gcs=true
<13>Jun 28 16:02:30 startup-script[1131]: + [[ true == \t\r\u\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + loginfo 'Enabling persisting MapReduce job history files to GCS.'
<13>Jun 28 16:02:30 startup-script[1131]: + echo 'Enabling persisting MapReduce job history files to GCS.'
<13>Jun 28 16:02:30 startup-script[1131]: Enabling persisting MapReduce job history files to GCS.
<13>Jun 28 16:02:30 startup-script[1131]: + [[ -n dataproc-temp-us-east1-746779145865-aaezsxz3 ]]
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_mapred_site mapreduce.jobhistory.done-dir gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=mapreduce.jobhistory.done-dir
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.done-dir gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done
<13>Jun 28 16:02:30 startup-script[1131]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=mapreduce.jobhistory.done-dir
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done
<13>Jun 28 16:02:30 startup-script[1131]: + local -r mode=overwrite
<13>Jun 28 16:02:30 startup-script[1131]: + local -r delimiter=,
<13>Jun 28 16:02:30 startup-script[1131]: + local skip=false
<13>Jun 28 16:02:30 startup-script[1131]: + local old_value
<13>Jun 28 16:02:30 startup-script[1131]: + local new_value
<13>Jun 28 16:02:30 startup-script[1131]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + new_value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done
<13>Jun 28 16:02:30 startup-script[1131]: + [[ false == \f\a\l\s\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.done-dir --value gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done --create_if_absent --clobber
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_mapred_site mapreduce.jobhistory.intermediate-done-dir gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done_intermediate
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=mapreduce.jobhistory.intermediate-done-dir
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done_intermediate
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.intermediate-done-dir gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done_intermediate
<13>Jun 28 16:02:30 startup-script[1131]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=mapreduce.jobhistory.intermediate-done-dir
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done_intermediate
<13>Jun 28 16:02:30 startup-script[1131]: + local -r mode=overwrite
<13>Jun 28 16:02:30 startup-script[1131]: + local -r delimiter=,
<13>Jun 28 16:02:30 startup-script[1131]: + local skip=false
<13>Jun 28 16:02:30 startup-script[1131]: + local old_value
<13>Jun 28 16:02:30 startup-script[1131]: + local new_value
<13>Jun 28 16:02:30 startup-script[1131]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + new_value=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done_intermediate
<13>Jun 28 16:02:30 startup-script[1131]: + [[ false == \f\a\l\s\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.intermediate-done-dir --value gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/mapreduce-job-history/done_intermediate --create_if_absent --clobber
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_mapred_site mapreduce.jobhistory.move.interval-ms 1000
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=mapreduce.jobhistory.move.interval-ms
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=1000
<13>Jun 28 16:02:30 startup-script[1131]: + set_property_in_xml /etc/hadoop/conf/mapred-site.xml mapreduce.jobhistory.move.interval-ms 1000
<13>Jun 28 16:02:30 startup-script[1131]: + local -r xml_file=/etc/hadoop/conf/mapred-site.xml
<13>Jun 28 16:02:30 startup-script[1131]: + local -r name=mapreduce.jobhistory.move.interval-ms
<13>Jun 28 16:02:30 startup-script[1131]: + local -r value=1000
<13>Jun 28 16:02:30 startup-script[1131]: + local -r mode=overwrite
<13>Jun 28 16:02:30 startup-script[1131]: + local -r delimiter=,
<13>Jun 28 16:02:30 startup-script[1131]: + local skip=false
<13>Jun 28 16:02:30 startup-script[1131]: + local old_value
<13>Jun 28 16:02:30 startup-script[1131]: + local new_value
<13>Jun 28 16:02:30 startup-script[1131]: + [[ overwrite == \o\v\e\r\w\r\i\t\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + new_value=1000
<13>Jun 28 16:02:30 startup-script[1131]: + [[ false == \f\a\l\s\e ]]
<13>Jun 28 16:02:30 startup-script[1131]: + bdconfig set_property --configuration_file /etc/hadoop/conf/mapred-site.xml --name mapreduce.jobhistory.move.interval-ms --value 1000 --create_if_absent --clobber
<13>Jun 28 16:02:30 startup-script[1131]: ++ get_dataproc_property dataproc.users
<13>Jun 28 16:02:30 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:30 startup-script[1131]: + DATAPROC_USERS=
<13>Jun 28 16:02:30 startup-script[1131]: + [[ -n '' ]]
<13>Jun 28 16:02:30 startup-script[1131]: + is_component_selected kerberos
<13>Jun 28 16:02:30 startup-script[1131]: + local -r component=kerberos
<13>Jun 28 16:02:30 startup-script[1131]: + local activated_components
<13>Jun 28 16:02:30 startup-script[1131]: ++ get_components_to_activate
<13>Jun 28 16:02:30 startup-script[1131]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:30 startup-script[1131]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:30 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:30 startup-script[1131]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:30 startup-script[1131]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:30 startup-script[1131]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:30 startup-script[1131]: + loginfo 'Merging user-specified cluster properties'
<13>Jun 28 16:02:30 startup-script[1131]: + echo 'Merging user-specified cluster properties'
<13>Jun 28 16:02:30 startup-script[1131]: Merging user-specified cluster properties
<13>Jun 28 16:02:30 startup-script[1131]: + merge_xml_properties /tmp/cluster/properties/capacity-scheduler.xml /etc/hadoop/conf/capacity-scheduler.xml
<13>Jun 28 16:02:30 startup-script[1131]: + local src=/tmp/cluster/properties/capacity-scheduler.xml
<13>Jun 28 16:02:30 startup-script[1131]: + local dest=/etc/hadoop/conf/capacity-scheduler.xml
<13>Jun 28 16:02:30 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/capacity-scheduler.xml ]]
<13>Jun 28 16:02:30 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/capacity-scheduler.xml --source_configuration_file /tmp/cluster/properties/capacity-scheduler.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/capacity-scheduler.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/capacity-scheduler.xml.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_xml_properties /tmp/cluster/properties/core.xml /etc/hadoop/conf/core-site.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local src=/tmp/cluster/properties/core.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local dest=/etc/hadoop/conf/core-site.xml
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/core.xml ]]
<13>Jun 28 16:02:31 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/core-site.xml --source_configuration_file /tmp/cluster/properties/core.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/core.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/core.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/core.xml.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_xml_properties /tmp/cluster/properties/distcp.xml /etc/hadoop/conf/distcp-default.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local src=/tmp/cluster/properties/distcp.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local dest=/etc/hadoop/conf/distcp-default.xml
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/distcp.xml ]]
<13>Jun 28 16:02:31 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/distcp-default.xml --source_configuration_file /tmp/cluster/properties/distcp.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/distcp.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/distcp.xml.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_xml_properties /tmp/cluster/properties/mapred.xml /etc/hadoop/conf/mapred-site.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local src=/tmp/cluster/properties/mapred.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local dest=/etc/hadoop/conf/mapred-site.xml
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/mapred.xml ]]
<13>Jun 28 16:02:31 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/mapred-site.xml --source_configuration_file /tmp/cluster/properties/mapred.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/mapred.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/mapred.xml.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_xml_properties /tmp/cluster/properties/yarn.xml /etc/hadoop/conf/yarn-site.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local src=/tmp/cluster/properties/yarn.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local dest=/etc/hadoop/conf/yarn-site.xml
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/yarn.xml ]]
<13>Jun 28 16:02:31 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/yarn-site.xml --source_configuration_file /tmp/cluster/properties/yarn.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/yarn.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/yarn.xml.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_sh_env_vars /tmp/cluster/properties/hadoop-env.sh /etc/hadoop/conf/hadoop-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + local src=/tmp/cluster/properties/hadoop-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + local dest=/etc/hadoop/conf/hadoop-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/hadoop-env.sh ]]
<13>Jun 28 16:02:31 startup-script[1131]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + cat /tmp/cluster/properties/hadoop-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/hadoop-env.sh.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/hadoop-env.sh.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_sh_env_vars /tmp/cluster/properties/mapred-env.sh /etc/hadoop/conf/mapred-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + local src=/tmp/cluster/properties/mapred-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + local dest=/etc/hadoop/conf/mapred-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/mapred-env.sh ]]
<13>Jun 28 16:02:31 startup-script[1131]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + cat /tmp/cluster/properties/mapred-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/mapred-env.sh.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/mapred-env.sh.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_sh_env_vars /tmp/cluster/properties/yarn-env.sh /etc/hadoop/conf/yarn-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + local src=/tmp/cluster/properties/yarn-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + local dest=/etc/hadoop/conf/yarn-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/yarn-env.sh ]]
<13>Jun 28 16:02:31 startup-script[1131]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + cat /tmp/cluster/properties/yarn-env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/yarn-env.sh.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/yarn-env.sh.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_java_properties /tmp/cluster/properties/hadoop-log4j.properties /etc/hadoop/conf/log4j.properties
<13>Jun 28 16:02:31 startup-script[1131]: + local -r src=/tmp/cluster/properties/hadoop-log4j.properties
<13>Jun 28 16:02:31 startup-script[1131]: + local -r dest=/etc/hadoop/conf/log4j.properties
<13>Jun 28 16:02:31 startup-script[1131]: + local -r 'header=\n# User-supplied properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/hadoop-log4j.properties ]]
<13>Jun 28 16:02:31 startup-script[1131]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + cat /tmp/cluster/properties/hadoop-log4j.properties
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/hadoop-log4j.properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/hadoop-log4j.properties.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/hadoop-log4j.properties.
<13>Jun 28 16:02:31 startup-script[1131]: + [[ -f /etc/hbase/conf/hbase-site.xml ]]
<13>Jun 28 16:02:31 startup-script[1131]: + merge_xml_properties /tmp/cluster/properties/hbase.xml /etc/hbase/conf/hbase-site.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local src=/tmp/cluster/properties/hbase.xml
<13>Jun 28 16:02:31 startup-script[1131]: + local dest=/etc/hbase/conf/hbase-site.xml
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/hbase.xml ]]
<13>Jun 28 16:02:31 startup-script[1131]: + bdconfig merge_configurations --configuration_file /etc/hbase/conf/hbase-site.xml --source_configuration_file /tmp/cluster/properties/hbase.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/hbase.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/hbase.xml.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/hbase.xml.
<13>Jun 28 16:02:31 startup-script[1131]: + merge_java_properties /tmp/cluster/properties/hbase-log4j.properties /etc/hbase/conf/log4j.properties
<13>Jun 28 16:02:31 startup-script[1131]: + local -r src=/tmp/cluster/properties/hbase-log4j.properties
<13>Jun 28 16:02:31 startup-script[1131]: + local -r dest=/etc/hbase/conf/log4j.properties
<13>Jun 28 16:02:31 startup-script[1131]: + local -r 'header=\n# User-supplied properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + [[ ! -f /tmp/cluster/properties/hbase-log4j.properties ]]
<13>Jun 28 16:02:31 startup-script[1131]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + cat /tmp/cluster/properties/hbase-log4j.properties
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Merged /tmp/cluster/properties/hbase-log4j.properties.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Merged /tmp/cluster/properties/hbase-log4j.properties.'
<13>Jun 28 16:02:31 startup-script[1131]: Merged /tmp/cluster/properties/hbase-log4j.properties.
<13>Jun 28 16:02:31 startup-script[1131]: + DATAPROC_COMPONENTS=(${DATAPROC_OPTIONAL_COMPONENTS})
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'DATAPROC_COMPONENTS: docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server'
<13>Jun 28 16:02:31 startup-script[1131]: DATAPROC_COMPONENTS: docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: + ARTIFACTS_TO_KEEP=("${SERVICES[@]}" ${DATAPROC_COMMON_PACKAGES})
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'ARTIFACTS_TO_KEEP: autofs bash-completion bc git jq netcat vim wget bigtop-utils hadoop-client hadoop-lzo temurin-8-jdk google-fluentd=1.9.12-1 stackdriver-agent docker-ce druid flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog hudi texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-j pig presto ranger rubix spark tez yarn zookeeper-server libapr1 libatlas3-base libjansi-java libopenblas-base libsnappy-dev libssl-dev libzstd-dev linux-headers-4.19.0-24-cloud-amd64 openssl uuid-runtime python-numpy python-pip python-requests python-setuptools linux-image-amd64 linux-headers-amd64 linux-kbuild-5.10=5.10.179-1~deb10u1'
<13>Jun 28 16:02:31 startup-script[1131]: ARTIFACTS_TO_KEEP: autofs bash-completion bc git jq netcat vim wget bigtop-utils hadoop-client hadoop-lzo temurin-8-jdk google-fluentd=1.9.12-1 stackdriver-agent docker-ce druid flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog hudi texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-j pig presto ranger rubix spark tez yarn zookeeper-server libapr1 libatlas3-base libjansi-java libopenblas-base libsnappy-dev libssl-dev libzstd-dev linux-headers-4.19.0-24-cloud-amd64 openssl uuid-runtime python-numpy python-pip python-requests python-setuptools linux-image-amd64 linux-headers-amd64 linux-kbuild-5.10=5.10.179-1~deb10u1
<13>Jun 28 16:02:31 startup-script[1131]: + COMPONENTS_TO_ACTIVATE=($(intersection SELECTED_COMPONENTS ARTIFACTS_TO_KEEP))
<13>Jun 28 16:02:31 startup-script[1131]: ++ intersection SELECTED_COMPONENTS ARTIFACTS_TO_KEEP
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n values=SELECTED_COMPONENTS
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n filter=ARTIFACTS_TO_KEEP
<13>Jun 28 16:02:31 startup-script[1131]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' autofs bash-completion bc git jq netcat vim wget bigtop-utils hadoop-client hadoop-lzo temurin-8-jdk google-fluentd=1.9.12-1 stackdriver-agent docker-ce druid flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog hudi texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-j pig presto ranger rubix spark tez yarn zookeeper-server libapr1 libatlas3-base libjansi-java libopenblas-base libsnappy-dev libssl-dev libzstd-dev linux-headers-4.19.0-24-cloud-amd64 openssl uuid-runtime python-numpy python-pip python-requests python-setuptools linux-image-amd64 linux-headers-amd64 linux-kbuild-5.10=5.10.179-1~deb10u1
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'COMPONENTS_TO_ACTIVATE: hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql pig spark tez yarn'
<13>Jun 28 16:02:31 startup-script[1131]: COMPONENTS_TO_ACTIVATE: hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql pig spark tez yarn
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Generating post_hdfs_env.sh'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Generating post_hdfs_env.sh'
<13>Jun 28 16:02:31 startup-script[1131]: Generating post_hdfs_env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + cat
<13>Jun 28 16:02:31 startup-script[1131]: + chmod +x /usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:31 startup-script[1131]: + NON_ACTIVATED_COMPONENTS=($(difference DATAPROC_COMPONENTS COMPONENTS_TO_ACTIVATE))
<13>Jun 28 16:02:31 startup-script[1131]: ++ difference DATAPROC_COMPONENTS COMPONENTS_TO_ACTIVATE
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n values=DATAPROC_COMPONENTS
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n filter=COMPONENTS_TO_ACTIVATE
<13>Jun 28 16:02:31 startup-script[1131]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql pig spark tez yarn
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'NON_ACTIVATED_COMPONENTS: docker-ce dpms-proxy druid earlyoom flink hbase hive-webhcat-server hudi jupyter kafka-server kerberos knox presto proxy-agent ranger rubix solr-server zeppelin zookeeper-server'
<13>Jun 28 16:02:31 startup-script[1131]: NON_ACTIVATED_COMPONENTS: docker-ce dpms-proxy druid earlyoom flink hbase hive-webhcat-server hudi jupyter kafka-server kerberos knox presto proxy-agent ranger rubix solr-server zeppelin zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: + ARTIFACTS_TO_UNINSTALL+=($(difference NON_ACTIVATED_COMPONENTS ARTIFACTS_TO_UNINSTALL))
<13>Jun 28 16:02:31 startup-script[1131]: ++ difference NON_ACTIVATED_COMPONENTS ARTIFACTS_TO_UNINSTALL
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n values=NON_ACTIVATED_COMPONENTS
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n filter=ARTIFACTS_TO_UNINSTALL
<13>Jun 28 16:02:31 startup-script[1131]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' docker-ce dpms-proxy druid earlyoom flink hbase hive-webhcat-server hudi jupyter kafka-server kerberos knox presto proxy-agent ranger rubix solr-server zeppelin zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' dpms-proxy earlyoom hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hadoop-hdfs-zkfc
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'ARTIFACTS_TO_UNINSTALL: dpms-proxy earlyoom hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hadoop-hdfs-zkfc docker-ce druid flink hbase hudi kafka-server kerberos presto ranger rubix zookeeper-server'
<13>Jun 28 16:02:31 startup-script[1131]: ARTIFACTS_TO_UNINSTALL: dpms-proxy earlyoom hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hadoop-hdfs-zkfc docker-ce druid flink hbase hudi kafka-server kerberos presto ranger rubix zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: + COMPONENTS_TO_UNINSTALL=($(intersection ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS))
<13>Jun 28 16:02:31 startup-script[1131]: ++ intersection ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n values=ARTIFACTS_TO_UNINSTALL
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n filter=DATAPROC_COMPONENTS
<13>Jun 28 16:02:31 startup-script[1131]: ++ comm -12 /dev/fd/63 /dev/fd/62
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' dpms-proxy earlyoom hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hadoop-hdfs-zkfc docker-ce druid flink hbase hudi kafka-server kerberos presto ranger rubix zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'COMPONENTS_TO_UNINSTALL: docker-ce dpms-proxy druid earlyoom flink hbase hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox presto proxy-agent ranger rubix solr-server zeppelin zookeeper-server'
<13>Jun 28 16:02:31 startup-script[1131]: COMPONENTS_TO_UNINSTALL: docker-ce dpms-proxy druid earlyoom flink hbase hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox presto proxy-agent ranger rubix solr-server zeppelin zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: + PACKAGES_TO_UNINSTALL=($(difference ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS))
<13>Jun 28 16:02:31 startup-script[1131]: ++ difference ARTIFACTS_TO_UNINSTALL DATAPROC_COMPONENTS
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n values=ARTIFACTS_TO_UNINSTALL
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n filter=DATAPROC_COMPONENTS
<13>Jun 28 16:02:31 startup-script[1131]: ++ comm -23 /dev/fd/63 /dev/fd/62
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' dpms-proxy earlyoom hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hadoop-hdfs-zkfc docker-ce druid flink hbase hudi kafka-server kerberos presto ranger rubix zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: +++ printf '%s\n' docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server
<13>Jun 28 16:02:31 startup-script[1131]: +++ sort -u
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'PACKAGES_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-mapreduce-historyserver hadoop-yarn-resourcemanager hadoop-yarn-timelineserver mysql-server spark-history-server'
<13>Jun 28 16:02:31 startup-script[1131]: PACKAGES_TO_UNINSTALL: hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-secondarynamenode hadoop-hdfs-zkfc hadoop-mapreduce-historyserver hadoop-yarn-resourcemanager hadoop-yarn-timelineserver mysql-server spark-history-server
<13>Jun 28 16:02:31 startup-script[1131]: + is_default_metrics_enabled
<13>Jun 28 16:02:31 startup-script[1131]: ++ get_dataproc_property dataproc.monitoring.default.metrics.enable
<13>Jun 28 16:02:31 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:31 startup-script[1131]: + local -r default_metrics_enabled=true
<13>Jun 28 16:02:31 startup-script[1131]: + [[ true == \t\r\u\e ]]
<13>Jun 28 16:02:31 startup-script[1131]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:31 startup-script[1131]: + SERVICES=("${SERVICES[@]}" stackdriver-agent)
<13>Jun 28 16:02:31 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/configure_default_metrics.sh
<13>Jun 28 16:02:31 startup-script[1131]: ++ set -e
<13>Jun 28 16:02:31 startup-script[1131]: ++ set -u
<13>Jun 28 16:02:31 startup-script[1131]: ++ loginfo 'Running configure_default_metrics.sh'
<13>Jun 28 16:02:31 startup-script[1131]: ++ echo 'Running configure_default_metrics.sh'
<13>Jun 28 16:02:31 startup-script[1131]: Running configure_default_metrics.sh
<13>Jun 28 16:02:31 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:31 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:31 startup-script[1131]: ++ update_default_collectd_config_files
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -r collectd_plugin_dir=/opt/stackdriver/collectd/etc/collectd.d/
<13>Jun 28 16:02:31 startup-script[1131]: ++ is_metric_source_enabled monitoringAgentDefaults
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -r input_metric_source=monitoringAgentDefaults
<13>Jun 28 16:02:31 startup-script[1131]: ++ local stack_driver_monitoring_enabled
<13>Jun 28 16:02:31 startup-script[1131]: +++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Jun 28 16:02:31 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:31 startup-script[1131]: ++ stack_driver_monitoring_enabled=false
<13>Jun 28 16:02:31 startup-script[1131]: ++ [[ false == \t\r\u\e ]]
<13>Jun 28 16:02:31 startup-script[1131]: ++ local metric_sources_enabled_array
<13>Jun 28 16:02:31 startup-script[1131]: ++ get_enabled_metric_sources_array metric_sources_enabled_array
<13>Jun 28 16:02:31 startup-script[1131]: ++ local -n ms_array=metric_sources_enabled_array
<13>Jun 28 16:02:31 startup-script[1131]: ++ local metric_sources_enabled_delimited_by_comma
<13>Jun 28 16:02:31 startup-script[1131]: +++ get_dataproc_property dataproc.monitoring.metric.sources
<13>Jun 28 16:02:31 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:31 startup-script[1131]: ++ metric_sources_enabled_delimited_by_comma=
<13>Jun 28 16:02:31 startup-script[1131]: ++ ms_array=(${metric_sources_enabled_delimited_by_comma//,/ })
<13>Jun 28 16:02:31 startup-script[1131]: ++ [[    =~  monitoringAgentDefaults  ]]
<13>Jun 28 16:02:31 startup-script[1131]: ++ return 1
<13>Jun 28 16:02:31 startup-script[1131]: ++ sed -i /stackdriver_agent/d /usr/local/share/google/dataproc/bdutil/conf/collectd_without_monitoring_agent_defaults.conf
<13>Jun 28 16:02:31 startup-script[1131]: ++ ln -sf /usr/local/share/google/dataproc/bdutil/conf/collectd_without_monitoring_agent_defaults.conf /etc/stackdriver/collectd.conf
<13>Jun 28 16:02:31 startup-script[1131]: ++ cp /usr/local/share/google/dataproc/bdutil/conf/collectd_processes_default_metrics.conf /opt/stackdriver/collectd/etc/collectd.d//collectd_processes_default_metrics.conf
<13>Jun 28 16:02:31 startup-script[1131]: ++ cp /usr/local/share/google/dataproc/bdutil/conf/collectd_spark_default_metrics.conf /opt/stackdriver/collectd/etc/collectd.d//collectd_spark_default_metrics.conf
<13>Jun 28 16:02:31 startup-script[1131]: + is_any_metric_source_enabled
<13>Jun 28 16:02:31 startup-script[1131]: + local stack_driver_monitoring_enabled
<13>Jun 28 16:02:31 startup-script[1131]: ++ get_dataproc_property dataproc.monitoring.stackdriver.enable
<13>Jun 28 16:02:31 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:31 startup-script[1131]: + stack_driver_monitoring_enabled=false
<13>Jun 28 16:02:31 startup-script[1131]: + [[ false == \t\r\u\e ]]
<13>Jun 28 16:02:31 startup-script[1131]: + local metric_sources_enabled_array
<13>Jun 28 16:02:31 startup-script[1131]: + get_enabled_metric_sources_array metric_sources_enabled_array
<13>Jun 28 16:02:31 startup-script[1131]: + local -n ms_array=metric_sources_enabled_array
<13>Jun 28 16:02:31 startup-script[1131]: + local metric_sources_enabled_delimited_by_comma
<13>Jun 28 16:02:31 startup-script[1131]: ++ get_dataproc_property dataproc.monitoring.metric.sources
<13>Jun 28 16:02:31 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:31 startup-script[1131]: + metric_sources_enabled_delimited_by_comma=
<13>Jun 28 16:02:31 startup-script[1131]: + ms_array=(${metric_sources_enabled_delimited_by_comma//,/ })
<13>Jun 28 16:02:31 startup-script[1131]: + [[ 0 -gt 0 ]]
<13>Jun 28 16:02:31 startup-script[1131]: + return 1
<13>Jun 28 16:02:31 startup-script[1131]: + loginfo 'Stackdriver monitoring disabled.'
<13>Jun 28 16:02:31 startup-script[1131]: + echo 'Stackdriver monitoring disabled.'
<13>Jun 28 16:02:31 startup-script[1131]: Stackdriver monitoring disabled.
<13>Jun 28 16:02:31 startup-script[1131]: + is_default_system_metrics_enabled
<13>Jun 28 16:02:31 startup-script[1131]: ++ get_dataproc_property dataproc.monitoring.default.metrics.system.enable
<13>Jun 28 16:02:31 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:31 startup-script[1131]: + local -r default_metrics_system_enabled=true
<13>Jun 28 16:02:31 startup-script[1131]: + [[ true == \t\r\u\e ]]
<13>Jun 28 16:02:31 startup-script[1131]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:31 startup-script[1131]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:31 startup-script[1131]: + set +x
<13>Jun 28 16:02:31 startup-script[1131]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:31 startup-script[1131]: + case ${compare_versions_result} in
<13>Jun 28 16:02:31 startup-script[1131]: + return 0
<13>Jun 28 16:02:31 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/configure_npd.sh
<13>Jun 28 16:02:31 startup-script[1131]: + configure_npd job_metrics_enabled
<13>Jun 28 16:02:31 startup-script[1131]: + local -r yarn_metrics_enabled=job_metrics_enabled
<13>Jun 28 16:02:31 startup-script[1131]: + local -r init_script=/etc/systemd/system/npd.service
<13>Jun 28 16:02:31 startup-script[1131]: + local -r config_dir=/usr/local/share/google/dataproc/npd-config
<13>Jun 28 16:02:31 startup-script[1131]: + local -r yarn_metrics_configuration=
<13>Jun 28 16:02:31 startup-script[1131]: + yarn_metrics_enabled
<13>Jun 28 16:02:31 startup-script[1131]: /usr/local/share/google/dataproc/bdutil/configure_npd.sh: line 12: yarn_metrics_enabled: command not found
<13>Jun 28 16:02:31 startup-script[1131]: + cat
<13>Jun 28 16:02:31 startup-script[1131]: + chmod a+rw /etc/systemd/system/npd.service
<13>Jun 28 16:02:31 startup-script[1131]: + systemctl daemon-reload
<13>Jun 28 16:02:32 startup-script[1131]: + systemctl enable npd
<13>Jun 28 16:02:32 startup-script[1131]: Created symlink /etc/systemd/system/multi-user.target.wants/npd.service → /etc/systemd/system/npd.service.
<13>Jun 28 16:02:32 startup-script[1131]: + systemctl start npd
<13>Jun 28 16:02:32 startup-script[1131]: + systemctl status npd
<13>Jun 28 16:02:32 startup-script[1131]: ● npd.service - Node Problem Detector
<13>Jun 28 16:02:32 startup-script[1131]:    Loaded: loaded (/etc/systemd/system/npd.service; enabled; vendor preset: enabled)
<13>Jun 28 16:02:32 startup-script[1131]:    Active: active (running) since Wed 2023-06-28 16:02:32 UTC; 9ms ago
<13>Jun 28 16:02:32 startup-script[1131]:  Main PID: 2095 (npd)
<13>Jun 28 16:02:32 startup-script[1131]:     Tasks: 1 (limit: 4915)
<13>Jun 28 16:02:32 startup-script[1131]:    Memory: 340.0K
<13>Jun 28 16:02:32 startup-script[1131]:    CGroup: /system.slice/npd.service
<13>Jun 28 16:02:32 startup-script[1131]:            └─2095 /usr/local/bin/npd --logtostderr --stackdriver-config=/usr/local/share/google/dataproc/npd-config/exporter/stackdriver-exporter.json --config.system-stats-monitor=/usr/local/share/google/dataproc/npd-config/system-stats-monitor.json,/usr/local/share/google/dataproc/npd-config/net-cgroup-system-stats-monitor.json
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: Jun 28 16:02:32 mjtelco-w-2 systemd[1]: Started Node Problem Detector.
<13>Jun 28 16:02:32 startup-script[1131]: + is_ubuntu
<13>Jun 28 16:02:32 startup-script[1131]: ++ os_id
<13>Jun 28 16:02:32 startup-script[1131]: ++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:32 startup-script[1131]: ++ cut -d= -f2
<13>Jun 28 16:02:32 startup-script[1131]: ++ xargs
<13>Jun 28 16:02:32 startup-script[1131]: + [[ debian == \u\b\u\n\t\u ]]
<13>Jun 28 16:02:32 startup-script[1131]: + configure_conscrypt
<13>Jun 28 16:02:32 startup-script[1131]: + local conscrypt_enabled
<13>Jun 28 16:02:32 startup-script[1131]: ++ get_dataproc_property dataproc.conscrypt.provider.enable
<13>Jun 28 16:02:32 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:32 startup-script[1131]: + conscrypt_enabled=true
<13>Jun 28 16:02:32 startup-script[1131]: + [[ true == \t\r\u\e ]]
<13>Jun 28 16:02:32 startup-script[1131]: + local suffix
<13>Jun 28 16:02:32 startup-script[1131]: + is_arm64
<13>Jun 28 16:02:32 startup-script[1131]: ++ uname -m
<13>Jun 28 16:02:32 startup-script[1131]: + [[ x86_64 == \a\a\r\c\h\6\4 ]]
<13>Jun 28 16:02:32 startup-script[1131]: + suffix=x86_64
<13>Jun 28 16:02:32 startup-script[1131]: + local conscrypt_jar
<13>Jun 28 16:02:32 startup-script[1131]: + local conscrypt_shared_lib=/usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so
<13>Jun 28 16:02:32 startup-script[1131]: + is_java8
<13>Jun 28 16:02:32 startup-script[1131]: + [[ /usr/lib/jvm/temurin-8-jdk-amd64 == *\-\8\-* ]]
<13>Jun 28 16:02:32 startup-script[1131]: ++ compgen -G '/usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-*.jar'
<13>Jun 28 16:02:32 startup-script[1131]: + conscrypt_jar=/usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-2.5.2-linux-x86_64.jar
<13>Jun 28 16:02:32 startup-script[1131]: + ln -s /usr/local/share/google/dataproc/conscrypt/conscrypt-openjdk-2.5.2-linux-x86_64.jar /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/ext/conscrypt-openjdk.jar
<13>Jun 28 16:02:32 startup-script[1131]: + ln -s /usr/local/share/google/dataproc/conscrypt/libconscrypt_openjdk_jni-linux-x86_64.so /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/amd64/libconscrypt_openjdk_jni.so
<13>Jun 28 16:02:32 startup-script[1131]: + cp /usr/local/share/google/dataproc/java.security.conscrypt /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/security/java.security.tmp
<13>Jun 28 16:02:32 startup-script[1131]: + mv -f /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/security/java.security.tmp /usr/lib/jvm/temurin-8-jdk-amd64/jre/lib/security/java.security
<13>Jun 28 16:02:32 startup-script[1131]: + loginfo 'Pre-activating components'
<13>Jun 28 16:02:32 startup-script[1131]: + echo 'Pre-activating components'
<13>Jun 28 16:02:32 startup-script[1131]: Pre-activating components
<13>Jun 28 16:02:32 startup-script[1131]: + pre_activate_components hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql pig spark tez yarn
<13>Jun 28 16:02:32 startup-script[1131]: + components=("$@")
<13>Jun 28 16:02:32 startup-script[1131]: + local components
<13>Jun 28 16:02:32 startup-script[1131]: + run_components_scripts pre-activate hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql pig spark tez yarn
<13>Jun 28 16:02:32 startup-script[1131]: + local -r script_type=pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + all_components=("${@:2}")
<13>Jun 28 16:02:32 startup-script[1131]: + local -r all_components
<13>Jun 28 16:02:32 startup-script[1131]: + components=()
<13>Jun 28 16:02:32 startup-script[1131]: + local components
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + components+=("${component}")
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + components+=("${component}")
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + components+=("${component}")
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mapreduce.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mapreduce.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/miniconda3.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/miniconda3.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + components+=("${component}")
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + components+=("${component}")
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + components+=("${component}")
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + components+=("${component}")
<13>Jun 28 16:02:32 startup-script[1131]: + for component in "${all_components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: + components+=("${component}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo 'Components with pre-activate script: hdfs hive-metastore hive-server2 mysql pig spark tez yarn'
<13>Jun 28 16:02:32 startup-script[1131]: Components with pre-activate script: hdfs hive-metastore hive-server2 mysql pig spark tez yarn
<13>Jun 28 16:02:32 startup-script[1131]: + local -r sentinel_dir=/tmp/dataproc/sentinel
<13>Jun 28 16:02:32 startup-script[1131]: + mkdir -p /tmp/dataproc/sentinel
<13>Jun 28 16:02:32 startup-script[1131]: + names=()
<13>Jun 28 16:02:32 startup-script[1131]: + local names
<13>Jun 28 16:02:32 startup-script[1131]: + scripts=()
<13>Jun 28 16:02:32 startup-script[1131]: + local scripts
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + for cmp in "${components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + names+=("${cmp}.${script_type}")
<13>Jun 28 16:02:32 startup-script[1131]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Jun 28 16:02:32 startup-script[1131]: + for cmp in "${components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + names+=("${cmp}.${script_type}")
<13>Jun 28 16:02:32 startup-script[1131]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Jun 28 16:02:32 startup-script[1131]: + for cmp in "${components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + names+=("${cmp}.${script_type}")
<13>Jun 28 16:02:32 startup-script[1131]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Jun 28 16:02:32 startup-script[1131]: + for cmp in "${components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + names+=("${cmp}.${script_type}")
<13>Jun 28 16:02:32 startup-script[1131]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Jun 28 16:02:32 startup-script[1131]: + for cmp in "${components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + names+=("${cmp}.${script_type}")
<13>Jun 28 16:02:32 startup-script[1131]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Jun 28 16:02:32 startup-script[1131]: + for cmp in "${components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + names+=("${cmp}.${script_type}")
<13>Jun 28 16:02:32 startup-script[1131]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Jun 28 16:02:32 startup-script[1131]: + for cmp in "${components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + names+=("${cmp}.${script_type}")
<13>Jun 28 16:02:32 startup-script[1131]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Jun 28 16:02:32 startup-script[1131]: + for cmp in "${components[@]}"
<13>Jun 28 16:02:32 startup-script[1131]: + names+=("${cmp}.${script_type}")
<13>Jun 28 16:02:32 startup-script[1131]: + scripts+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.sh")
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${BDUTIL_DIR}/components/${script_type}/${cmp}.deps")
<13>Jun 28 16:02:32 startup-script[1131]: + execute_task_graph 'hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps /usr/local/share/google/dataproc/bdutil/co
<13>Jun 28 16:02:32 startup-script[1131]: mponents/pre-activate/hive-server2.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps' /tmp/dataproc/sentinel 2.0
<13>Jun 28 16:02:32 startup-script[1131]: + echo 'Generating makefile for the task graph'
<13>Jun 28 16:02:32 startup-script[1131]: Generating makefile for the task graph
<13>Jun 28 16:02:32 startup-script[1131]: + mkdir -p /tmp/dataproc/make
<13>Jun 28 16:02:32 startup-script[1131]: + local makefile
<13>Jun 28 16:02:32 startup-script[1131]: ++ mktemp /tmp/dataproc/make/makefile.XXXXXX
<13>Jun 28 16:02:32 startup-script[1131]: + makefile=/tmp/dataproc/make/makefile.tMlPns
<13>Jun 28 16:02:32 startup-script[1131]: + generate_task_graph_makefile 'hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh' '/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps /usr/local/share/google/dataproc
<13>Jun 28 16:02:32 startup-script[1131]: /bdutil/components/pre-activate/hive-server2.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps' /tmp/dataproc/sentinel 2.0
<13>Jun 28 16:02:32 startup-script[1131]: + names=()
<13>Jun 28 16:02:32 startup-script[1131]: + local names
<13>Jun 28 16:02:32 startup-script[1131]: + read -r -a names
<13>Jun 28 16:02:32 startup-script[1131]: + scripts=()
<13>Jun 28 16:02:32 startup-script[1131]: + local scripts
<13>Jun 28 16:02:32 startup-script[1131]: + read -r -a scripts
<13>Jun 28 16:02:32 startup-script[1131]: + deps_manifests=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifests
<13>Jun 28 16:02:32 startup-script[1131]: + read -r -a deps_manifests
<13>Jun 28 16:02:32 startup-script[1131]: + local -r sentinel_dir=/tmp/dataproc/sentinel
<13>Jun 28 16:02:32 startup-script[1131]: + task_args=("${@:5}")
<13>Jun 28 16:02:32 startup-script[1131]: + local -r task_args
<13>Jun 28 16:02:32 startup-script[1131]: + targets=()
<13>Jun 28 16:02:32 startup-script[1131]: + local targets
<13>Jun 28 16:02:32 startup-script[1131]: + (( i = 0 ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local name=hdfs.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.deps ]]
<13>Jun 28 16:02:32 startup-script[1131]: + local target=/tmp/dataproc/sentinel/hdfs.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + targets+=("${target}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '/tmp/dataproc/sentinel/hdfs.pre-activate: '
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\t@echo '\''Running task: hdfs.pre-activate'\'''
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\ttouch /tmp/dataproc/sentinel/hdfs.pre-activate\n'
<13>Jun 28 16:02:32 startup-script[1131]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local name=hive-metastore.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.deps ]]
<13>Jun 28 16:02:32 startup-script[1131]: + local target=/tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + targets+=("${target}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '/tmp/dataproc/sentinel/hive-metastore.pre-activate: '
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\t@echo '\''Running task: hive-metastore.pre-activate'\'''
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\ttouch /tmp/dataproc/sentinel/hive-metastore.pre-activate\n'
<13>Jun 28 16:02:32 startup-script[1131]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local name=hive-server2.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps ]]
<13>Jun 28 16:02:32 startup-script[1131]: + dep_names=($(cat "${deps_manifest}"))
<13>Jun 28 16:02:32 startup-script[1131]: ++ cat /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.deps
<13>Jun 28 16:02:32 startup-script[1131]: + local dep_names
<13>Jun 28 16:02:32 startup-script[1131]: + (( j = 0 ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( j < 4 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local dep_name=mysql.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  mysql\.pre-activate  ]]
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Jun 28 16:02:32 startup-script[1131]: + (( j++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( j < 4 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local dep_name=hdfs.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  hdfs\.pre-activate  ]]
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Jun 28 16:02:32 startup-script[1131]: + (( j++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( j < 4 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local dep_name=hive-metastore.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  hive-metastore\.pre-activate  ]]
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Jun 28 16:02:32 startup-script[1131]: + (( j++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( j < 4 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local dep_name=tez.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + [[  hdfs.pre-activate hive-metastore.pre-activate hive-server2.pre-activate mysql.pre-activate pig.pre-activate spark.pre-activate tez.pre-activate yarn.pre-activate  =~  tez\.pre-activate  ]]
<13>Jun 28 16:02:32 startup-script[1131]: + deps+=("${sentinel_dir}/${dep_name}")
<13>Jun 28 16:02:32 startup-script[1131]: + (( j++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( j < 4 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local target=/tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + targets+=("${target}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '/tmp/dataproc/sentinel/hive-server2.pre-activate: /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/tez.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\t@echo '\''Running task: hive-server2.pre-activate'\'''
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\ttouch /tmp/dataproc/sentinel/hive-server2.pre-activate\n'
<13>Jun 28 16:02:32 startup-script[1131]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local name=mysql.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.deps ]]
<13>Jun 28 16:02:32 startup-script[1131]: + local target=/tmp/dataproc/sentinel/mysql.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + targets+=("${target}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '/tmp/dataproc/sentinel/mysql.pre-activate: '
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\t@echo '\''Running task: mysql.pre-activate'\'''
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\ttouch /tmp/dataproc/sentinel/mysql.pre-activate\n'
<13>Jun 28 16:02:32 startup-script[1131]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local name=pig.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.deps ]]
<13>Jun 28 16:02:32 startup-script[1131]: + local target=/tmp/dataproc/sentinel/pig.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + targets+=("${target}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '/tmp/dataproc/sentinel/pig.pre-activate: '
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\t@echo '\''Running task: pig.pre-activate'\'''
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\ttouch /tmp/dataproc/sentinel/pig.pre-activate\n'
<13>Jun 28 16:02:32 startup-script[1131]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local name=spark.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.deps ]]
<13>Jun 28 16:02:32 startup-script[1131]: + local target=/tmp/dataproc/sentinel/spark.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + targets+=("${target}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '/tmp/dataproc/sentinel/spark.pre-activate: '
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\t@echo '\''Running task: spark.pre-activate'\'''
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\ttouch /tmp/dataproc/sentinel/spark.pre-activate\n'
<13>Jun 28 16:02:32 startup-script[1131]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local name=tez.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.deps ]]
<13>Jun 28 16:02:32 startup-script[1131]: + local target=/tmp/dataproc/sentinel/tez.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + targets+=("${target}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '/tmp/dataproc/sentinel/tez.pre-activate: '
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\t@echo '\''Running task: tez.pre-activate'\'''
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\ttouch /tmp/dataproc/sentinel/tez.pre-activate\n'
<13>Jun 28 16:02:32 startup-script[1131]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + local name=yarn.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + local script=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:32 startup-script[1131]: + local deps_manifest=/usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps
<13>Jun 28 16:02:32 startup-script[1131]: + deps=()
<13>Jun 28 16:02:32 startup-script[1131]: + local deps
<13>Jun 28 16:02:32 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.deps ]]
<13>Jun 28 16:02:32 startup-script[1131]: + local target=/tmp/dataproc/sentinel/yarn.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + targets+=("${target}")
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '/tmp/dataproc/sentinel/yarn.pre-activate: '
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\t@echo '\''Running task: yarn.pre-activate'\'''
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\tbash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }'
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e '\ttouch /tmp/dataproc/sentinel/yarn.pre-activate\n'
<13>Jun 28 16:02:32 startup-script[1131]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: + (( i < 8 ))
<13>Jun 28 16:02:32 startup-script[1131]: + echo -e 'all: /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/hive-server2.pre-activate /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/pig.pre-activate /tmp/dataproc/sentinel/spark.pre-activate /tmp/dataproc/sentinel/tez.pre-activate /tmp/dataproc/sentinel/yarn.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: + echo 'Generated makefile:'
<13>Jun 28 16:02:32 startup-script[1131]: Generated makefile:
<13>Jun 28 16:02:32 startup-script[1131]: + cat /tmp/dataproc/make/makefile.tMlPns
<13>Jun 28 16:02:32 startup-script[1131]: /tmp/dataproc/sentinel/hdfs.pre-activate: 
<13>Jun 28 16:02:32 startup-script[1131]: 	@echo 'Running task: hdfs.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: 	touch /tmp/dataproc/sentinel/hdfs.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: /tmp/dataproc/sentinel/hive-metastore.pre-activate: 
<13>Jun 28 16:02:32 startup-script[1131]: 	@echo 'Running task: hive-metastore.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: 	touch /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: /tmp/dataproc/sentinel/hive-server2.pre-activate: /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/tez.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 	@echo 'Running task: hive-server2.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: 	touch /tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: /tmp/dataproc/sentinel/mysql.pre-activate: 
<13>Jun 28 16:02:32 startup-script[1131]: 	@echo 'Running task: mysql.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: 	touch /tmp/dataproc/sentinel/mysql.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: /tmp/dataproc/sentinel/pig.pre-activate: 
<13>Jun 28 16:02:32 startup-script[1131]: 	@echo 'Running task: pig.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: 	touch /tmp/dataproc/sentinel/pig.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: /tmp/dataproc/sentinel/spark.pre-activate: 
<13>Jun 28 16:02:32 startup-script[1131]: 	@echo 'Running task: spark.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: 	touch /tmp/dataproc/sentinel/spark.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: /tmp/dataproc/sentinel/tez.pre-activate: 
<13>Jun 28 16:02:32 startup-script[1131]: 	@echo 'Running task: tez.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: 	touch /tmp/dataproc/sentinel/tez.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: /tmp/dataproc/sentinel/yarn.pre-activate: 
<13>Jun 28 16:02:32 startup-script[1131]: 	@echo 'Running task: yarn.pre-activate'
<13>Jun 28 16:02:32 startup-script[1131]: 	bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: 	touch /tmp/dataproc/sentinel/yarn.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: 
<13>Jun 28 16:02:32 startup-script[1131]: all: /tmp/dataproc/sentinel/hdfs.pre-activate /tmp/dataproc/sentinel/hive-metastore.pre-activate /tmp/dataproc/sentinel/hive-server2.pre-activate /tmp/dataproc/sentinel/mysql.pre-activate /tmp/dataproc/sentinel/pig.pre-activate /tmp/dataproc/sentinel/spark.pre-activate /tmp/dataproc/sentinel/tez.pre-activate /tmp/dataproc/sentinel/yarn.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: + echo 'Running task graph:'
<13>Jun 28 16:02:32 startup-script[1131]: Running task graph:
<13>Jun 28 16:02:32 startup-script[1131]: + make -f /tmp/dataproc/make/makefile.tMlPns all
<13>Jun 28 16:02:32 startup-script[1131]: Running task: hdfs.pre-activate
<13>Jun 28 16:02:32 startup-script[1131]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh"; exit 1; }
<13>Jun 28 16:02:32 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:32 startup-script[1131]: + set -a
<13>Jun 28 16:02:32 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:32 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:32 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:32 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:32 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:32 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:32 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:32 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:32 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:32 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:32 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:32 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:32 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:32 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:32 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:32 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:32 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:32 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:32 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:32 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:32 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:32 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:32 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:32 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:32 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:32 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:32 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:32 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:32 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:32 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:32 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:32 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:32 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:32 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:32 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:32 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:32 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:32 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:32 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:32 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:32 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:32 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:32 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:32 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:32 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:32 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:32 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:32 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:32 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:32 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:32 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:32 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:32 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:32 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:32 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:32 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:32 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:32 startup-script[1131]: + set +a
<13>Jun 28 16:02:32 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_logging.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:32 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:32 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:32 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:32 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_properties.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_packages.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_components.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_misc.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:32 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hdfs.sh
<13>Jun 28 16:02:32 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_versions.sh
<13>Jun 28 16:02:32 startup-script[1131]: + set -x
<13>Jun 28 16:02:32 startup-script[1131]: + set_log_tag pre-activate-component-hdfs
<13>Jun 28 16:02:32 startup-script[1131]: + local -r tag=pre-activate-component-hdfs
<13>Jun 28 16:02:32 startup-script[1131]: + exec
<13>Jun 28 16:02:32 startup-script[1131]: ++ logger -s -t 'pre-activate-component-hdfs[2139]'
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + readonly HDFS_ADMIN=hdfs
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_ADMIN=hdfs
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + export HDFS_NAME_DIR=/hadoop/dfs/name
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_NAME_DIR=/hadoop/dfs/name
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + export HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_SECONDARY_NAME_DIR=/hadoop/dfs/namesecondary
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_metadata_cluster_name
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ set +x
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + CLUSTER_NAME=mjtelco
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + export CLUSTER_NAME
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_metadata_master
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ set +x
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_metadata_master_additional
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ set +x
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + DATAPROC_MASTER_ADDITIONAL=
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + NUM_MASTERS=1
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_metadata_worker_count
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ set +x
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + NUM_WORKERS=5
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + DATANODE_PACKAGES=('hadoop-hdfs-datanode')
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + THIRD_MASTER_OPTIONAL_PACKAGES=('hadoop-hdfs-namenode' 'hadoop-hdfs-zkfc')
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + (( i = 0 ))
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + (( i < 1 ))
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + declare MASTER_HOSTNAME_0=mjtelco-m
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + export MASTER_HOSTNAME_0
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + (( i++ ))
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + (( i < 1 ))
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ is_component_selected kerberos
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ local -r component=kerberos
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ local activated_components
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: +++ get_components_to_activate
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: +++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: +++ set +x
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: +++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ echo false
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + KERBEROS_ENABLED=false
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + export ENABLE_HDFS_PERMISSIONS=false
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + ENABLE_HDFS_PERMISSIONS=false
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + DATA_DIRS_ARRAY=($(get_data_dirs))
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ get_data_dirs
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ local -a mount_points
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ mapfile -t mount_points
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: +++ find '/mnt/[0-9]*/' -maxdepth 0
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: find: ‘/mnt/[0-9]*/’: No such file or directory
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: +++ true
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ (( 0 ))
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ echo /
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ return
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_DIRS=/hadoop/dfs
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_DIRS_ARRAY=(${HDFS_DIRS})
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_DATA_DIRS_ARRAY=(${HDFS_DATA_DIRS})
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + mkdir -p /hadoop/dfs /hadoop/dfs/data
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + chown -L -R hdfs:hadoop /hadoop/dfs /hadoop/dfs
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + chmod -R 700 /hadoop/dfs
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ free -m
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ awk '/^Mem:/{print $2}'
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + TOTAL_MEM=7452
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ python -c 'print(int(7452 * 0.4 / 2))'
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + NAMENODE_MEM_MB=1490
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + SECONDARYNAMENODE_MEM_MB=1490
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + is_version_at_least 2.0 2.1
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + set +x
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: Comparing if version 2.0 is at least version 2.1 
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + case ${compare_versions_result} in
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + return 1
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + GC_LOG_OPTS='-XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails'
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + cat
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + export HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_DATA_DIRS=/hadoop/dfs/data
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + [[ 1 -gt 1 ]]
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + readonly HDFS_TEMPLATE=hdfs-template.xml
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_TEMPLATE=hdfs-template.xml
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file hdfs-template.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + (( NUM_WORKERS == 0 ))
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + [[ 1 -gt 1 ]]
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + readonly HDFS_SIMPLIFICATION_MIXINS=hdfs-simplification-mixins.xml
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + HDFS_SIMPLIFICATION_MIXINS=hdfs-simplification-mixins.xml
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /usr/local/share/google/dataproc/bdutil/conf/hdfs-simplification-mixins.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + update_hostname_in_xml
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + local xml_filename=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + local host_name
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + local domain
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ dnsdomainname
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + domain=us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + host_name=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.http-address mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + local prop_name=dfs.namenode.http-address
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + local host_name=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + local updated_value
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + local prop_value
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.http-address
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: I0628 16:02:32.982690 140549876131648 xml_config_commands.py:181] Property value is: "mjtelco-m:50070"
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + prop_value=mjtelco-m:50070
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + [[ mjtelco-m:50070 != \N\o\n\e ]]
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + loginfo 'Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Jun 28 16:02:32 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: + echo 'Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:32 pre-activate-component-hdfs[2139]: Reading dfs.namenode.http-address in /etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ echo mjtelco-m:50070
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ sed -e s/0.0.0.0/mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal/g
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + updated_value=mjtelco-m:50070
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ mjtelco-m:50070 != \m\j\t\e\l\c\o\-\m\:\5\0\0\7\0 ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.https-address mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_name=dfs.namenode.https-address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local host_name=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local updated_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.https-address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: I0628 16:02:33.128057 139767388567360 xml_config_commands.py:181] Property value is: "None"
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + prop_value=None
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ None != \N\o\n\e ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.secondary.http-address mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_name=dfs.namenode.secondary.http-address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local host_name=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local updated_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.secondary.http-address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: I0628 16:02:33.266683 139637072545600 xml_config_commands.py:181] Property value is: "mjtelco-m:50090"
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + prop_value=mjtelco-m:50090
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ mjtelco-m:50090 != \N\o\n\e ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + loginfo 'Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + echo 'Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml'
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: Reading dfs.namenode.secondary.http-address in /etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ echo mjtelco-m:50090
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ sed -e s/0.0.0.0/mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal/g
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + updated_value=mjtelco-m:50090
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ mjtelco-m:50090 != \m\j\t\e\l\c\o\-\m\:\5\0\0\9\0 ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.namenode.secondary.https-address mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_name=dfs.namenode.secondary.https-address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local host_name=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local updated_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.namenode.secondary.https-address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: I0628 16:02:33.415953 140649850156864 xml_config_commands.py:181] Property value is: "None"
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + prop_value=None
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ None != \N\o\n\e ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ hostname -f
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + host_name=mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.address mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_name=dfs.datanode.address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local host_name=mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local updated_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: I0628 16:02:33.555714 139636156335936 xml_config_commands.py:181] Property value is: "None"
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + prop_value=None
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ None != \N\o\n\e ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.http.address mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_name=dfs.datanode.http.address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local host_name=mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local updated_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.http.address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: I0628 16:02:33.696599 139841643378496 xml_config_commands.py:181] Property value is: "None"
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + prop_value=None
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ None != \N\o\n\e ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.https.address mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_name=dfs.datanode.https.address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local host_name=mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local updated_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.https.address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: I0628 16:02:33.836425 140185343489856 xml_config_commands.py:181] Property value is: "None"
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + prop_value=None
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ None != \N\o\n\e ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + update_hostname_for_property /etc/hadoop/conf/hdfs-site.xml dfs.datanode.ipc.address mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local xml_file=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_name=dfs.datanode.ipc.address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local host_name=mjtelco-w-2.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local updated_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local prop_value
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: ++ bdconfig get_property_value --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.datanode.ipc.address
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: I0628 16:02:33.977316 140509334046528 xml_config_commands.py:181] Property value is: "None"
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + prop_value=None
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ None != \N\o\n\e ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + merge_xml_properties /tmp/cluster/properties/hdfs.xml /etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local src=/tmp/cluster/properties/hdfs.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + local dest=/etc/hadoop/conf/hdfs-site.xml
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + [[ ! -f /tmp/cluster/properties/hdfs.xml ]]
<13>Jun 28 16:02:33 startup-script[1131]: <13>Jun 28 16:02:33 pre-activate-component-hdfs[2139]: + bdconfig merge_configurations --configuration_file /etc/hadoop/conf/hdfs-site.xml --source_configuration_file /tmp/cluster/properties/hdfs.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + loginfo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + echo 'Merged /tmp/cluster/properties/hdfs.xml.'
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: Merged /tmp/cluster/properties/hdfs.xml.
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ create_or_validate_include_file_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ local include_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ get_include_file_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ local include_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-include-file-location
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ echo ''
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ include_path=
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ [[ -z '' ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ get_include_file_path_deprecated
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ local include_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ [[ 1 -eq 1 ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ include_path=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ echo /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ include_path=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ echo /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ include_path=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ [[ /etc/hadoop/conf/nodes_include == gs://* ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ [[ ! -f /etc/hadoop/conf/nodes_include ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ echo /etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + INCLUDE_PATH=/etc/hadoop/conf/nodes_include
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ create_or_validate_exclude_file_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ local exclude_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ get_exclude_file_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ local exclude_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ /usr/share/google/get_metadata_value attributes/dataproc-exclude-file-location
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ echo ''
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ exclude_path=
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ [[ -z '' ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ get_exclude_file_path_deprecated
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ local exclude_path
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ MASTER_RUN_DRIVER_LOCATION=LOCAL
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ [[ LOCAL == \Y\A\R\N ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ [[ 1 -eq 1 ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ exclude_path=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++++ echo /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ exclude_path=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: +++ echo /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ exclude_path=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ [[ /etc/hadoop/conf/nodes_exclude == gs://* ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ [[ ! -f /etc/hadoop/conf/nodes_exclude ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ echo /etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + EXCLUDE_PATH=/etc/hadoop/conf/nodes_exclude
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.hosts --value /etc/hadoop/conf/nodes_include --clobber
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + bdconfig set_property --configuration_file /etc/hadoop/conf/hdfs-site.xml --name dfs.hosts.exclude --value /etc/hadoop/conf/nodes_exclude --clobber
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + [[ false == \t\r\u\e ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ get_metadata_datanode_enabled
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ set +x
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + [[ true != \t\r\u\e ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ get_metadata_role
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: ++ set +x
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hdfs[2139]: + [[ Worker == \M\a\s\t\e\r ]]
<13>Jun 28 16:02:34 startup-script[1131]: touch /tmp/dataproc/sentinel/hdfs.pre-activate
<13>Jun 28 16:02:34 startup-script[1131]: Running task: hive-metastore.pre-activate
<13>Jun 28 16:02:34 startup-script[1131]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh"; exit 1; }
<13>Jun 28 16:02:34 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:34 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:34 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:34 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:34 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:34 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:34 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:34 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:34 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:34 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:34 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:34 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:34 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:34 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:34 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:34 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:34 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:34 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:34 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:34 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:34 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:34 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:34 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:34 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:34 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:34 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:34 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:34 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:34 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:34 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:34 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:34 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:34 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:34 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:34 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:34 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:34 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:34 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:34 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:34 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:34 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:34 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:34 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:34 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:34 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:34 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:34 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:34 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:34 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:34 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:34 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:34 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:34 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_gcs.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_logging.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:34 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:34 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:34 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:34 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_components.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_properties.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ set -euo pipefail
<13>Jun 28 16:02:34 startup-script[1131]: +++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-metastore.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:34 startup-script[1131]: ++ set -x
<13>Jun 28 16:02:34 startup-script[1131]: +++ get_metadata_master
<13>Jun 28 16:02:34 startup-script[1131]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:34 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:34 startup-script[1131]: +++ get_metadata_master_additional
<13>Jun 28 16:02:34 startup-script[1131]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Jun 28 16:02:34 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:34 startup-script[1131]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Jun 28 16:02:34 startup-script[1131]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:34 startup-script[1131]: ++ NUM_MASTERS=1
<13>Jun 28 16:02:34 startup-script[1131]: +++ get_metadata_role
<13>Jun 28 16:02:34 startup-script[1131]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Jun 28 16:02:34 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:34 startup-script[1131]: ++ ROLE=Worker
<13>Jun 28 16:02:34 startup-script[1131]: ++ [[ 1 -gt 1 ]]
<13>Jun 28 16:02:34 startup-script[1131]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://mjtelco-m:9083
<13>Jun 28 16:02:34 startup-script[1131]: + set -x
<13>Jun 28 16:02:34 startup-script[1131]: + set_log_tag pre-activate-component-hive-metastore
<13>Jun 28 16:02:34 startup-script[1131]: + local -r tag=pre-activate-component-hive-metastore
<13>Jun 28 16:02:34 startup-script[1131]: + exec
<13>Jun 28 16:02:34 startup-script[1131]: ++ logger -s -t 'pre-activate-component-hive-metastore[2333]'
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.metastore.uris --value thrift://mjtelco-m:9083 --clobber
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + is_component_selected hdfs
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + local -r component=hdfs
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + local activated_components
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: ++ get_components_to_activate
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: ++ set +x
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *hdfs* ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: ++ get_property_in_xml /etc/hive/conf/hive-site.xml hive.metastore.warehouse.dir
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: ++ set +x
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + HIVE_WAREHOUSE_DIR=
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + [[ '' == \g\s\:\/\/* ]]
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + METADATASTORE_JDBC_URI=jdbc:mysql://mjtelco-m/metastore
<13>Jun 28 16:02:34 startup-script[1131]: <13>Jun 28 16:02:34 pre-activate-component-hive-metastore[2333]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name javax.jdo.option.ConnectionURL --value jdbc:mysql://mjtelco-m/metastore --clobber
<13>Jun 28 16:02:35 startup-script[1131]: touch /tmp/dataproc/sentinel/hive-metastore.pre-activate
<13>Jun 28 16:02:35 startup-script[1131]: Running task: mysql.pre-activate
<13>Jun 28 16:02:35 startup-script[1131]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh"; exit 1; }
<13>Jun 28 16:02:35 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:35 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:35 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:35 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:35 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:35 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:35 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:35 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:35 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:35 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:35 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:35 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:35 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:35 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:35 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:35 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:35 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:35 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:35 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:35 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:35 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:35 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:35 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:35 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:35 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:35 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:35 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:35 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:35 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:35 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:35 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:35 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:35 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:35 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:35 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:35 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:35 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:35 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_helpers.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:35 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:35 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:35 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:35 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:35 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:35 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:35 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:35 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:35 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/mysql.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/mysql.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ MYSQL_DEFAULT_ROOT_PASSWORD=Root+password1
<13>Jun 28 16:02:35 startup-script[1131]: ++ is_version_at_least 2.0 2.1
<13>Jun 28 16:02:35 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:35 startup-script[1131]: Comparing if version 2.0 is at least version 2.1 
<13>Jun 28 16:02:35 startup-script[1131]: ++ case ${compare_versions_result} in
<13>Jun 28 16:02:35 startup-script[1131]: ++ return 1
<13>Jun 28 16:02:35 startup-script[1131]: ++ MYSQL_VERSION=5.7
<13>Jun 28 16:02:35 startup-script[1131]: ++ MYSQL_EL_VERSION=7
<13>Jun 28 16:02:35 startup-script[1131]: + set -x
<13>Jun 28 16:02:35 startup-script[1131]: + set_log_tag pre-activate-component-mysql
<13>Jun 28 16:02:35 startup-script[1131]: + local -r tag=pre-activate-component-mysql
<13>Jun 28 16:02:35 startup-script[1131]: + exec
<13>Jun 28 16:02:35 startup-script[1131]: ++ logger -s -t 'pre-activate-component-mysql[2368]'
<13>Jun 28 16:02:35 startup-script[1131]: touch /tmp/dataproc/sentinel/mysql.pre-activate
<13>Jun 28 16:02:35 startup-script[1131]: Running task: tez.pre-activate
<13>Jun 28 16:02:35 startup-script[1131]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh"; exit 1; }
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-mysql[2368]: + [[ mjtelco-w-2 != \m\j\t\e\l\c\o\-\m ]]
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-mysql[2368]: + echo 'Skip running MySQL on mjtelco-w-2'
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-mysql[2368]: Skip running MySQL on mjtelco-w-2
<13>Jun 28 16:02:35 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:35 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:35 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:35 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:35 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:35 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:35 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:35 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:35 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:35 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:35 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:35 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:35 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:35 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:35 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:35 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:35 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:35 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:35 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:35 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:35 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:35 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:35 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:35 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:35 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:35 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:35 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:35 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:35 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:35 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:35 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:35 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:35 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:35 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:35 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:35 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:35 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:35 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:35 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:35 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:35 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_logging.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:35 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:35 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_properties.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_components.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/tez.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_versions.sh
<13>Jun 28 16:02:35 startup-script[1131]: + source /usr/local/share/google/dataproc/dataproc_env.sh
<13>Jun 28 16:02:35 startup-script[1131]: ++ CLUSTER_NAME=mjtelco
<13>Jun 28 16:02:35 startup-script[1131]: ++ CLUSTER_UUID=7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:35 startup-script[1131]: ++ CONFIGBUCKET=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:35 startup-script[1131]: ++ TEMP_BUCKET=dataproc-temp-us-east1-746779145865-aaezsxz3
<13>Jun 28 16:02:35 startup-script[1131]: ++ HCFS_ROOT_URI=hdfs://mjtelco-m
<13>Jun 28 16:02:35 startup-script[1131]: ++ MASTER_HOSTNAME_0=mjtelco-m
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:35 startup-script[1131]: ++ DATAPROC_MASTER_FQDN=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:35 startup-script[1131]: ++ MASTER_HOSTNAMES=(mjtelco-m)
<13>Jun 28 16:02:35 startup-script[1131]: ++ NUM_MASTERS=1
<13>Jun 28 16:02:35 startup-script[1131]: ++ NUM_WORKERS=5
<13>Jun 28 16:02:35 startup-script[1131]: ++ PREFIX=mjtelco
<13>Jun 28 16:02:35 startup-script[1131]: ++ PROJECT=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:35 startup-script[1131]: ++ ROLE=Worker
<13>Jun 28 16:02:35 startup-script[1131]: + set -x
<13>Jun 28 16:02:35 startup-script[1131]: + set_log_tag pre-activate-component-tez
<13>Jun 28 16:02:35 startup-script[1131]: + local -r tag=pre-activate-component-tez
<13>Jun 28 16:02:35 startup-script[1131]: + exec
<13>Jun 28 16:02:35 startup-script[1131]: ++ logger -s -t 'pre-activate-component-tez[2379]'
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ ls /usr/lib/tez/tez-ui-0.9.2.war
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + readonly TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + TEZ_UI_WAR=/usr/lib/tez/tez-ui-0.9.2.war
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + pre_activate_tez
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + configure_ui_war
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local tmp_dir
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ mktemp -d
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + tmp_dir=/tmp/tmp.NuH8NUB5fh
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + unzip -q /usr/lib/tez/tez-ui-0.9.2.war -d /tmp/tmp.NuH8NUB5fh
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local -r tez_configs=/tmp/tmp.NuH8NUB5fh/config/configs.env
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local -r tez_ui_js=/tmp/tmp.NuH8NUB5fh/assets/tez-ui.js
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local ats_v2_port=8192
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + is_component_selected kerberos
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local -r component=kerberos
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local activated_components
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ get_components_to_activate
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ set +x
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local ats_v2_enabled=false
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ set +x
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + [[ -n '' ]]
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + [[ -f /tmp/tmp.NuH8NUB5fh/config/configs.env ]]
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + sed -i 's#\(.*\)//atsV2Enabled: true\(.*\)#\1atsV2Enabled: false\2#' /tmp/tmp.NuH8NUB5fh/config/configs.env
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + sed -i 's#\(.*\)//timeline: "http://localhost:8188"\(.*\)#\1timeline: "http://mjtelco-w-2:8188"\2#' /tmp/tmp.NuH8NUB5fh/config/configs.env
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + sed -i 's#\(.*\)//timelineV2: "http://localhost:8192"\(.*\)#\1timelineV2: "http://mjtelco-w-2:8192"\2#' /tmp/tmp.NuH8NUB5fh/config/configs.env
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + sed -i 's#\(.*\)//rm: "http://localhost:8088"\(.*\)#\1rm: "http://mjtelco-w-2:8088"\2#' /tmp/tmp.NuH8NUB5fh/config/configs.env
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + [[ -f /tmp/tmp.NuH8NUB5fh/assets/tez-ui.js ]]
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + sed -i 's#"timeline":"localhost:8188"#"timeline":"mjtelco-w-2:8188"#' /tmp/tmp.NuH8NUB5fh/assets/tez-ui.js
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + sed -i 's#"timelineV2":"localhost:8192"#"timelineV2":"mjtelco-w-2:8192"#' /tmp/tmp.NuH8NUB5fh/assets/tez-ui.js
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + sed -i 's#"rm":"localhost:8088"#"rm":"mjtelco-w-2:8088"#' /tmp/tmp.NuH8NUB5fh/assets/tez-ui.js
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + is_component_selected knox
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local -r component=knox
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + local activated_components
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ get_components_to_activate
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ set +x
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *knox* ]]
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + cd /tmp/tmp.NuH8NUB5fh
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + zip -q /usr/lib/tez/tez-ui-0.9.2.war -r ./META-INF ./WEB-INF ./assets ./config ./fonts ./index.html
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + cd ..
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + rm -rf /tmp/tmp.NuH8NUB5fh
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: ++ stat /usr/lib/tez/tez-common-0.9.2.jar --format=%Y
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + touch -d @1685713551 /usr/lib/tez/tez-ui-0.9.2.war
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + configure_yarn_for_tez
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-names --value tez --clobber
<13>Jun 28 16:02:35 startup-script[1131]: <13>Jun 28 16:02:35 pre-activate-component-tez[2379]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-on-disk-path.tez --value /usr/lib/tez/tez-ui-0.9.2.war --clobber
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + bdconfig set_property --configuration_file /etc/hadoop/conf/yarn-site.xml --name yarn.timeline-service.ui-web-path.tez --value /tez-ui --clobber
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + configure_tez
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + local history_logging_service_class=ATSHistoryLoggingService
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: ++ set +x
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + [[ -n '' ]]
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.history.logging.service.class --value org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService --clobber
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.tez-ui.history-url.base --value http://mjtelco-m:8188/tez-ui/ --clobber
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + bdconfig set_property --configuration_file /etc/tez/conf/tez-site.xml --name tez.am.node-blacklisting.enabled --value false --clobber
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + merge_user_properties
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + [[ -f /etc/tez/conf/tez-site.xml ]]
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + merge_xml_properties /tmp/cluster/properties/tez.xml /etc/tez/conf/tez-site.xml
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + local src=/tmp/cluster/properties/tez.xml
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + local dest=/etc/tez/conf/tez-site.xml
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + [[ ! -f /tmp/cluster/properties/tez.xml ]]
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + bdconfig merge_configurations --configuration_file /etc/tez/conf/tez-site.xml --source_configuration_file /tmp/cluster/properties/tez.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + loginfo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: + echo 'Merged /tmp/cluster/properties/tez.xml.'
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-tez[2379]: Merged /tmp/cluster/properties/tez.xml.
<13>Jun 28 16:02:36 startup-script[1131]: touch /tmp/dataproc/sentinel/tez.pre-activate
<13>Jun 28 16:02:36 startup-script[1131]: Running task: hive-server2.pre-activate
<13>Jun 28 16:02:36 startup-script[1131]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh"; exit 1; }
<13>Jun 28 16:02:36 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:36 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Jun 28 16:02:36 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Jun 28 16:02:36 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:36 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:36 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:36 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:36 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:36 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:36 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:36 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:36 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:36 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:36 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:36 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:36 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:36 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:36 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:36 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:36 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:36 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:36 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:36 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:36 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:36 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:36 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:36 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:36 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:36 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:36 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:36 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:36 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:36 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:36 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:36 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:36 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:36 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:36 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:36 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:36 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:36 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:36 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:36 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:36 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:36 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:36 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:36 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:36 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:36 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:36 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:36 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:36 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:36 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:36 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:36 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:36 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:36 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:36 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:36 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:36 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:36 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:36 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:36 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Jun 28 16:02:36 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_logging.sh
<13>Jun 28 16:02:36 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:36 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:36 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:36 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:36 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Jun 28 16:02:36 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:36 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Jun 28 16:02:36 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_properties.sh
<13>Jun 28 16:02:36 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/hive-server2.sh
<13>Jun 28 16:02:36 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_versions.sh
<13>Jun 28 16:02:36 startup-script[1131]: + set -x
<13>Jun 28 16:02:36 startup-script[1131]: + set_log_tag pre-activate-component-hive-server2
<13>Jun 28 16:02:36 startup-script[1131]: + local -r tag=pre-activate-component-hive-server2
<13>Jun 28 16:02:36 startup-script[1131]: + exec
<13>Jun 28 16:02:36 startup-script[1131]: ++ logger -s -t 'pre-activate-component-hive-server2[2457]'
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + CLUSTER_PROPERTIES_DIR=/tmp/cluster/properties
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ get_metadata_bucket
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ get_dataproc_metadata DATAPROC_METADATA_BUCKET attributes/dataproc-bucket
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ set +x
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + CONFIGBUCKET=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ get_metadata_cluster_uuid
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_UUID attributes/dataproc-cluster-uuid
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ set +x
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + CLUSTER_UUID=7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ get_metadata_master
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ set +x
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ get_metadata_master_additional
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: ++ set +x
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + DATAPROC_MASTER_ADDITIONAL=
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + NUM_MASTERS=1
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + [[ 1 -gt 1 ]]
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + bdconfig set_property --configuration_file /etc/hive/conf/hive-site.xml --name hive.user.install.directory --value gs://qwiklabs-gcp-03-e04e71dd72c2/google-cloud-dataproc-metainfo/7db0aaf5-a842-4945-a93b-c6d17e834515/hive/user-install-dir --clobber
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + is_version_at_least 2.0 2.1
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + set +x
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: Comparing if version 2.0 is at least version 2.1 
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + case ${compare_versions_result} in
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + return 1
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + merge_xml_properties /tmp/cluster/properties/hive.xml /etc/hive/conf/hive-site.xml
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + local src=/tmp/cluster/properties/hive.xml
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + local dest=/etc/hive/conf/hive-site.xml
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + [[ ! -f /tmp/cluster/properties/hive.xml ]]
<13>Jun 28 16:02:36 startup-script[1131]: <13>Jun 28 16:02:36 pre-activate-component-hive-server2[2457]: + bdconfig merge_configurations --configuration_file /etc/hive/conf/hive-site.xml --source_configuration_file /tmp/cluster/properties/hive.xml --resolve_environment_variables --create_if_absent --clobber
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + loginfo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + echo 'Merged /tmp/cluster/properties/hive.xml.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: Merged /tmp/cluster/properties/hive.xml.
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + merge_java_properties /tmp/cluster/properties/hive-log4j2.properties /etc/hive/conf/hive-log4j2.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + local -r src=/tmp/cluster/properties/hive-log4j2.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + local -r dest=/etc/hive/conf/hive-log4j2.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + local -r 'header=\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + [[ ! -f /tmp/cluster/properties/hive-log4j2.properties ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + cat /tmp/cluster/properties/hive-log4j2.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + loginfo 'Merged /tmp/cluster/properties/hive-log4j2.properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: + echo 'Merged /tmp/cluster/properties/hive-log4j2.properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-hive-server2[2457]: Merged /tmp/cluster/properties/hive-log4j2.properties.
<13>Jun 28 16:02:37 startup-script[1131]: touch /tmp/dataproc/sentinel/hive-server2.pre-activate
<13>Jun 28 16:02:37 startup-script[1131]: Running task: pig.pre-activate
<13>Jun 28 16:02:37 startup-script[1131]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh"; exit 1; }
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/pig.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_properties.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + set_log_tag pre-activate-component-pig
<13>Jun 28 16:02:37 startup-script[1131]: + local -r tag=pre-activate-component-pig
<13>Jun 28 16:02:37 startup-script[1131]: + exec
<13>Jun 28 16:02:37 startup-script[1131]: ++ logger -s -t 'pre-activate-component-pig[2506]'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + PIG_CONF_DIR=/etc/pig/conf
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + pre_activate_pig
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + update_hcat_file_location
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + merge_user_properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + merge_java_properties /tmp/cluster/properties/pig.properties /etc/pig/conf/pig.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + local -r src=/tmp/cluster/properties/pig.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + local -r dest=/etc/pig/conf/pig.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + local -r 'header=\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + [[ ! -f /tmp/cluster/properties/pig.properties ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + cat /tmp/cluster/properties/pig.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + loginfo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + echo 'Merged /tmp/cluster/properties/pig.properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: Merged /tmp/cluster/properties/pig.properties.
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + copy_log4j_config
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: ++ get_java_property /etc/pig/conf/pig.properties log4jconf
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + pig_log4j_location=
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-pig[2506]: + [[ -n '' ]]
<13>Jun 28 16:02:37 startup-script[1131]: touch /tmp/dataproc/sentinel/pig.pre-activate
<13>Jun 28 16:02:37 startup-script[1131]: Running task: spark.pre-activate
<13>Jun 28 16:02:37 startup-script[1131]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh"; exit 1; }
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_components.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_misc.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_networking.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:37 startup-script[1131]: ++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_os.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ is_rocky
<13>Jun 28 16:02:37 startup-script[1131]: +++ os_id
<13>Jun 28 16:02:37 startup-script[1131]: +++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:37 startup-script[1131]: +++ cut -d= -f2
<13>Jun 28 16:02:37 startup-script[1131]: +++ xargs
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_properties.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_retry.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_versions.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/hdfs.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ HCFS_DIRS=("/ hdfs:hadoop 1777" "/tmp hdfs:hadoop 1777" "/tmp/hadoop-yarn/staging yarn:hadoop 1777" "/tmp/hadoop-yarn/staging/history yarn:hadoop 755" "/user hdfs:hadoop 755" "/var hdfs:hadoop 775" "/var/tmp hdfs:hadoop 1777")
<13>Jun 28 16:02:37 startup-script[1131]: +++ is_component_selected kerberos
<13>Jun 28 16:02:37 startup-script[1131]: +++ local -r component=kerberos
<13>Jun 28 16:02:37 startup-script[1131]: +++ local activated_components
<13>Jun 28 16:02:37 startup-script[1131]: ++++ get_components_to_activate
<13>Jun 28 16:02:37 startup-script[1131]: ++++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:37 startup-script[1131]: ++++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: ++++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:37 startup-script[1131]: +++ activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:37 startup-script[1131]: +++ [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:37 startup-script[1131]: +++ echo false
<13>Jun 28 16:02:37 startup-script[1131]: ++ KERBEROS_ENABLED=false
<13>Jun 28 16:02:37 startup-script[1131]: +++ get_property_in_xml /etc/hadoop/conf/hdfs-site.xml dfs.webhdfs.enabled true
<13>Jun 28 16:02:37 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: ++ WEBHDFS_ENABLED=true
<13>Jun 28 16:02:37 startup-script[1131]: ++ WEBHDFS_BASE_URI=
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/hive-metastore.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: +++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ set -x
<13>Jun 28 16:02:37 startup-script[1131]: +++ get_metadata_master
<13>Jun 28 16:02:37 startup-script[1131]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:37 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:37 startup-script[1131]: +++ get_metadata_master_additional
<13>Jun 28 16:02:37 startup-script[1131]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Jun 28 16:02:37 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Jun 28 16:02:37 startup-script[1131]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_MASTERS=1
<13>Jun 28 16:02:37 startup-script[1131]: +++ get_metadata_role
<13>Jun 28 16:02:37 startup-script[1131]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Jun 28 16:02:37 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: ++ ROLE=Worker
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ 1 -gt 1 ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://mjtelco-m:9083
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../shared/spark.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ export SPARK_HOME=/usr/lib/spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HOME=/usr/lib/spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ export SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ export SPARK_AUTH_SECRET_FILE=/tmp/cluster/spark.auth.secret
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_AUTH_SECRET_FILE=/tmp/cluster/spark.auth.secret
<13>Jun 28 16:02:37 startup-script[1131]: ++ export SPARK_DEFAULT_IMAGE_FILE=/etc/dataproc/spark-default-image.tar
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_IMAGE_FILE=/etc/dataproc/spark-default-image.tar
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ export SPARK_DATA_DIR=/hadoop/spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DATA_DIR=/hadoop/spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ export SPARK_LOG_DIR=/var/log/spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_LOG_DIR=/var/log/spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ export SPARK_TMP_DIR=/hadoop/spark/tmp
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_TMP_DIR=/hadoop/spark/tmp
<13>Jun 28 16:02:37 startup-script[1131]: ++ export SPARK_WORK_DIR=/hadoop/spark/work
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORK_DIR=/hadoop/spark/work
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/dataproc_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ CLUSTER_NAME=mjtelco
<13>Jun 28 16:02:37 startup-script[1131]: ++ CLUSTER_UUID=7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:37 startup-script[1131]: ++ CONFIGBUCKET=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEMP_BUCKET=dataproc-temp-us-east1-746779145865-aaezsxz3
<13>Jun 28 16:02:37 startup-script[1131]: ++ HCFS_ROOT_URI=hdfs://mjtelco-m
<13>Jun 28 16:02:37 startup-script[1131]: ++ MASTER_HOSTNAME_0=mjtelco-m
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_MASTER_FQDN=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:37 startup-script[1131]: ++ MASTER_HOSTNAMES=(mjtelco-m)
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_MASTERS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=5
<13>Jun 28 16:02:37 startup-script[1131]: ++ PREFIX=mjtelco
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROJECT=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:37 startup-script[1131]: ++ ROLE=Worker
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + set_log_tag pre-activate-component-spark
<13>Jun 28 16:02:37 startup-script[1131]: + local -r tag=pre-activate-component-spark
<13>Jun 28 16:02:37 startup-script[1131]: + exec
<13>Jun 28 16:02:37 startup-script[1131]: ++ logger -s -t 'pre-activate-component-spark[2578]'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + readonly HCFS_EVENTLOG_DIR=hdfs://mjtelco-m/user/spark/eventlog
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + HCFS_EVENTLOG_DIR=hdfs://mjtelco-m/user/spark/eventlog
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + readonly SPARK_DAEMON_MEMORY_MB=312
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + SPARK_DAEMON_MEMORY_MB=312
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + readonly SPARK_STANDALONE_LOCAL_DATA_DIR=/hadoop/spark/local-dir
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + SPARK_STANDALONE_LOCAL_DATA_DIR=/hadoop/spark/local-dir
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + pre_activate_spark
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + init_local_dirs_common
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + mkdir -p /hadoop/spark/tmp /hadoop/spark/work /var/log/spark
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + chown spark:spark -R /hadoop/spark /var/log/spark
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + chmod 1777 -R /hadoop/spark /var/log/spark
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + ln -sf /hadoop/spark/work /usr/lib/spark/work
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + configure_env_common
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + configure_arrow
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + case ${compare_versions_result} in
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + return 0
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + configure_event_log_dir
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local event_log_dir=hdfs://mjtelco-m/user/spark/eventlog
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local persist_history_to_gcs
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_dataproc_property_or_default job.history.to-gcs.enabled false
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + persist_history_to_gcs=true
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ true == \t\r\u\e ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + loginfo 'Enabling persisting Spark job history files to GCS.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + echo 'Enabling persisting Spark job history files to GCS.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Enabling persisting Spark job history files to GCS.
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ -n dataproc-temp-us-east1-746779145865-aaezsxz3 ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local gcs_history_dir_path=7db0aaf5-a842-4945-a93b-c6d17e834515/spark-job-history
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + event_log_dir=gs://dataproc-temp-us-east1-746779145865-aaezsxz3/7db0aaf5-a842-4945-a93b-c6d17e834515/spark-job-history
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + configure_env_yarn
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_version_at_least 2.0 2.1
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Comparing if version 2.0 is at least version 2.1 
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + case ${compare_versions_result} in
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + return 1
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + configure_defaults_yarn
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + case ${compare_versions_result} in
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + return 0
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_component_selected hive-server2
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r component=hive-server2
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local activated_components
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_components_to_activate
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *hive-server2* ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_version_at_least 2.0 2.1
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Comparing if version 2.0 is at least version 2.1 
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + case ${compare_versions_result} in
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + return 1
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local enable_docker_yarn
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_dataproc_property_or_default yarn.docker.enable ''
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + enable_docker_yarn=
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ -z '' ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_dataproc_property_or_default docker.yarn.enable ''
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + enable_docker_yarn=
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ '' == \t\r\u\e ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + case ${compare_versions_result} in
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + return 0
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + configure_efm
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local spark_efm_property
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_dataproc_property efm.spark.shuffle
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + spark_efm_property=
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ '' == \p\r\i\m\a\r\y\-\w\o\r\k\e\r ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_component_selected kerberos
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r component=kerberos
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local activated_components
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_components_to_activate
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + merge_user_properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + merge_java_properties /tmp/cluster/properties/spark.properties /etc/spark/conf/spark-defaults.conf
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r src=/tmp/cluster/properties/spark.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r dest=/etc/spark/conf/spark-defaults.conf
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r 'header=\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ ! -f /tmp/cluster/properties/spark.properties ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat /tmp/cluster/properties/spark.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + loginfo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + echo 'Merged /tmp/cluster/properties/spark.properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Merged /tmp/cluster/properties/spark.properties.
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + merge_sh_env_vars /tmp/cluster/properties/spark-env.sh /etc/spark/conf/spark-env.sh
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local src=/tmp/cluster/properties/spark-env.sh
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local dest=/etc/spark/conf/spark-env.sh
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ ! -f /tmp/cluster/properties/spark-env.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat /tmp/cluster/properties/spark-env.sh
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + loginfo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + echo 'Merged /tmp/cluster/properties/spark-env.sh.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Merged /tmp/cluster/properties/spark-env.sh.
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_version_at_least 2.0 2.1
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Comparing if version 2.0 is at least version 2.1 
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + case ${compare_versions_result} in
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + return 1
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + merge_java_properties /tmp/cluster/properties/spark-log4j.properties /etc/spark/conf/log4j.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r src=/tmp/cluster/properties/spark-log4j.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r dest=/etc/spark/conf/log4j.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r 'header=\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ ! -f /tmp/cluster/properties/spark-log4j.properties ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + echo -e '\n# User-supplied properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat /tmp/cluster/properties/spark-log4j.properties
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + loginfo 'Merged /tmp/cluster/properties/spark-log4j.properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + echo 'Merged /tmp/cluster/properties/spark-log4j.properties.'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Merged /tmp/cluster/properties/spark-log4j.properties.
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + case ${compare_versions_result} in
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + return 0
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + configure_broadcast_join 0.0075 200
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r fraction=0.0075
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local -r max_mb=200
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local user_broadcast_join_threshold_mb
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.sql.autoBroadcastJoinThreshold
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + user_broadcast_join_threshold_mb=
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ -n '' ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local spark_executor_memory
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_java_property /etc/spark/conf/spark-defaults.conf spark.executor.memory
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + spark_executor_memory=2688m
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local spark_executor_memory_bytes
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ get_size_in_bytes 2688m
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ local size=2688M
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ size=2688M
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ size=2688M
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ numfmt --from=iec 2688M
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + spark_executor_memory_bytes=2818572288
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + local broadcast_join_threshold_mb
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: ++ python -c 'print(min(max(int(2818572288 * 0.0075 / 1024 / 1024), 10), 200))'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + broadcast_join_threshold_mb=20
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + cat
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-spark[2578]: + [[ Worker == \M\a\s\t\e\r ]]
<13>Jun 28 16:02:37 startup-script[1131]: touch /tmp/dataproc/sentinel/spark.pre-activate
<13>Jun 28 16:02:37 startup-script[1131]: Running task: yarn.pre-activate
<13>Jun 28 16:02:37 startup-script[1131]: bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh 2.0 || { echo "Error: /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh"; exit 1; }
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_misc.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_os.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ is_rocky
<13>Jun 28 16:02:37 startup-script[1131]: +++ os_id
<13>Jun 28 16:02:37 startup-script[1131]: +++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:37 startup-script[1131]: +++ xargs
<13>Jun 28 16:02:37 startup-script[1131]: +++ cut -d= -f2
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_properties.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_versions.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_components.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../bdutil_collections.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-activate/yarn.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-activate/../../configure_bigtable_hbase_client.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + set_log_tag pre-activate-component-yarn
<13>Jun 28 16:02:37 startup-script[1131]: + local -r tag=pre-activate-component-yarn
<13>Jun 28 16:02:37 startup-script[1131]: + exec
<13>Jun 28 16:02:37 startup-script[1131]: ++ logger -s -t 'pre-activate-component-yarn[2686]'
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-yarn[2686]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-yarn[2686]: ++ set +x
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-yarn[2686]: + readonly ATSV2_BIGTABLE_RESOURCE=
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-yarn[2686]: + ATSV2_BIGTABLE_RESOURCE=
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-yarn[2686]: + [[ -n '' ]]
<13>Jun 28 16:02:37 startup-script[1131]: <13>Jun 28 16:02:37 pre-activate-component-yarn[2686]: + rm -Rf /usr/local/share/google/dataproc/lib/bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: touch /tmp/dataproc/sentinel/yarn.pre-activate
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'All pre-activate scripts done'
<13>Jun 28 16:02:37 startup-script[1131]: All pre-activate scripts done
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling unselected components'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling unselected components'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling unselected components
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_components docker-ce dpms-proxy druid earlyoom flink hbase hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox presto proxy-agent ranger rubix solr-server zeppelin zookeeper-server
<13>Jun 28 16:02:37 startup-script[1131]: + components=("$@")
<13>Jun 28 16:02:37 startup-script[1131]: + local components
<13>Jun 28 16:02:37 startup-script[1131]: + mkdir -p /tmp/dataproc/components/pre-uninstall
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component docker-ce'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component docker-ce'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component docker-ce
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component docker-ce
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=docker-ce
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/docker-ce.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall docker-ce
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall/docker-ce
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/docker-ce.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component dpms-proxy'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component dpms-proxy'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component dpms-proxy
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component dpms-proxy
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=dpms-proxy
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/dpms-proxy.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/dpms-proxy.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Component dpms-proxy doesn'\''t have a pre-uninstall script'
<13>Jun 28 16:02:37 startup-script[1131]: Component dpms-proxy doesn't have a pre-uninstall script
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component druid'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component druid'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component druid
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component druid
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=druid
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/druid.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/druid.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall druid
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall/druid
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/druid.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component earlyoom'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component earlyoom'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component earlyoom
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component earlyoom
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=earlyoom
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/earlyoom.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/earlyoom.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/earlyoom.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/earlyoom.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/earlyoom.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/earlyoom.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/earlyoom.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/earlyoom.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/earlyoom.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall earlyoom
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall/earlyoom
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/earlyoom.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component flink'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component flink'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component flink
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component flink
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=flink
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/flink.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/flink.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall flink
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall/flink
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/flink.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component hbase'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component hbase'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component hbase
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component hbase
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=hbase
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/hbase.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hbase.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall_pre_activate hbase hive-hbase
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall-pre-activate/hbase
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall-pre-activate/hive-hbase
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/hbase.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component hive-metastore'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component hive-metastore'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component hive-metastore
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component hive-metastore
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=hive-metastore
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-metastore.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-metastore.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-metastore.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-metastore.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/hive-metastore.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-metastore.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-metastore.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-metastore.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-metastore.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall hive-metastore
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall/hive-metastore
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/hive-metastore.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component hive-server2'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component hive-server2'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component hive-server2
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component hive-server2
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=hive-server2
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-server2.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-server2.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-server2.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-server2.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/hive-server2.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-server2.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-server2.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-server2.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-server2.sh
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../../bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -x
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall hive-server2
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall/hive-server2
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/hive-server2.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component hive-webhcat-server'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component hive-webhcat-server'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component hive-webhcat-server
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component hive-webhcat-server
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=hive-webhcat-server
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/hive-webhcat-server.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hive-webhcat-server.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:37 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:37 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:37 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:37 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:37 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:37 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:37 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall hive-webhcat-server
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall/hive-webhcat-server
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/hive-webhcat-server.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component hudi'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component hudi'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component hudi
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component hudi
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=hudi
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hudi.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/hudi.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Component hudi doesn'\''t have a pre-uninstall script'
<13>Jun 28 16:02:37 startup-script[1131]: Component hudi doesn't have a pre-uninstall script
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component jupyter'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component jupyter'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component jupyter
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component jupyter
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=jupyter
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/jupyter.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/jupyter.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Component jupyter doesn'\''t have a pre-uninstall script'
<13>Jun 28 16:02:37 startup-script[1131]: Component jupyter doesn't have a pre-uninstall script
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component kafka-server'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component kafka-server'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component kafka-server
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component kafka-server
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=kafka-server
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/kafka-server.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kafka-server.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:37 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:37 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:37 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:37 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:37 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:37 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:37 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:37 startup-script[1131]: + mark_packages_to_uninstall kafka-server
<13>Jun 28 16:02:37 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/uninstall/kafka-server
<13>Jun 28 16:02:37 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/kafka-server.done
<13>Jun 28 16:02:37 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:37 startup-script[1131]: + loginfo 'Pre-uninstalling component kerberos'
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Pre-uninstalling component kerberos'
<13>Jun 28 16:02:37 startup-script[1131]: Pre-uninstalling component kerberos
<13>Jun 28 16:02:37 startup-script[1131]: + pre_uninstall_component kerberos
<13>Jun 28 16:02:37 startup-script[1131]: + local -r component=kerberos
<13>Jun 28 16:02:37 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Jun 28 16:02:37 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh'
<13>Jun 28 16:02:37 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Jun 28 16:02:37 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/kerberos.running
<13>Jun 28 16:02:37 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:37 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Jun 28 16:02:37 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:37 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:37 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:37 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:37 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:37 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:37 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:37 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:37 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:37 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:37 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:37 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:37 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:37 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:37 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:37 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:37 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:37 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:37 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:37 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:37 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/kerberos.sh
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/../shared/kerberos.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/components/shared/kerberos.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ KDC_SERVICE=krb5-kdc
<13>Jun 28 16:02:38 startup-script[1131]: +++ KADMIN_SERVICE=krb5-admin-server
<13>Jun 28 16:02:38 startup-script[1131]: +++ KPROP_SERVICE=krb5-kpropd
<13>Jun 28 16:02:38 startup-script[1131]: +++ KERBEROS_LIBRARIES=(krb5-user krb5-config krb5-kdc krb5-admin-server krb5-kpropd xinetd)
<13>Jun 28 16:02:38 startup-script[1131]: +++ KERBEROS_SERVICES=(${KDC_SERVICE} ${KADMIN_SERVICE} ${KPROP_SERVICE})
<13>Jun 28 16:02:38 startup-script[1131]: +++ KDC_CONF_DIR=/etc/krb5kdc
<13>Jun 28 16:02:38 startup-script[1131]: +++ KERBEROS_DATABASE_DIR=/var/lib/krb5kdc
<13>Jun 28 16:02:38 startup-script[1131]: +++ KERBEROS_USER_CONFIG=/etc/krb5.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ readonly HANDS_OFF_PASSWORD=/tmp/cluster/kerberos.password
<13>Jun 28 16:02:38 startup-script[1131]: ++ HANDS_OFF_PASSWORD=/tmp/cluster/kerberos.password
<13>Jun 28 16:02:38 startup-script[1131]: + is_rocky
<13>Jun 28 16:02:38 startup-script[1131]: ++ os_id
<13>Jun 28 16:02:38 startup-script[1131]: ++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:38 startup-script[1131]: ++ cut -d= -f2
<13>Jun 28 16:02:38 startup-script[1131]: ++ xargs
<13>Jun 28 16:02:38 startup-script[1131]: + [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:38 startup-script[1131]: + mark_packages_to_uninstall krb5-user krb5-config krb5-kdc krb5-admin-server krb5-kpropd xinetd
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/krb5-user
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/krb5-config
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/krb5-kdc
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/krb5-admin-server
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/krb5-kpropd
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/xinetd
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/kerberos.done
<13>Jun 28 16:02:38 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Pre-uninstalling component knox'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Pre-uninstalling component knox'
<13>Jun 28 16:02:38 startup-script[1131]: Pre-uninstalling component knox
<13>Jun 28 16:02:38 startup-script[1131]: + pre_uninstall_component knox
<13>Jun 28 16:02:38 startup-script[1131]: + local -r component=knox
<13>Jun 28 16:02:38 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh'
<13>Jun 28 16:02:38 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/knox.running
<13>Jun 28 16:02:38 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:38 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/knox.sh
<13>Jun 28 16:02:38 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:38 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:38 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:38 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:38 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:38 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:38 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:38 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:38 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:38 startup-script[1131]: + mark_packages_to_uninstall knox
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/knox
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/knox.done
<13>Jun 28 16:02:38 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Pre-uninstalling component presto'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Pre-uninstalling component presto'
<13>Jun 28 16:02:38 startup-script[1131]: Pre-uninstalling component presto
<13>Jun 28 16:02:38 startup-script[1131]: + pre_uninstall_component presto
<13>Jun 28 16:02:38 startup-script[1131]: + local -r component=presto
<13>Jun 28 16:02:38 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh'
<13>Jun 28 16:02:38 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/presto.running
<13>Jun 28 16:02:38 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:38 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/presto.sh
<13>Jun 28 16:02:38 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:38 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:38 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:38 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:38 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:38 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:38 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:38 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:38 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:38 startup-script[1131]: + mark_packages_to_uninstall presto
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/presto
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/presto.done
<13>Jun 28 16:02:38 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Pre-uninstalling component proxy-agent'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Pre-uninstalling component proxy-agent'
<13>Jun 28 16:02:38 startup-script[1131]: Pre-uninstalling component proxy-agent
<13>Jun 28 16:02:38 startup-script[1131]: + pre_uninstall_component proxy-agent
<13>Jun 28 16:02:38 startup-script[1131]: + local -r component=proxy-agent
<13>Jun 28 16:02:38 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/proxy-agent.sh
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/proxy-agent.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Component proxy-agent doesn'\''t have a pre-uninstall script'
<13>Jun 28 16:02:38 startup-script[1131]: Component proxy-agent doesn't have a pre-uninstall script
<13>Jun 28 16:02:38 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Pre-uninstalling component ranger'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Pre-uninstalling component ranger'
<13>Jun 28 16:02:38 startup-script[1131]: Pre-uninstalling component ranger
<13>Jun 28 16:02:38 startup-script[1131]: + pre_uninstall_component ranger
<13>Jun 28 16:02:38 startup-script[1131]: + local -r component=ranger
<13>Jun 28 16:02:38 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh'
<13>Jun 28 16:02:38 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/ranger.running
<13>Jun 28 16:02:38 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:38 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/ranger.sh
<13>Jun 28 16:02:38 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:38 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:38 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:38 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:38 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:38 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:38 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:38 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:38 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:38 startup-script[1131]: + mark_packages_to_uninstall ranger
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/ranger
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/ranger.done
<13>Jun 28 16:02:38 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Pre-uninstalling component rubix'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Pre-uninstalling component rubix'
<13>Jun 28 16:02:38 startup-script[1131]: Pre-uninstalling component rubix
<13>Jun 28 16:02:38 startup-script[1131]: + pre_uninstall_component rubix
<13>Jun 28 16:02:38 startup-script[1131]: + local -r component=rubix
<13>Jun 28 16:02:38 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh'
<13>Jun 28 16:02:38 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/rubix.running
<13>Jun 28 16:02:38 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:38 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/rubix.sh
<13>Jun 28 16:02:38 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:38 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:38 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:38 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:38 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:38 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:38 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:38 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:38 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:38 startup-script[1131]: + mark_packages_to_uninstall rubix
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/rubix
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/rubix.done
<13>Jun 28 16:02:38 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Pre-uninstalling component solr-server'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Pre-uninstalling component solr-server'
<13>Jun 28 16:02:38 startup-script[1131]: Pre-uninstalling component solr-server
<13>Jun 28 16:02:38 startup-script[1131]: + pre_uninstall_component solr-server
<13>Jun 28 16:02:38 startup-script[1131]: + local -r component=solr-server
<13>Jun 28 16:02:38 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh'
<13>Jun 28 16:02:38 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/solr-server.running
<13>Jun 28 16:02:38 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:38 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/solr-server.sh
<13>Jun 28 16:02:38 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:38 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:38 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:38 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:38 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:38 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:38 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:38 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:38 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:38 startup-script[1131]: + mark_packages_to_uninstall solr-server
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/solr-server
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/solr-server.done
<13>Jun 28 16:02:38 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Pre-uninstalling component zeppelin'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Pre-uninstalling component zeppelin'
<13>Jun 28 16:02:38 startup-script[1131]: Pre-uninstalling component zeppelin
<13>Jun 28 16:02:38 startup-script[1131]: + pre_uninstall_component zeppelin
<13>Jun 28 16:02:38 startup-script[1131]: + local -r component=zeppelin
<13>Jun 28 16:02:38 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh'
<13>Jun 28 16:02:38 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/zeppelin.running
<13>Jun 28 16:02:38 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:38 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zeppelin.sh
<13>Jun 28 16:02:38 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:38 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:38 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:38 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:38 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:38 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:38 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:38 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:38 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:38 startup-script[1131]: + mark_packages_to_uninstall zeppelin
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/zeppelin
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/zeppelin.done
<13>Jun 28 16:02:38 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Pre-uninstalling component zookeeper-server'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Pre-uninstalling component zookeeper-server'
<13>Jun 28 16:02:38 startup-script[1131]: Pre-uninstalling component zookeeper-server
<13>Jun 28 16:02:38 startup-script[1131]: + pre_uninstall_component zookeeper-server
<13>Jun 28 16:02:38 startup-script[1131]: + local -r component=zookeeper-server
<13>Jun 28 16:02:38 startup-script[1131]: + local -r pre_uninstall_script=/usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh'
<13>Jun 28 16:02:38 startup-script[1131]: Running component pre-uninstall script: /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/zookeeper-server.running
<13>Jun 28 16:02:38 startup-script[1131]: + local exit_code=0
<13>Jun 28 16:02:38 startup-script[1131]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/zookeeper-server.sh
<13>Jun 28 16:02:38 startup-script[1131]: + set -euxo pipefail
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:38 startup-script[1131]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_REPO=cran40
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ R_VERSION=4.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:38 startup-script[1131]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:38 startup-script[1131]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:38 startup-script[1131]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:38 startup-script[1131]: ++ WORKERS=()
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:38 startup-script[1131]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:38 startup-script[1131]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:38 startup-script[1131]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:38 startup-script[1131]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:38 startup-script[1131]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:38 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_logging.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: +++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_retry.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_properties.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metadata.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_os.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ is_rocky
<13>Jun 28 16:02:38 startup-script[1131]: ++++ os_id
<13>Jun 28 16:02:38 startup-script[1131]: ++++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:38 startup-script[1131]: ++++ xargs
<13>Jun 28 16:02:38 startup-script[1131]: ++++ cut -d= -f2
<13>Jun 28 16:02:38 startup-script[1131]: +++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_networking.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: +++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_versions.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_collections.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_services.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_packages.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_components.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_misc.sh
<13>Jun 28 16:02:38 startup-script[1131]: +++ source /usr/local/share/google/dataproc/bdutil/bdutil_gcs.sh
<13>Jun 28 16:02:38 startup-script[1131]: ++ source /usr/local/share/google/dataproc/bdutil/bdutil_metrics.sh
<13>Jun 28 16:02:38 startup-script[1131]: + mark_packages_to_uninstall zookeeper-server
<13>Jun 28 16:02:38 startup-script[1131]: + for package in "$@"
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/uninstall/zookeeper-server
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + touch /tmp/dataproc/components/pre-uninstall/zookeeper-server.done
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Uninstalling packages which must be uninstalled before activating components'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Uninstalling packages which must be uninstalled before activating components'
<13>Jun 28 16:02:38 startup-script[1131]: Uninstalling packages which must be uninstalled before activating components
<13>Jun 28 16:02:38 startup-script[1131]: + uninstall_packages_pre_activate
<13>Jun 28 16:02:38 startup-script[1131]: + local packages_to_uninstall
<13>Jun 28 16:02:38 startup-script[1131]: + packages_to_uninstall=($(list_packages_to_uninstall_pre_activate))
<13>Jun 28 16:02:38 startup-script[1131]: ++ list_packages_to_uninstall_pre_activate
<13>Jun 28 16:02:38 startup-script[1131]: ++ find /tmp/dataproc/uninstall-pre-activate/ -type f -exec basename '{}' ';'
<13>Jun 28 16:02:38 startup-script[1131]: + [[ 2 -gt 0 ]]
<13>Jun 28 16:02:38 startup-script[1131]: + os_uninstall_packages hbase hive-hbase
<13>Jun 28 16:02:38 startup-script[1131]: + packages_to_uninstall=("$@")
<13>Jun 28 16:02:38 startup-script[1131]: + local -r packages_to_uninstall
<13>Jun 28 16:02:38 startup-script[1131]: + loginfo 'Uninstalling packages: hbase hive-hbase'
<13>Jun 28 16:02:38 startup-script[1131]: + echo 'Uninstalling packages: hbase hive-hbase'
<13>Jun 28 16:02:38 startup-script[1131]: Uninstalling packages: hbase hive-hbase
<13>Jun 28 16:02:38 startup-script[1131]: + local -r 'uninstall_cmd=DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hbase hive-hbase'
<13>Jun 28 16:02:38 startup-script[1131]: + retry_constant_short bash -c 'DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hbase hive-hbase'
<13>Jun 28 16:02:38 startup-script[1131]: + retry_constant_custom 30 1 bash -c 'DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hbase hive-hbase'
<13>Jun 28 16:02:38 startup-script[1131]: + local -r max_retry_time=30
<13>Jun 28 16:02:38 startup-script[1131]: + local -r retry_delay=1
<13>Jun 28 16:02:38 startup-script[1131]: + cmd=("${@:3}")
<13>Jun 28 16:02:38 startup-script[1131]: + local -r cmd
<13>Jun 28 16:02:38 startup-script[1131]: + local -r max_retries=30
<13>Jun 28 16:02:38 startup-script[1131]: + local reenable_x=false
<13>Jun 28 16:02:38 startup-script[1131]: + [[ -o xtrace ]]
<13>Jun 28 16:02:38 startup-script[1131]: + set +x
<13>Jun 28 16:02:38 startup-script[1131]: About to run 'bash -c DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hbase hive-hbase' with retries...
<13>Jun 28 16:02:39 startup-script[1131]: Reading package lists...
<13>Jun 28 16:02:39 startup-script[1131]: Building dependency tree...
<13>Jun 28 16:02:39 startup-script[1131]: Reading state information...
<13>Jun 28 16:02:39 startup-script[1131]: The following packages will be REMOVED:
<13>Jun 28 16:02:39 startup-script[1131]:   hbase* hive-hbase*
<13>Jun 28 16:02:40 startup-script[1131]: 0 upgraded, 0 newly installed, 2 to remove and 2 not upgraded.
<13>Jun 28 16:02:40 startup-script[1131]: After this operation, 287 MB disk space will be freed.
<13>Jun 28 16:02:41 startup-script[1131]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 182683 files and directories currently installed.)
<13>Jun 28 16:02:41 startup-script[1131]: Removing hive-hbase (3.1.2-1) ...
<13>Jun 28 16:02:41 startup-script[1131]: Removing hbase (2.2.7-1) ...
<13>Jun 28 16:02:41 startup-script[1131]: Processing triggers for man-db (2.8.5-2) ...
<13>Jun 28 16:02:42 startup-script[1131]: (Reading database ... (Reading database ... 5%(Reading database ... 10%(Reading database ... 15%(Reading database ... 20%(Reading database ... 25%(Reading database ... 30%(Reading database ... 35%(Reading database ... 40%(Reading database ... 45%(Reading database ... 50%(Reading database ... 55%(Reading database ... 60%(Reading database ... 65%(Reading database ... 70%(Reading database ... 75%(Reading database ... 80%(Reading database ... 85%(Reading database ... 90%(Reading database ... 95%(Reading database ... 100%(Reading database ... 182160 files and directories currently installed.)
<13>Jun 28 16:02:42 startup-script[1131]: Purging configuration files for hbase (2.2.7-1) ...
<13>Jun 28 16:02:42 startup-script[1131]: 'bash -c DEBIAN_FRONTEND=noninteractive     apt-get autoremove -y --purge hbase hive-hbase' succeeded after 1 execution(s).
<13>Jun 28 16:02:42 startup-script[1131]: + return 0
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Starting to uninstall artifacts'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Starting to uninstall artifacts'
<13>Jun 28 16:02:42 startup-script[1131]: Starting to uninstall artifacts
<13>Jun 28 16:02:42 startup-script[1131]: + start_uninstall_artifacts
<13>Jun 28 16:02:42 startup-script[1131]: + local blocking_default=
<13>Jun 28 16:02:42 startup-script[1131]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:42 startup-script[1131]: + is_version_at_least 2.0 2.0
<13>Jun 28 16:02:42 startup-script[1131]: + set +x
<13>Jun 28 16:02:42 startup-script[1131]: Comparing if version 2.0 is at least version 2.0 
<13>Jun 28 16:02:42 startup-script[1131]: + case ${compare_versions_result} in
<13>Jun 28 16:02:42 startup-script[1131]: + return 0
<13>Jun 28 16:02:42 startup-script[1131]: + local blocking
<13>Jun 28 16:02:42 startup-script[1131]: ++ get_dataproc_property_or_default dataproc.uninstall.packages.blocking ''
<13>Jun 28 16:02:42 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:42 startup-script[1131]: + blocking=
<13>Jun 28 16:02:42 startup-script[1131]: + [[ -z '' ]]
<13>Jun 28 16:02:42 startup-script[1131]: + local init_action_count
<13>Jun 28 16:02:42 startup-script[1131]: ++ /usr/share/google/get_metadata_value attributes/dataproc-initialization-script-count
<13>Jun 28 16:02:42 startup-script[1131]: + init_action_count=0
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 0 == \0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + blocking=false
<13>Jun 28 16:02:42 startup-script[1131]: + [[ false == \t\r\u\e ]]
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag delayed_uninstall_artifacts delayed_uninstall_artifacts
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3029
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=delayed_uninstall_artifacts
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 1 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + delayed_uninstall_artifacts
<13>Jun 28 16:02:42 startup-script[1131]: ++ get_dataproc_property_or_default dataproc.multi.user.metadata.proxy.enabled false
<13>Jun 28 16:02:42 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'delayed_uninstall_artifacts[3029]'
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 delayed_uninstall_artifacts[3029]: + sleep 60
<13>Jun 28 16:02:42 startup-script[1131]: + MULTI_USER_METADATA_PROXY_ENABLED=false
<13>Jun 28 16:02:42 startup-script[1131]: + readonly MULTI_USER_METADATA_PROXY_ENABLED
<13>Jun 28 16:02:42 startup-script[1131]: + [[ false == \t\r\u\e ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Starting services'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Starting services'
<13>Jun 28 16:02:42 startup-script[1131]: Starting services
<13>Jun 28 16:02:42 startup-script[1131]: + start_services stackdriver-agent
<13>Jun 28 16:02:42 startup-script[1131]: + services=("$@")
<13>Jun 28 16:02:42 startup-script[1131]: + local services
<13>Jun 28 16:02:42 startup-script[1131]: + for service in "${services[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + in_array stackdriver-agent DATAPROC_COMPONENTS
<13>Jun 28 16:02:42 startup-script[1131]: + local -r value=stackdriver-agent
<13>Jun 28 16:02:42 startup-script[1131]: + local -n values=DATAPROC_COMPONENTS
<13>Jun 28 16:02:42 startup-script[1131]: + [[ !  docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server  == *\ \s\t\a\c\k\d\r\i\v\e\r\-\a\g\e\n\t\ * ]]
<13>Jun 28 16:02:42 startup-script[1131]: + return 1
<13>Jun 28 16:02:42 startup-script[1131]: + in_array stackdriver-agent COMPONENT_SERVICES
<13>Jun 28 16:02:42 startup-script[1131]: + local -r value=stackdriver-agent
<13>Jun 28 16:02:42 startup-script[1131]: + local -n values=COMPONENT_SERVICES
<13>Jun 28 16:02:42 startup-script[1131]: + [[ !  hadoop-hdfs-namenode hadoop-hdfs-datanode hadoop-hdfs-zkfc hadoop-hdfs-secondarynamenode hadoop-hdfs-journalnode hive-metastore hive-server2 mysql-server spark-history-server  == *\ \s\t\a\c\k\d\r\i\v\e\r\-\a\g\e\n\t\ * ]]
<13>Jun 28 16:02:42 startup-script[1131]: + return 1
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag setup-stackdriver-agent setup_service stackdriver-agent
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3041
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3041.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'setup_service stackdriver-agent'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [setup_service stackdriver-agent] as pid 3041'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [setup_service stackdriver-agent] as pid 3041
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating components'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating components'
<13>Jun 28 16:02:42 startup-script[1131]: Activating components
<13>Jun 28 16:02:42 startup-script[1131]: + activate_components hdfs hive-metastore hive-server2 mapreduce miniconda3 mysql pig spark tez yarn
<13>Jun 28 16:02:42 startup-script[1131]: + components=("$@")
<13>Jun 28 16:02:42 startup-script[1131]: + local components
<13>Jun 28 16:02:42 startup-script[1131]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ hdfs != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: hdfs'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: hdfs'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: hdfs
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-hdfs activate_component hdfs
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3043
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3043.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component hdfs'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component hdfs] as pid 3043'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component hdfs] as pid 3043
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ hive-metastore != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: hive-metastore'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: hive-metastore'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: hive-metastore
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-hive-metastore activate_component hive-metastore
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3044
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3044.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component hive-metastore'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component hive-metastore] as pid 3044'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component hive-metastore] as pid 3044
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ hive-server2 != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: hive-server2'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: hive-server2'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: hive-server2
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-hive-server2 activate_component hive-server2
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3045
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3045.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component hive-server2'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component hive-server2] as pid 3045'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component hive-server2] as pid 3045
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ mapreduce != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: mapreduce'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: mapreduce'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: mapreduce
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-mapreduce activate_component mapreduce
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3046
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3046.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component mapreduce'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component mapreduce] as pid 3046'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component mapreduce] as pid 3046
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ miniconda3 != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: miniconda3'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: miniconda3'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: miniconda3
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-miniconda3 activate_component miniconda3
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3047
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3047.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component miniconda3'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component miniconda3] as pid 3047'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component miniconda3] as pid 3047
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ mysql != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: mysql'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: mysql'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: mysql
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-mysql activate_component mysql
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3048
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3048.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component mysql'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component mysql] as pid 3048'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component mysql] as pid 3048
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ pig != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: pig'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: pig'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: pig
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-pig activate_component pig
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3049
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3049.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component pig'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component pig] as pid 3049'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component pig] as pid 3049
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ spark != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: spark'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: spark'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: spark
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-spark activate_component spark
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3050
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3050.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component spark'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component spark] as pid 3050'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component spark] as pid 3050
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ tez != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: tez'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: tez'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: tez
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-tez activate_component tez
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3051
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3051.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component tez'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component tez] as pid 3051'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component tez] as pid 3051
<13>Jun 28 16:02:42 startup-script[1131]: + for component in "${components[@]}"
<13>Jun 28 16:02:42 startup-script[1131]: + [[ yarn != \k\e\r\b\e\r\o\s ]]
<13>Jun 28 16:02:42 startup-script[1131]: + loginfo 'Activating: yarn'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Activating: yarn'
<13>Jun 28 16:02:42 startup-script[1131]: Activating: yarn
<13>Jun 28 16:02:42 startup-script[1131]: + run_in_background --tag activate-component-yarn activate_component yarn
<13>Jun 28 16:02:42 startup-script[1131]: + local -r pid=3052
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-mysql activate_component mysql
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3048
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-mysql
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component mysql
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-mapreduce activate_component mapreduce
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3046
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-mapreduce
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component mapreduce
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-miniconda3 activate_component miniconda3
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3047
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-miniconda3
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component miniconda3
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-pig activate_component pig
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3049
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-pig
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component pig
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-hive-server2 activate_component hive-server2
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3045
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-hive-server2
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component hive-server2
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-hive-metastore activate_component hive-metastore
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3044
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-hive-metastore
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component hive-metastore
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-spark activate_component spark
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3050
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-spark
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component spark
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-tez activate_component tez
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3051
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-tez
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component tez
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'activate-component-mysql[3048]'
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-yarn activate_component yarn
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3052
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-yarn
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component yarn
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'activate-component-mapreduce[3046]'
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'activate-component-miniconda3[3047]'
<13>Jun 28 16:02:42 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:42 startup-script[1131]: + run_with_logger --tag activate-component-hdfs activate_component hdfs
<13>Jun 28 16:02:42 startup-script[1131]: + local tag=
<13>Jun 28 16:02:42 startup-script[1131]: + local pid=3043
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + tag=activate-component-hdfs
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:42 startup-script[1131]: + activate_component hdfs
<13>Jun 28 16:02:42 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:42 startup-script[1131]: + shift 2
<13>Jun 28 16:02:42 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3052.running ]]
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'activate_component yarn'
<13>Jun 28 16:02:42 startup-script[1131]: + echo 'Started background process [activate_component yarn] as pid 3052'
<13>Jun 28 16:02:42 startup-script[1131]: Started background process [activate_component yarn] as pid 3052
<13>Jun 28 16:02:42 startup-script[1131]: + is_service_installed google-osconfig-agent
<13>Jun 28 16:02:42 startup-script[1131]: + local -r service=google-osconfig-agent
<13>Jun 28 16:02:42 startup-script[1131]: + local output
<13>Jun 28 16:02:42 startup-script[1131]: + local status
<13>Jun 28 16:02:42 startup-script[1131]: + (( i = 0 ))
<13>Jun 28 16:02:42 startup-script[1131]: + (( i < 10 ))
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'activate-component-pig[3049]'
<13>Jun 28 16:02:42 startup-script[1131]: ++ systemctl cat google-osconfig-agent.service
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'activate-component-hive-server2[3045]'
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-pig[3049]: + local -r component=pig
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-pig[3049]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-pig[3049]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh ]]
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-pig[3049]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh'
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-pig[3049]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-pig[3049]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'activate-component-spark[3050]'
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-pig[3049]: + touch /tmp/dataproc/components/activate/pig.running
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'activate-component-hive-metastore[3044]'
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-spark[3050]: + local -r component=spark
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-spark[3050]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-spark[3050]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh ]]
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-spark[3050]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh'
<13>Jun 28 16:02:42 startup-script[1131]: ++ logger -s -t 'activate-component-tez[3051]'
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-spark[3050]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:42 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-spark[3050]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-hive-metastore[3044]: + local -r component=hive-metastore
<13>Jun 28 16:02:43 startup-script[1131]: + output='# /lib/systemd/system/google-osconfig-agent.service
<13>Jun 28 16:02:43 startup-script[1131]: [Unit]
<13>Jun 28 16:02:43 startup-script[1131]: Description=Google OSConfig Agent
<13>Jun 28 16:02:43 startup-script[1131]: After=local-fs.target network-online.target
<13>Jun 28 16:02:43 startup-script[1131]: Wants=local-fs.target network-online.target
<13>Jun 28 16:02:43 startup-script[1131]: 
<13>Jun 28 16:02:43 startup-script[1131]: [Service]
<13>Jun 28 16:02:43 startup-script[1131]: ExecStart=/usr/bin/google_osconfig_agent
<13>Jun 28 16:02:43 startup-script[1131]: Restart=always
<13>Jun 28 16:02:43 startup-script[1131]: RestartSec=1
<13>Jun 28 16:02:43 startup-script[1131]: StartLimitInterval=120
<13>Jun 28 16:02:43 startup-script[1131]: StartLimitBurst=3
<13>Jun 28 16:02:43 startup-script[1131]: KillMode=mixed
<13>Jun 28 16:02:43 startup-script[1131]: KillSignal=SIGTERM
<13>Jun 28 16:02:43 startup-script[1131]: 
<13>Jun 28 16:02:43 startup-script[1131]: [Install]
<13>Jun 28 16:02:43 startup-script[1131]: WantedBy=multi-user.target
<13>Jun 28 16:02:43 startup-script[1131]: 
<13>Jun 28 16:02:43 startup-script[1131]: # /etc/systemd/system/google-osconfig-agent.service.d/dataproc.conf
<13>Jun 28 16:02:43 startup-script[1131]: [Unit]
<13>Jun 28 16:02:43 startup-script[1131]: ConditionPathExists=!/etc/google-dataproc/hermetic_vm'
<13>Jun 28 16:02:43 startup-script[1131]: + status=0
<13>Jun 28 16:02:43 startup-script[1131]: + [[ 0 -eq 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: + return 0
<13>Jun 28 16:02:43 startup-script[1131]: + is_hermetic_vm
<13>Jun 28 16:02:43 startup-script[1131]: + local -r sentinel_file=/etc/google-dataproc/hermetic_vm
<13>Jun 28 16:02:43 startup-script[1131]: + [[ -f /etc/google-dataproc/hermetic_vm ]]
<13>Jun 28 16:02:43 startup-script[1131]: + local hermetic_vm
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:42 activate-component-hive-metastore[3044]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + local -r component=mapreduce
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: ++ logger -s -t 'activate-component-hdfs[3043]'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + touch /tmp/dataproc/components/activate/mapreduce.running
<13>Jun 28 16:02:43 startup-script[1131]: ++ /usr/share/google/get_metadata_value attributes/hermetic-vm
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + touch /tmp/dataproc/components/activate/hive-metastore.running
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r component=miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + touch /tmp/dataproc/components/activate/miniconda3.running
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + local -r component=mysql
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + touch /tmp/dataproc/components/activate/mysql.running
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + local -r component=hive-server2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + touch /tmp/dataproc/components/activate/hive-server2.running
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + touch /tmp/dataproc/components/activate/spark.running
<13>Jun 28 16:02:43 startup-script[1131]: ++ logger -s -t 'activate-component-yarn[3052]'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + local -r component=tez
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + touch /tmp/dataproc/components/activate/tez.running
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + local -r component=hdfs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + local -r component=yarn
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:43 startup-script[1131]: + run_with_logger --tag setup-stackdriver-agent setup_service stackdriver-agent
<13>Jun 28 16:02:43 startup-script[1131]: + local tag=
<13>Jun 28 16:02:43 startup-script[1131]: + local pid=3041
<13>Jun 28 16:02:43 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:43 startup-script[1131]: + tag=setup-stackdriver-agent
<13>Jun 28 16:02:43 startup-script[1131]: + shift 2
<13>Jun 28 16:02:43 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: + setup_service stackdriver-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + touch /tmp/dataproc/components/activate/hdfs.running
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + local -r activate_script=/usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + [[ -f /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + echo 'Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: Running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + mkdir -p /tmp/dataproc/components/activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + touch /tmp/dataproc/components/activate/yarn.running
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + local exit_code=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: ++ logger -s -t 'setup-stackdriver-agent[3041]'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r start=1687968163.056996421
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + local -r start=1687968163.057212942
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + local -r start=1687968163.057768487
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + local -r start=1687968163.059571274
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + local -r start=1687968163.061856856
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + local -r start=1687968163.063786591
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + local -r start=1687968163.065507423
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + local -r start=1687968163.066757516
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /etc/environment
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_HOME=/usr/lib/spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ JAVA_HOME=/usr/lib/jvm/temurin-8-jdk-amd64
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ JAVA_CONFIG_DIR=/usr/lib/jvm/temurin-8-jdk-amd64/jre/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ BDUTIL_DIR=/usr/local/share/google/dataproc/bdutil
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ OS_BDUTIL_DIR=/usr/local/share/google/dataproc/bdutil/os/debian
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_AGENT_JAR=/usr/local/share/google/dataproc/dataproc-agent.jar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_STARTUP_SCRIPT=/usr/local/share/google/dataproc/startup-script.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_POST_HDFS_STARTUP_SCRIPT=/usr/local/share/google/dataproc/post-hdfs-startup-script.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_COMMON_PACKAGES='autofs bash-completion bc git jq netcat vim wget bigtop-utils hadoop-client hadoop-lzo temurin-8-jdk google-fluentd=1.9.12-1 stackdriver-agent docker-ce druid flink hbase hdfs libhdfs0 hive-metastore hive-server2 hive-hcatalog hudi texlive-xetex texlive-fonts-recommended texlive-plain-generic kafka-server kerberos mapreduce miniconda3 mysql mysql-connector-j pig presto ranger rubix spark tez yarn zookeeper-server libapr1 libatlas3-base libjansi-java libopenblas-base libsnappy-dev libssl-dev libzstd-dev linux-headers-4.19.0-24-cloud-amd64 openssl uuid-runtime python-numpy python-pip python-requests python-setuptools linux-image-amd64 linux-headers-amd64 linux-kbuild-5.10=5.10.179-1~deb10u1'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_MASTER_SERVICES='dpms-proxy earlyoom hadoop-hdfs-namenode hive-metastore hive-server2 hive-webhcat-server knox proxy-agent solr-server spark-history-server hadoop-yarn-resourcemanager'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_MASTER_EXCLUSIVE_SERVICES='jupyter hadoop-mapreduce-historyserver mysql-server hadoop-yarn-timelineserver zeppelin'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_MASTER_STANDALONE_SERVICES=hadoop-hdfs-secondarynamenode
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_MASTER_HA_SERVICES='hadoop-hdfs-journalnode hadoop-hdfs-zkfc'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ TWO_MASTER_HA_SERVICES=
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_WORKER_SERVICES='hadoop-hdfs-datanode hadoop-yarn-nodemanager'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_IMAGE_TYPE=standard
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_IMAGE_VERSION=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_IMAGE_BUILD=20230611-234341-RC01-2_0_deb10_20230601_231239-RC01
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_OPTIONAL_COMPONENTS='docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_NON_DEBIAN_COMPONENTS='docker-ce dpms-proxy druid earlyoom flink hbase hdfs hive-metastore hive-server2 hive-webhcat-server hudi jupyter kafka-server kerberos knox mapreduce miniconda3 mysql pig presto proxy-agent ranger rubix solr-server spark tez yarn zeppelin zookeeper-server'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_GENERATED_KEYS_DIR=/mnt/cluster-keys-ram-disk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_COMPONENTS_DEFAULT=hdfs,yarn,mapreduce,mysql,pig,tez,hive-metastore,hive-server2,spark,earlyoom
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HADOOP_HOME=/usr/lib/hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_VERSION=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + local -r start=1687968163.083942399
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_components.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + local -r start=1687968163.089314159
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + bash -ex /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + local -r end=1687968163.091965946
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + local -r runtime_s=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + echo 'Component tez took 0s to activate'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: Component tez took 0s to activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + local -r time_file=/tmp/dataproc/components/activate/tez.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + touch /tmp/dataproc/components/activate/tez.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + local -r service=stackdriver-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + enable_service stackdriver-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + local -r service=stackdriver-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + local -r unit=stackdriver-agent.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + retry_constant_short systemctl enable stackdriver-agent.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + retry_constant_custom 30 1 systemctl enable stackdriver-agent.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + local -r max_retry_time=30
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + local -r retry_delay=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + cmd=("${@:3}")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + local -r cmd
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + local -r max_retries=30
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + local reenable_x=false
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + [[ -o xtrace ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: + set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: About to run 'systemctl enable stackdriver-agent.service' with retries...
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: stackdriver-agent.service is not a native service, redirecting to systemd-sysv-install.
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-stackdriver-agent[3041]: Executing: /lib/systemd/systemd-sysv-install enable stackdriver-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + cat
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ WORKERS=()
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + local -r end=1687968163.119252895
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + local -r runtime_s=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + echo 'Component mysql took 0s to activate'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: Component mysql took 0s to activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + local -r time_file=/tmp/dataproc/components/activate/mysql.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + touch /tmp/dataproc/components/activate/mysql.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_logging.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ WORKERS=()
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/tez.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-tez[3051]: + touch /tmp/dataproc/components/activate/tez.done
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + cat
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_logging.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ WORKERS=()
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_retry.sh
<13>Jun 28 16:02:43 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_logging.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ WORKERS=()
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_properties.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mysql.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mysql[3048]: + touch /tmp/dataproc/components/activate/mysql.done
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_logging.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ WORKERS=()
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_retry.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_retry.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_components.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_logging.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_properties.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_networking.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_logging.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + local -r end=1687968163.249183480
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + local -r runtime_s=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + echo 'Component pig took 0s to activate'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: Component pig took 0s to activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + local -r time_file=/tmp/dataproc/components/activate/pig.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + touch /tmp/dataproc/components/activate/pig.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_networking.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ source /usr/local/share/google/dataproc/bdutil/image_config/2.0.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_retry.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ source /usr/local/share/google/dataproc/bdutil/components/shared/conda.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ export CONDA_HOME=/opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ CONDA_HOME=/opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ set -euxo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ export MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ export MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ export PATH=/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ PATH=/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ BIGTABLE_HBASE_CLIENT_CONNECTION_IMPL=com.google.cloud.bigtable.hbase2_x.BigtableConnection
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_retry.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ BIGTABLE_HBASE_CLIENT_JAR_PREFIX=bigtable-hbase-2.x-hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ BIGTABLE_HBASE_CLIENT_VERSION=1.26.2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_services.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ CONDA_VERSION=4.9
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_networking.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ DOCKER_VERSION=19.03
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ DPMS_PROXY_DOCKER_IMAGE_VERSION=v0.0.24
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_properties.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ DPMS_PROXY_HIVE_VERSIONS=("3.1.2")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly DPMS_PROXY_HIVE_VERSIONS
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + cat
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ GCS_CONNECTOR_VERSION=hadoop3-2.2.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_networking.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ readonly IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ IPTABLES_RESTORE_SCRIPT=/usr/local/share/google/dataproc/iptables_restore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ readonly IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ IPTABLES_SAVE_LOCATION=/usr/local/share/google/dataproc/iptables_saved
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ HUDI_VERSION=0.12.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ JAVA_PACKAGE=temurin-8-jdk
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ MINICONDA_VERSION=Miniconda3-py38_4.9.2-Linux-x86_64.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ PARQUET_VERSION=1.11.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ PYTHON_VERSION=3.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_components.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + set -x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + [[ Worker == \M\a\s\t\e\r ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + echo 'Skip running MapReduce history server on Worker node mjtelco-w-2'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: Skip running MapReduce history server on Worker node mjtelco-w-2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ R_APT_SITE=http://cloud.r-project.org
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/effective-python.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ R_REPO=cran40
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ R_VERSION=4.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ source /usr/local/share/google/dataproc/bdutil/components/shared/conda.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ export CONDA_HOME=/opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ CONDA_HOME=/opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ export EFFECTIVE_PYTHON_PROFILE=/etc/profile.d/effective-python.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ EFFECTIVE_PYTHON_PROFILE=/etc/profile.d/effective-python.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ SCALA_VERSION=2.12.14
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ SPARK_MINOR_VERSION=3.1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ readonly TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ TINKEY_VERSION=1.5.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ [[ /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin == *:/snap/bin* ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ DATAPROC_DIR=/usr/local/share/google/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ DATAPROC_TMP_DIR=/tmp/dataproc
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ DATAPROC_COMMON_LIB_DIR=/usr/local/share/google/dataproc/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/pig.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-pig[3049]: + touch /tmp/dataproc/components/activate/pig.done
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ DATAPROC_ARTIFACTS_DIR=/usr/local/share/google/dataproc/artifacts
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ DATAPROC_CONF_DIR=/usr/local/share/google/dataproc/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_properties.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ DATAPROC_METADATA_CONF=/usr/local/share/google/dataproc/conf/dataproc_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ COMMANDS_TMP_DIR=/tmp/dataproc/commands
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ DATAPROC_COMPONENTS_TMP_DIR=/tmp/dataproc/components
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ POST_HDFS_ENV=/usr/local/share/google/dataproc/bdutil/components/post_hdfs_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ [[ -f /usr/local/share/google/dataproc/conf/dataproc_metadata.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ INSTALL_GCS_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ INSTALL_BIGQUERY_CONNECTOR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ ENABLE_HDFS=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HADOOP_INSTALL_DIR=/usr/lib/hadoop
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HADOOP_LIB_DIR=/usr/lib/hadoop/lib
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HADOOP_CONF_DIR=/etc/hadoop/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HADOOP_MASTER_MAPREDUCE_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HDFS_MASTER_MEMORY_FRACTION=0.4
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_retry.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ NODEMANAGER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ NUM_WORKERS=10
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ WORKERS=()
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ CORES_PER_MAP_TASK=1.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ CORES_PER_REDUCE_TASK=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ CORES_PER_APP_MASTER=2.0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HDFS_DATA_DIRS_PERM=700
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/configure_conda_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_WORKER_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_EXECUTOR_MEMORY_FRACTION=0.8
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_DAEMON_MEMORY_FRACTION=0.15
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + set -x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + install_effective_python_profile
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + [[ ! -f /etc/profile.d/effective-python.sh ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + generate_effective_python_profile
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + is_component_selected miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r component=miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local activated_components
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_EXECUTORS_PER_VM=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ TEZ_LIB_DIR=/usr/lib/tez
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_properties.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ [[ -v BIGTABLE_HBASE_CLIENT_VERSION ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ BIGTABLE_HBASE_CLIENT_JAR=bigtable-hbase-2.x-hadoop-1.26.2.jar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ BIGTABLE_HBASE_CONF_DIR=/etc/bigtable-hbase/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HDFS_METRIC_SOURCE=hdfs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HIVESERVER2_METRIC_SOURCE=hiveserver2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HIVEMETASTORE_METRIC_SOURCE=hivemetastore
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ MONITORING_AGENT_DEFAULTS_SOURCE=monitoringAgentDefaults
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_HISTORY_SERVER_METRIC_SOURCE=sparkHistoryServer
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_METRIC_SOURCE=spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ YARN_METRIC_SOURCE=yarn
<13>Jun 28 16:02:43 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HDFS_COLLECTD_CONF_FILE_NAME=collectd_hdfs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HIVESERVER2_COLLECTD_CONF_FILE_NAME=collectd_hiveserver2_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ HIVEMETASTORE_COLLECTD_CONF_FILE_NAME=collectd_hivemetastore_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME=collectd_shs_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_COLLECTD_CONF_FILE_NAME=collectd_spark_yarn_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ YARN_COLLECTD_CONF_FILE_NAME=collectd_yarn_jmx_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ PROCESSES_DEFAULT_METRICS_CONF_FILE_NAME=collectd_processes_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ SPARK_DEFAULT_METRICS_CONF_FILE_NAME=collectd_spark_default_metrics.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ COLLECTD_CONF_FILE_NAMES=($HDFS_COLLECTD_CONF_FILE_NAME $HIVESERVER2_COLLECTD_CONF_FILE_NAME $HIVEMETASTORE_COLLECTD_CONF_FILE_NAME $SPARK_HISTORY_SERVER_COLLECTD_CONF_FILE_NAME $SPARK_COLLECTD_CONF_FILE_NAME $YARN_COLLECTD_CONF_FILE_NAME)
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ get_components_to_activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + local -r end=1687968163.357023418
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + local -r runtime_s=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + echo 'Component mapreduce took 0s to activate'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: Component mapreduce took 0s to activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + local -r time_file=/tmp/dataproc/components/activate/mapreduce.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + touch /tmp/dataproc/components/activate/mapreduce.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_components.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_components.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_services.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_properties.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_services.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + cat
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_versions.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++ source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ set -x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ get_metadata_master
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_os.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ is_rocky
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_services.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/mapreduce.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-mapreduce[3046]: + touch /tmp/dataproc/components/activate/mapreduce.done
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_os.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ is_rocky
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ os_id
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ cut -d= -f2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ export SPARK_HOME=/usr/lib/spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_HOME=/usr/lib/spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ export SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_CONF_DIR=/etc/spark/conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ export SPARK_AUTH_SECRET_FILE=/tmp/cluster/spark.auth.secret
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_AUTH_SECRET_FILE=/tmp/cluster/spark.auth.secret
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ export SPARK_DEFAULT_IMAGE_FILE=/etc/dataproc/spark-default-image.tar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_DEFAULT_IMAGE_FILE=/etc/dataproc/spark-default-image.tar
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ export SPARK_DATA_DIR=/hadoop/spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_DATA_DIR=/hadoop/spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ export SPARK_LOG_DIR=/var/log/spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_LOG_DIR=/var/log/spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ export SPARK_TMP_DIR=/hadoop/spark/tmp
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_TMP_DIR=/hadoop/spark/tmp
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ export SPARK_WORK_DIR=/hadoop/spark/work
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ SPARK_WORK_DIR=/hadoop/spark/work
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + source /usr/local/share/google/dataproc/dataproc_env.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ CLUSTER_NAME=mjtelco
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ CLUSTER_UUID=7db0aaf5-a842-4945-a93b-c6d17e834515
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ CONFIGBUCKET=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ TEMP_BUCKET=dataproc-temp-us-east1-746779145865-aaezsxz3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ HCFS_ROOT_URI=hdfs://mjtelco-m
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ MASTER_HOSTNAME_0=mjtelco-m
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ DATAPROC_MASTER_FQDN=mjtelco-m.us-east1-b.c.qwiklabs-gcp-03-e04e71dd72c2.internal
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ MASTER_HOSTNAMES=(mjtelco-m)
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ NUM_MASTERS=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ NUM_WORKERS=5
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ PREFIX=mjtelco
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ PROJECT=qwiklabs-gcp-03-e04e71dd72c2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ ROLE=Worker
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + set -x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + readonly WAIT_TIMEOUT_SECONDS=200
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + WAIT_TIMEOUT_SECONDS=200
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + readonly INITIAL_WORKER_COUNT=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + INITIAL_WORKER_COUNT=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_components.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_services.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ os_id
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ set -x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ get_metadata_master
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ xargs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ get_dataproc_property_or_default dataproc:componentgateway.ha.enabled false
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: +++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ cut -d= -f2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_properties.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ xargs
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_retry.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_versions.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: ++ echo false
<13>Jun 28 16:02:43 startup-script[1131]: + hermetic_vm=false
<13>Jun 28 16:02:43 startup-script[1131]: + [[ false == \t\r\u\e ]]
<13>Jun 28 16:02:43 startup-script[1131]: + return 1
<13>Jun 28 16:02:43 startup-script[1131]: + loginfo 'Starting service google-osconfig-agent'
<13>Jun 28 16:02:43 startup-script[1131]: + echo 'Starting service google-osconfig-agent'
<13>Jun 28 16:02:43 startup-script[1131]: Starting service google-osconfig-agent
<13>Jun 28 16:02:43 startup-script[1131]: + run_in_background --tag start-osconfig-service enable_and_start_service google-osconfig-agent
<13>Jun 28 16:02:43 startup-script[1131]: + local -r pid=3269
<13>Jun 28 16:02:43 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:43 startup-script[1131]: + shift 2
<13>Jun 28 16:02:43 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3269.running ]]
<13>Jun 28 16:02:43 startup-script[1131]: + echo 'enable_and_start_service google-osconfig-agent'
<13>Jun 28 16:02:43 startup-script[1131]: + echo 'Started background process [enable_and_start_service google-osconfig-agent] as pid 3269'
<13>Jun 28 16:02:43 startup-script[1131]: Started background process [enable_and_start_service google-osconfig-agent] as pid 3269
<13>Jun 28 16:02:43 startup-script[1131]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *miniconda3* ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_services.sh
<13>Jun 28 16:02:43 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:43 startup-script[1131]: + run_with_logger --tag start-osconfig-service enable_and_start_service google-osconfig-agent
<13>Jun 28 16:02:43 startup-script[1131]: + local tag=
<13>Jun 28 16:02:43 startup-script[1131]: + local pid=3269
<13>Jun 28 16:02:43 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:43 startup-script[1131]: + tag=start-osconfig-service
<13>Jun 28 16:02:43 startup-script[1131]: + shift 2
<13>Jun 28 16:02:43 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: + enable_and_start_service google-osconfig-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: ++ logger -s -t 'start-osconfig-service[3269]'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ [[ debian == \r\o\c\k\y ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ source /usr/local/share/google/dataproc/bdutil/os/debian/bdutil_helpers.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_logging.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ source /usr/local/share/google/dataproc/bdutil/os/shared.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ readonly EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ APT_SENTINEL=apt.lastupdate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ EXIT_CODE_INTERNAL_ERROR=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ readonly EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ EXIT_CODE_CLIENT_ERROR=2
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + set -x
<13>Jun 28 16:02:43 startup-script[1131]: ++ get_dataproc_property dataproc.logging.stackdriver.enable
<13>Jun 28 16:02:43 startup-script[1131]: ++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ get_dataproc_property yarn.atsv2.bigtable.instance
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hdfs.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ HCFS_DIRS=("/ hdfs:hadoop 1777" "/tmp hdfs:hadoop 1777" "/tmp/hadoop-yarn/staging yarn:hadoop 1777" "/tmp/hadoop-yarn/staging/history yarn:hadoop 755" "/user hdfs:hadoop 755" "/var hdfs:hadoop 775" "/var/tmp hdfs:hadoop 1777")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/miniconda3.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ source /usr/local/share/google/dataproc/bdutil/components/shared/conda.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ export CONDA_HOME=/opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: +++ CONDA_HOME=/opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ set -euxo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ export MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ MINICONDA3_INSTALL_PATH=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ export MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ MINICONDA3_BIN_DIR=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ export PATH=/opt/conda/miniconda3/bin:/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ PATH=/opt/conda/miniconda3/bin:/opt/conda/miniconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + [[ /opt/conda/miniconda3 != \/\o\p\t\/\c\o\n\d\a\/\d\e\f\a\u\l\t ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + ln -f -s /opt/conda/miniconda3 /opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ is_component_selected kerberos
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ local -r component=kerberos
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ local activated_components
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + readonly IS_COMPONENT_GATEWAY_HA_ENABLED=false
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + IS_COMPONENT_GATEWAY_HA_ENABLED=false
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + activate_spark
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + should_start_history_server
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + [[ Worker == \M\a\s\t\e\r ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + local -r service=google-osconfig-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + enable_service google-osconfig-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + local -r service=google-osconfig-agent
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + local -r unit=google-osconfig-agent.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + retry_constant_short systemctl enable google-osconfig-agent.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + retry_constant_custom 30 1 systemctl enable google-osconfig-agent.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + local -r max_retry_time=30
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + local -r retry_delay=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + cmd=("${@:3}")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + local -r cmd
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + local -r max_retries=30
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + local reenable_x=false
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + [[ -o xtrace ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: + set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: About to run 'systemctl enable google-osconfig-agent.service' with retries...
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + emit_conda_profile /opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r conda_dir=/opt/conda/default
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r python_bin=/opt/conda/default/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local temp
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++++ get_components_to_activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + local -r end=1687968163.644997562
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + local -r runtime_s=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + echo 'Component spark took 0s to activate'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: Component spark took 0s to activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + local -r time_file=/tmp/dataproc/components/activate/spark.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + touch /tmp/dataproc/components/activate/spark.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++++ tr '[:upper:]' '[:lower:]'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++++ get_dataproc_property dataproc.components.activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + cat
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ get_metadata_master_additional
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: + STACKDRIVER_LOGGING_ENABLED=
<13>Jun 28 16:02:43 startup-script[1131]: + [[ '' == \f\a\l\s\e ]]
<13>Jun 28 16:02:43 startup-script[1131]: + loginfo 'Stackdriver enabled; enabling google-fluentd.'
<13>Jun 28 16:02:43 startup-script[1131]: + echo 'Stackdriver enabled; enabling google-fluentd.'
<13>Jun 28 16:02:43 startup-script[1131]: Stackdriver enabled; enabling google-fluentd.
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ mktemp
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/spark.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-spark[3050]: + touch /tmp/dataproc/components/activate/spark.done
<13>Jun 28 16:02:43 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:43 startup-script[1131]: + source /usr/local/share/google/dataproc/bdutil/configure_fluentd.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + readonly ATSV2_BIGTABLE_RESOURCE=
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + ATSV2_BIGTABLE_RESOURCE=
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + [[ -n '' ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ activated_components='hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ [[ hdfs yarn mapreduce mysql pig tez hive-metastore hive-server2 spark earlyoom miniconda3 == *kerberos* ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ echo false
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: ++ KERBEROS_ENABLED=false
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: ++ date +%s.%N
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ get_property_in_xml /etc/hadoop/conf/hdfs-site.xml dfs.webhdfs.enabled true
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hdfs[3043]: +++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + local -r end=1687968163.708305770
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + local -r runtime_s=0
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + echo 'Component yarn took 0s to activate'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: Component yarn took 0s to activate
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + local -r time_file=/tmp/dataproc/components/activate/yarn.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + touch /tmp/dataproc/components/activate/yarn.time
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 start-osconfig-service[3269]: Created symlink /etc/systemd/system/multi-user.target.wants/google-osconfig-agent.service → /lib/systemd/system/google-osconfig-agent.service.
<13>Jun 28 16:02:43 startup-script[1131]: ++ set -e
<13>Jun 28 16:02:43 startup-script[1131]: ++ set -u
<13>Jun 28 16:02:43 startup-script[1131]: ++ loginfo 'Running configure_fluentd.sh'
<13>Jun 28 16:02:43 startup-script[1131]: ++ echo 'Running configure_fluentd.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + temp=/tmp/tmp.aUY4ZQvChL
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + ln -f -s /opt/conda/default/etc/profile.d/conda.sh /etc/profile.d/conda.sh
<13>Jun 28 16:02:43 startup-script[1131]: Running configure_fluentd.sh
<13>Jun 28 16:02:43 startup-script[1131]: ++ FLUENTD_BASE_DIR=/etc/google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: ++ FLUENTD_CONF_DIR=/etc/google-fluentd/config.d
<13>Jun 28 16:02:43 startup-script[1131]: ++ FLUENTD_PLUGIN_DIR=/etc/google-fluentd/plugin
<13>Jun 28 16:02:43 startup-script[1131]: ++ mkdir -p /etc/google-fluentd/config.d
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ get_metadata_master_additional
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + cat
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ DATAPROC_MASTER_ADDITIONAL=
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ NUM_MASTERS=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + chmod +x /opt/conda/default/etc/profile.d/conda.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ get_metadata_role
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: ++ mkdir -p /etc/google-fluentd/plugin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + cat
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + mv -n -v /tmp/tmp.aUY4ZQvChL /etc/profile.d/effective-python.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/yarn.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-yarn[3052]: + touch /tmp/dataproc/components/activate/yarn.done
<13>Jun 28 16:02:43 startup-script[1131]: ++ sed -i -e 's/enable_monitoring true/enable_monitoring false/' /etc/google-fluentd/google-fluentd.conf
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ DATAPROC_MASTER_ADDITIONAL=
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: +++ NUM_MASTERS=1
<13>Jun 28 16:02:43 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:43 startup-script[1131]: ++ [[ standard == \s\e\r\v\e\r\l\e\s\s\-\s\p\a\r\k ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ get_metadata_role
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-server2[3045]: ++++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: ++ cp -r /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc-agent.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc-dpms-proxy.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc-startup.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc-yarn-userlogs.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/dataproc_fluentd.conf /usr/local/share/google/dataproc/bdutil/fluentd/config.d/metadata-proxy.conf /etc/google-fluentd/config.d
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: renamed '/tmp/tmp.aUY4ZQvChL' -> '/etc/profile.d/effective-python.sh'
<13>Jun 28 16:02:43 startup-script[1131]: ++ cp -r /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/filter_add_insert_ids.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/in_object_space_dump.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/monitoring.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/out_google_cloud.rb /usr/local/share/google/dataproc/bdutil/fluentd/job_logging/plugin/statusz.rb /etc/google-fluentd/plugin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + chmod a+r /etc/profile.d/effective-python.sh
<13>Jun 28 16:02:43 startup-script[1131]: +++ get_dataproc_property dataproc.logging.stackdriver.job.driver.enable
<13>Jun 28 16:02:43 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + rm -Rf /tmp/tmp.aUY4ZQvChL
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + customize_conda_env /opt/conda/miniconda3 /opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r conda_install_path=/opt/conda/miniconda3
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r conda_bin_dir=/opt/conda/miniconda3/bin
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ get_dataproc_property conda.env.config.uri
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: ++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: ++ JOB_DRIVER_LOGGING_ENABLED=
<13>Jun 28 16:02:43 startup-script[1131]: ++ [[ '' == \t\r\u\e ]]
<13>Jun 28 16:02:43 startup-script[1131]: +++ get_dataproc_property dataproc.logging.stackdriver.job.yarn.container.enable
<13>Jun 28 16:02:43 startup-script[1131]: +++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ ROLE=Worker
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ [[ 1 -gt 1 ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ CLUSTER_MASTER_METASTORE_URIS=thrift://mjtelco-m:9083
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Jun 28 16:02:43 startup-script[1131]: ++ CONTAINER_LOGGING_ENABLED=
<13>Jun 28 16:02:43 startup-script[1131]: ++ [[ '' == \t\r\u\e ]]
<13>Jun 28 16:02:43 startup-script[1131]: + run_in_background --tag setup-google-fluentd setup_service google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: + local -r pid=3404
<13>Jun 28 16:02:43 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:43 startup-script[1131]: + shift 2
<13>Jun 28 16:02:43 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3404.running ]]
<13>Jun 28 16:02:43 startup-script[1131]: + echo 'setup_service google-fluentd'
<13>Jun 28 16:02:43 startup-script[1131]: + echo 'Started background process [setup_service google-fluentd] as pid 3404'
<13>Jun 28 16:02:43 startup-script[1131]: Started background process [setup_service google-fluentd] as pid 3404
<13>Jun 28 16:02:43 startup-script[1131]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:43 startup-script[1131]: + [[ us-east1 != \g\l\o\b\a\l ]]
<13>Jun 28 16:02:43 startup-script[1131]: + add_regional_bigtop_repo us-east1
<13>Jun 28 16:02:43 startup-script[1131]: + local -r region=us-east1
<13>Jun 28 16:02:43 startup-script[1131]: + local -r dataproc_repo_file=/etc/apt/sources.list.d/dataproc.list
<13>Jun 28 16:02:43 startup-script[1131]: + is_test_bigtop_repo /etc/apt/sources.list.d/dataproc.list
<13>Jun 28 16:02:43 startup-script[1131]: + local -r dataproc_repo_file=/etc/apt/sources.list.d/dataproc.list
<13>Jun 28 16:02:43 startup-script[1131]: + return '!' grep -q dataproc-bigtop-repo /etc/apt/sources.list.d/dataproc.list
<13>Jun 28 16:02:43 startup-script[1131]: /usr/local/share/google/dataproc/bdutil/os/shared.sh: line 8: return: !: numeric argument required
<13>Jun 28 16:02:43 startup-script[1131]: + local regional_bigtop_repo_uri
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: + source /usr/local/share/google/dataproc/bdutil/components/activate/../shared/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ set -euo pipefail
<13>Jun 28 16:02:43 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:43 startup-script[1131]: + run_with_logger --tag setup-google-fluentd setup_service google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: + local tag=
<13>Jun 28 16:02:43 startup-script[1131]: + local pid=3404
<13>Jun 28 16:02:43 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:43 startup-script[1131]: + tag=setup-google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: + shift 2
<13>Jun 28 16:02:43 startup-script[1131]: + [[ 2 -eq 0 ]]
<13>Jun 28 16:02:43 startup-script[1131]: + setup_service google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ dirname /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:43 startup-script[1131]: ++ logger -s -t 'setup-google-fluentd[3404]'
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ source /usr/local/share/google/dataproc/bdutil/components/activate/../../bdutil_metadata.sh
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: ++ set -x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ get_metadata_master
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-hive-metastore[3044]: +++ set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + local -r service=google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + enable_service google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + local -r service=google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + local -r unit=google-fluentd.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + retry_constant_short systemctl enable google-fluentd.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + retry_constant_custom 30 1 systemctl enable google-fluentd.service
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + local -r max_retry_time=30
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + local -r retry_delay=1
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + cmd=("${@:3}")
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + local -r cmd
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + local -r max_retries=30
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + local reenable_x=false
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + [[ -o xtrace ]]
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: + set +x
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: About to run 'systemctl enable google-fluentd.service' with retries...
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: google-fluentd.service is not a native service, redirecting to systemd-sysv-install.
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 setup-google-fluentd[3404]: Executing: /lib/systemd/systemd-sysv-install enable google-fluentd
<13>Jun 28 16:02:43 startup-script[1131]: <13>Jun 28 16:02:43 activate-component-miniconda3[3047]: + local -r conda_env_config_uri=
<13>Jun 28 16:02:43 startup-script[1131]: ++ sed s#dataproc-bigtop-repo#goog-dataproc-bigtop-repo-us-east1#
<13>Jun 28 16:02:43 startup-script[1131]: ++ cat /etc/apt/sources.list.d/dataproc.list
<13>Jun 28 16:02:43 startup-script[1131]: ++ grep 'deb .*goog-dataproc-bigtop-repo-us-east1.* dataproc contrib'
<13>Jun 28 16:02:44 startup-script[1131]: ++ cut -d ' ' -f 2
<13>Jun 28 16:02:44 startup-script[1131]: ++ head -1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: ++ get_dataproc_property conda.packages
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: ++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: + regional_bigtop_repo_uri=https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-east1/2_0_deb10_20230601_231239-RC01
<13>Jun 28 16:02:44 startup-script[1131]: + [[ https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-east1/2_0_deb10_20230601_231239-RC01 == */ ]]
<13>Jun 28 16:02:44 startup-script[1131]: + local -r bigtop_key_uri=https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-east1/2_0_deb10_20230601_231239-RC01/archive.key
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: 'systemctl enable google-osconfig-agent.service' succeeded after 1 execution(s).
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + return 0
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r drop_in_dir=/etc/systemd/system/google-osconfig-agent.service.d
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + mkdir -p /etc/systemd/system/google-osconfig-agent.service.d
<13>Jun 28 16:02:44 startup-script[1131]: + apt-key add -
<13>Jun 28 16:02:44 startup-script[1131]: + curl -fsS --retry-connrefused --retry 3 --retry-delay 5 https://storage.googleapis.com/goog-dataproc-bigtop-repo-us-east1/2_0_deb10_20230601_231239-RC01/archive.key
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local props
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ retry_constant_short systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ retry_constant_custom 30 1 systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ local -r max_retry_time=30
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ local -r retry_delay=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ cmd=("${@:3}")
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ local -r cmd
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ local -r max_retries=30
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ local reenable_x=false
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ [[ -o xtrace ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: About to run 'systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit' with retries...
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: +++ ROLE=Worker
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: +++ [[ 1 -gt 1 ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: +++ CLUSTER_MASTER_METASTORE_URIS=thrift://mjtelco-m:9083
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: ++ HIVE_CONF_DIR=/etc/hive/conf
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + set -x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + activate_hive_server2
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + [[ Worker == \M\a\s\t\e\r ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + echo 'Skip running Hive server on Worker node mjtelco-w-2'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: Skip running Hive server on Worker node mjtelco-w-2
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r conda_packages=
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: ++ date +%s.%N
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: ++ get_dataproc_property pip.packages
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: ++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + local -r end=1687968164.140106543
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + local -r runtime_s=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + echo 'Component hive-server2 took 1s to activate'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: Component hive-server2 took 1s to activate
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + local -r time_file=/tmp/dataproc/components/activate/hive-server2.time
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + touch /tmp/dataproc/components/activate/hive-server2.time
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: +++ get_metadata_master_additional
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: +++ get_dataproc_metadata DATAPROC_METADATA_MASTER_ADDITIONAL attributes/dataproc-master-additional
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: +++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + cat
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-server2.sh
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-server2[3045]: + touch /tmp/dataproc/components/activate/hive-server2.done
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r pip_packages=
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + [[ -n '' ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + retry_constant_custom 4 1 install_conda_packages /opt/conda/miniconda3/bin ''
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r max_retry_time=4
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r retry_delay=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + cmd=("${@:3}")
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r cmd
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r max_retries=4
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local reenable_x=false
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + [[ -o xtrace ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: About to run 'install_conda_packages /opt/conda/miniconda3/bin ' with retries...
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: 'install_conda_packages /opt/conda/miniconda3/bin ' succeeded after 1 execution(s).
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + return 0
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + retry_constant_custom 4 1 install_pip_packages ''
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r max_retry_time=4
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r retry_delay=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + cmd=("${@:3}")
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r cmd
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r max_retries=4
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local reenable_x=false
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + [[ -o xtrace ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: About to run 'install_pip_packages ' with retries...
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: 'install_pip_packages ' succeeded after 1 execution(s).
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + return 0
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: ++ date +%s.%N
<13>Jun 28 16:02:44 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r end=1687968164.208892817
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r runtime_s=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + echo 'Component miniconda3 took 1s to activate'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: Component miniconda3 took 1s to activate
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + local -r time_file=/tmp/dataproc/components/activate/miniconda3.time
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + touch /tmp/dataproc/components/activate/miniconda3.time
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + cat
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/miniconda3.sh
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-miniconda3[3047]: + touch /tmp/dataproc/components/activate/miniconda3.done
<13>Jun 28 16:02:44 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ DATAPROC_MASTER_ADDITIONAL=
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ MASTER_HOSTNAMES=($DATAPROC_MASTER ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ NUM_MASTERS=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: +++ get_metadata_role
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: +++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: +++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: 'systemctl show google-osconfig-agent.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: ++ return 0
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + props='Restart=always
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: RemainAfterExit=no'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ google-osconfig-agent != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ google-osconfig-agent != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ Restart=always
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: RemainAfterExit=no == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ google-osconfig-agent == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ google-osconfig-agent == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + start_service google-osconfig-agent
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r service=google-osconfig-agent
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r unit=google-osconfig-agent.service
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + retry_constant_short systemctl start google-osconfig-agent.service
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + retry_constant_custom 30 1 systemctl start google-osconfig-agent.service
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r max_retry_time=30
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r retry_delay=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + cmd=("${@:3}")
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r cmd
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local -r max_retries=30
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + local reenable_x=false
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + [[ -o xtrace ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: + set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 start-osconfig-service[3269]: About to run 'systemctl start google-osconfig-agent.service' with retries...
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ ROLE=Worker
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ [[ 1 -gt 1 ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ CLUSTER_MASTER_METASTORE_URIS=thrift://mjtelco-m:9083
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + set -x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + HIVE_CONF_DIR=/etc/hive/conf
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ get_metadata_role
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ get_dataproc_metadata DATAPROC_METADATA_ROLE attributes/dataproc-role
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + ROLE=Worker
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ get_metadata_master
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ get_dataproc_metadata DATAPROC_METADATA_MASTER attributes/dataproc-master
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: ++ WEBHDFS_ENABLED=true
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: ++ WEBHDFS_BASE_URI=
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + set -x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + MASTER_HOSTNAMES=(${DATAPROC_MASTER} ${DATAPROC_MASTER_ADDITIONAL//,/ })
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: ++ get_metadata_cluster_name
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: ++ get_dataproc_metadata DATAPROC_METADATA_CLUSTER_NAME attributes/dataproc-cluster-name
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: ++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + DATAPROC_MASTER=mjtelco-m
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + activate_hive_metastore
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + [[ Worker == \M\a\s\t\e\r ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + echo 'Skip running Hive metastore on Worker node mjtelco-w-2'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: Skip running Hive metastore on Worker node mjtelco-w-2
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: ++ date +%s.%N
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + local -r end=1687968164.564129690
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + CLUSTER_NAME=mjtelco
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + activate_hdfs
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + local -r runtime_s=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + mkdir -p /var/run/hadoop-hdfs
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + echo 'Component hive-metastore took 1s to activate'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: Component hive-metastore took 1s to activate
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + local -r time_file=/tmp/dataproc/components/activate/hive-metastore.time
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + touch /tmp/dataproc/components/activate/hive-metastore.time
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + sudo -u hdfs hdfs namenode -genclusterid
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + chown root:hdfs /var/run/hadoop-hdfs
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + cat
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + chmod 775 /var/run/hadoop-hdfs
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + [[ Worker == \M\a\s\t\e\r ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + enable_worker_services
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: ++ get_metadata_datanode_enabled
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: ++ get_dataproc_metadata DATAPROC_METADATA_DATANODE_ENABLED attributes/dataproc-datanode-enabled
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: ++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh'
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hive-metastore.sh
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hive-metastore[3044]: + touch /tmp/dataproc/components/activate/hive-metastore.done
<13>Jun 28 16:02:44 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:44 startup-script[1131]: Warning: apt-key output should not be parsed (stdout is not a terminal)
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + local -r datanode_enabled=true
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + [[ true == \t\r\u\e ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + enable_service hadoop-hdfs-datanode
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + local -r service=hadoop-hdfs-datanode
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + local -r unit=hadoop-hdfs-datanode.service
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + retry_constant_short systemctl enable hadoop-hdfs-datanode.service
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + retry_constant_custom 30 1 systemctl enable hadoop-hdfs-datanode.service
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + local -r max_retry_time=30
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + local -r retry_delay=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + cmd=("${@:3}")
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + local -r cmd
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + local -r max_retries=30
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + local reenable_x=false
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + [[ -o xtrace ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: + set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: About to run 'systemctl enable hadoop-hdfs-datanode.service' with retries...
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: hadoop-hdfs-datanode.service is not a native service, redirecting to systemd-sysv-install.
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 activate-component-hdfs[3043]: Executing: /lib/systemd/systemd-sysv-install enable hadoop-hdfs-datanode
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: 'systemctl enable stackdriver-agent.service' succeeded after 1 execution(s).
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: + return 0
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: + local -r drop_in_dir=/etc/systemd/system/stackdriver-agent.service.d
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: + mkdir -p /etc/systemd/system/stackdriver-agent.service.d
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: + local props
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ retry_constant_short systemctl show stackdriver-agent.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ retry_constant_custom 30 1 systemctl show stackdriver-agent.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ local -r max_retry_time=30
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ local -r retry_delay=1
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ cmd=("${@:3}")
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ local -r cmd
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ local -r max_retries=30
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ local reenable_x=false
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ [[ -o xtrace ]]
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: ++ set +x
<13>Jun 28 16:02:44 startup-script[1131]: <13>Jun 28 16:02:44 setup-stackdriver-agent[3041]: About to run 'systemctl show stackdriver-agent.service -p Restart,RemainAfterExit' with retries...
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: 'systemctl show stackdriver-agent.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: ++ return 0
<13>Jun 28 16:02:45 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + props='Restart=no
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: RemainAfterExit=yes'
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ stackdriver-agent != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ stackdriver-agent != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ Restart=no
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ Restart=no
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ stackdriver-agent == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ stackdriver-agent == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + local am_on_primary_worker_enabled
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: ++ get_dataproc_property am.primary_only
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: ++ set +x
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 start-osconfig-service[3269]: 'systemctl start google-osconfig-agent.service' succeeded after 1 execution(s).
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 start-osconfig-service[3269]: + return 0
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + am_on_primary_worker_enabled=false
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ stackdriver-agent == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + local master_run_driver_location
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + master_run_driver_location=LOCAL
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ stackdriver-agent == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + retry_constant systemctl start stackdriver-agent
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + retry_constant_custom 300 1 systemctl start stackdriver-agent
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + local -r max_retry_time=300
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + local -r retry_delay=1
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + cmd=("${@:3}")
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + local -r cmd
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + local -r max_retries=300
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + local reenable_x=false
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + [[ -o xtrace ]]
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: + set +x
<13>Jun 28 16:02:45 startup-script[1131]: <13>Jun 28 16:02:45 setup-stackdriver-agent[3041]: About to run 'systemctl start stackdriver-agent' with retries...
<13>Jun 28 16:02:45 startup-script[1131]: OK
<13>Jun 28 16:02:46 startup-script[1131]: + echo 'Adding regional Bigtop repo for us-east1 in APT sources.'
<13>Jun 28 16:02:46 startup-script[1131]: Adding regional Bigtop repo for us-east1 in APT sources.
<13>Jun 28 16:02:46 startup-script[1131]: + cat
<13>Jun 28 16:02:46 startup-script[1131]: + cat /etc/apt/sources.list.d/dataproc.list
<13>Jun 28 16:02:46 startup-script[1131]: + mv -f /tmp/dataproc.list /etc/apt/sources.list.d/dataproc.list
<13>Jun 28 16:02:46 startup-script[1131]: + [[ standard == \s\t\a\n\d\a\r\d ]]
<13>Jun 28 16:02:46 startup-script[1131]: + run_in_background --tag backup-original-configs backup_original_configs
<13>Jun 28 16:02:46 startup-script[1131]: + local -r pid=3956
<13>Jun 28 16:02:46 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:46 startup-script[1131]: + shift 2
<13>Jun 28 16:02:46 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3956.running ]]
<13>Jun 28 16:02:46 startup-script[1131]: + echo backup_original_configs
<13>Jun 28 16:02:46 startup-script[1131]: + echo 'Started background process [backup_original_configs] as pid 3956'
<13>Jun 28 16:02:46 startup-script[1131]: Started background process [backup_original_configs] as pid 3956
<13>Jun 28 16:02:46 startup-script[1131]: + wait_on_async_processes
<13>Jun 28 16:02:46 startup-script[1131]: + loginfo 'Waiting on async processes'
<13>Jun 28 16:02:46 startup-script[1131]: + echo 'Waiting on async processes'
<13>Jun 28 16:02:46 startup-script[1131]: Waiting on async processes
<13>Jun 28 16:02:46 startup-script[1131]: + local running_file
<13>Jun 28 16:02:46 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:46 startup-script[1131]: + local pid
<13>Jun 28 16:02:46 startup-script[1131]: ++ basename /tmp/dataproc/commands/3041
<13>Jun 28 16:02:46 startup-script[1131]: + trap 'echo "$?" >"${COMMANDS_TMP_DIR}/${BASHPID}.exitcode"' EXIT
<13>Jun 28 16:02:46 startup-script[1131]: + run_with_logger --tag backup-original-configs backup_original_configs
<13>Jun 28 16:02:46 startup-script[1131]: + local tag=
<13>Jun 28 16:02:46 startup-script[1131]: + local pid=3956
<13>Jun 28 16:02:46 startup-script[1131]: + [[ --tag == \-\-\t\a\g ]]
<13>Jun 28 16:02:46 startup-script[1131]: + tag=backup-original-configs
<13>Jun 28 16:02:46 startup-script[1131]: + shift 2
<13>Jun 28 16:02:46 startup-script[1131]: + [[ 1 -eq 0 ]]
<13>Jun 28 16:02:46 startup-script[1131]: + backup_original_configs
<13>Jun 28 16:02:46 startup-script[1131]: + pid=3041
<13>Jun 28 16:02:46 startup-script[1131]: + local cmd
<13>Jun 28 16:02:46 startup-script[1131]: + cmd='setup_service stackdriver-agent'
<13>Jun 28 16:02:46 startup-script[1131]: + loginfo 'Waiting on pid=3041 cmd=[setup_service stackdriver-agent]'
<13>Jun 28 16:02:46 startup-script[1131]: + echo 'Waiting on pid=3041 cmd=[setup_service stackdriver-agent]'
<13>Jun 28 16:02:46 startup-script[1131]: Waiting on pid=3041 cmd=[setup_service stackdriver-agent]
<13>Jun 28 16:02:46 startup-script[1131]: + echo 'setup_service stackdriver-agent'
<13>Jun 28 16:02:46 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3041.exitcode
<13>Jun 28 16:02:46 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3041.exitcode ]]
<13>Jun 28 16:02:46 startup-script[1131]: + sleep 1
<13>Jun 28 16:02:46 startup-script[1131]: ++ logger -s -t 'backup-original-configs[3956]'
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + local -r HIVE_CONF_DIR=/etc/hive/conf
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + local -r TEZ_CONF_DIR=/etc/tez/conf
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + mkdir -p /usr/local/share/google/dataproc/conf/original
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + cp /etc/hadoop/conf/yarn-site.xml /usr/local/share/google/dataproc/conf/original/original-yarn-site.xml
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + cp /etc/hadoop/conf/hdfs-site.xml /usr/local/share/google/dataproc/conf/original/original-hdfs-site.xml
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: 'systemctl enable google-fluentd.service' succeeded after 1 execution(s).
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + return 0
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + cp /etc/hadoop/conf/core-site.xml /usr/local/share/google/dataproc/conf/original/original-core-site.xml
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local -r drop_in_dir=/etc/systemd/system/google-fluentd.service.d
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + mkdir -p /etc/systemd/system/google-fluentd.service.d
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local props
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ retry_constant_short systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + cp /etc/hadoop/conf/mapred-site.xml /usr/local/share/google/dataproc/conf/original/original-mapred-site.xml
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + cp /etc/hadoop/conf/capacity-scheduler.xml /usr/local/share/google/dataproc/conf/original/original-capacity-scheduler.xml
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ retry_constant_custom 30 1 systemctl show google-fluentd.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ local -r max_retry_time=30
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ local -r retry_delay=1
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ cmd=("${@:3}")
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ local -r cmd
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ local -r max_retries=30
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ local reenable_x=false
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ [[ -o xtrace ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ set +x
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: About to run 'systemctl show google-fluentd.service -p Restart,RemainAfterExit' with retries...
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + cp /etc/hive/conf/hive-site.xml /usr/local/share/google/dataproc/conf/original/original-hive-site.xml
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + cp /etc/tez/conf/tez-site.xml /usr/local/share/google/dataproc/conf/original/original-tez-site.xml
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 backup-original-configs[3956]: + cp /etc/spark/conf/spark-defaults.conf /usr/local/share/google/dataproc/conf/original/original-spark-defaults.conf
<13>Jun 28 16:02:46 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: 'systemctl enable hadoop-hdfs-datanode.service' succeeded after 1 execution(s).
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + return 0
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local -r common_restart_drop_in=/etc/systemd/system/common/restart.conf
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + [[ ! -f /etc/systemd/system/common/restart.conf ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local -r worker_restart_drop_in=/etc/systemd/system/common/worker-restart.conf
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + [[ ! -f /etc/systemd/system/common/worker-restart.conf ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local -r drop_in_dir=/etc/systemd/system/hadoop-hdfs-datanode.service.d
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + mkdir -p /etc/systemd/system/hadoop-hdfs-datanode.service.d
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local props
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ retry_constant_short systemctl show hadoop-hdfs-datanode.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ retry_constant_custom 30 1 systemctl show hadoop-hdfs-datanode.service -p Restart,RemainAfterExit
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ local -r max_retry_time=30
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ local -r retry_delay=1
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ cmd=("${@:3}")
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ local -r cmd
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ local -r max_retries=30
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ local reenable_x=false
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ [[ -o xtrace ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ set +x
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: About to run 'systemctl show hadoop-hdfs-datanode.service -p Restart,RemainAfterExit' with retries...
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: 'systemctl show google-fluentd.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ return 0
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + props='Restart=no
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: RemainAfterExit=yes'
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ google-fluentd != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ google-fluentd != \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ Restart=no
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: RemainAfterExit=yes == *\R\e\s\t\a\r\t\=\n\o* ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ Restart=no
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: RemainAfterExit=yes == *\R\e\m\a\i\n\A\f\t\e\r\E\x\i\t\=\n\o* ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ google-fluentd == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\n\o\d\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local am_on_primary_worker_enabled
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ get_dataproc_property am.primary_only
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ set +x
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: 'systemctl show hadoop-hdfs-datanode.service -p Restart,RemainAfterExit' succeeded after 1 execution(s).
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ return 0
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + props='Restart=no
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: RemainAfterExit=no'
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + [[ hadoop-hdfs-datanode != \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local -r gate_start_on_agent_success_drop_in=/etc/systemd/system/common/agent-gate.conf
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + [[ ! -f /etc/systemd/system/common/agent-gate.conf ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + [[ hadoop-hdfs-datanode == \h\a\d\o\o\p\-\h\d\f\s\-\d\a\t\a\n\o\d\e ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ get_metadata_worker_count
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ get_dataproc_metadata DATAPROC_METADATA_WORKER_COUNT attributes/dataproc-worker-count
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ set +x
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + am_on_primary_worker_enabled=false
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local master_run_driver_location
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: ++ /usr/share/google/get_metadata_value attributes/master-run-driver-location
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local -r worker_count=5
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + [[ 5 != 0 ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + ln -s -f /etc/systemd/system/common/agent-gate.conf /etc/systemd/system/hadoop-hdfs-datanode.service.d
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + master_run_driver_location=LOCAL
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ google-fluentd == \h\a\d\o\o\p\-\y\a\r\n\-\r\e\s\o\u\r\c\e\m\a\n\a\g\e\r ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + retry_constant systemctl start google-fluentd
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + ln -s -f /etc/systemd/system/common/worker-restart.conf /etc/systemd/system/hadoop-hdfs-datanode.service.d
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + retry_constant_custom 300 1 systemctl start google-fluentd
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local -r max_retry_time=300
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local -r retry_delay=1
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + cmd=("${@:3}")
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local -r cmd
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local -r max_retries=300
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: ++ date +%s.%N
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + local reenable_x=false
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + [[ -o xtrace ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: + set +x
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-google-fluentd[3404]: About to run 'systemctl start google-fluentd' with retries...
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local -r end=1687968166.352560633
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local -r runtime_s=3
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + echo 'Component hdfs took 3s to activate'
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: Component hdfs took 3s to activate
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + local -r time_file=/tmp/dataproc/components/activate/hdfs.time
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + touch /tmp/dataproc/components/activate/hdfs.time
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + cat
<13>Jun 28 16:02:46 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-stackdriver-agent[3041]: 'systemctl start stackdriver-agent' succeeded after 1 execution(s).
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 setup-stackdriver-agent[3041]: + return 0
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + [[ 0 -ne 0 ]]
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + echo 'Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh'
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: Done running component activation script: /usr/local/share/google/dataproc/bdutil/components/activate/hdfs.sh
<13>Jun 28 16:02:46 startup-script[1131]: <13>Jun 28 16:02:46 activate-component-hdfs[3043]: + touch /tmp/dataproc/components/activate/hdfs.done
<13>Jun 28 16:02:46 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3041.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[setup_service stackdriver-agent] pid=3041 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3041.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[setup_service stackdriver-agent] pid=3041 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3041.exitcode /tmp/dataproc/commands/3041.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3043
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3043
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component hdfs'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3043 cmd=[activate_component hdfs]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3043 cmd=[activate_component hdfs]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3043 cmd=[activate_component hdfs]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component hdfs'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3043.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3043.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component hdfs] pid=3043 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3043.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component hdfs] pid=3043 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3043.exitcode /tmp/dataproc/commands/3043.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3044
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3044
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component hive-metastore'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3044 cmd=[activate_component hive-metastore]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3044 cmd=[activate_component hive-metastore]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3044 cmd=[activate_component hive-metastore]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component hive-metastore'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3044.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3044.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component hive-metastore] pid=3044 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3044.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component hive-metastore] pid=3044 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3044.exitcode /tmp/dataproc/commands/3044.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3045
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3045
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component hive-server2'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3045 cmd=[activate_component hive-server2]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3045 cmd=[activate_component hive-server2]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3045 cmd=[activate_component hive-server2]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component hive-server2'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3045.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3045.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component hive-server2] pid=3045 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3045.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component hive-server2] pid=3045 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3045.exitcode /tmp/dataproc/commands/3045.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3046
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3046
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component mapreduce'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3046 cmd=[activate_component mapreduce]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3046 cmd=[activate_component mapreduce]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3046 cmd=[activate_component mapreduce]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component mapreduce'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3046.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3046.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component mapreduce] pid=3046 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3046.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component mapreduce] pid=3046 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3046.exitcode /tmp/dataproc/commands/3046.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3047
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3047
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component miniconda3'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3047 cmd=[activate_component miniconda3]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3047 cmd=[activate_component miniconda3]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3047 cmd=[activate_component miniconda3]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component miniconda3'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3047.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3047.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3047.done
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component miniconda3] pid=3047 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component miniconda3] pid=3047 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3047.exitcode /tmp/dataproc/commands/3047.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3048
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3048
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component mysql'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3048 cmd=[activate_component mysql]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3048 cmd=[activate_component mysql]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3048 cmd=[activate_component mysql]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component mysql'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3048.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3048.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3048.done
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component mysql] pid=3048 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component mysql] pid=3048 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3048.exitcode /tmp/dataproc/commands/3048.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3049
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3049
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component pig'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3049 cmd=[activate_component pig]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3049 cmd=[activate_component pig]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3049 cmd=[activate_component pig]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component pig'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3049.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3049.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component pig] pid=3049 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3049.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component pig] pid=3049 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3049.exitcode /tmp/dataproc/commands/3049.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3050
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3050
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component spark'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3050 cmd=[activate_component spark]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3050 cmd=[activate_component spark]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3050 cmd=[activate_component spark]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component spark'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3050.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3050.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component spark] pid=3050 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3050.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component spark] pid=3050 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3050.exitcode /tmp/dataproc/commands/3050.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3051
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3051
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component tez'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3051 cmd=[activate_component tez]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3051 cmd=[activate_component tez]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3051 cmd=[activate_component tez]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component tez'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3051.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3051.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3051.done
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component tez] pid=3051 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component tez] pid=3051 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3051.exitcode /tmp/dataproc/commands/3051.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3052
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3052
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='activate_component yarn'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3052 cmd=[activate_component yarn]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3052 cmd=[activate_component yarn]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3052 cmd=[activate_component yarn]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'activate_component yarn'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3052.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3052.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[activate_component yarn] pid=3052 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3052.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[activate_component yarn] pid=3052 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3052.exitcode /tmp/dataproc/commands/3052.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3269
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3269
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='enable_and_start_service google-osconfig-agent'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3269 cmd=[enable_and_start_service google-osconfig-agent]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3269 cmd=[enable_and_start_service google-osconfig-agent]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3269 cmd=[enable_and_start_service google-osconfig-agent]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'enable_and_start_service google-osconfig-agent'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3269.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3269.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + local status
<13>Jun 28 16:02:47 startup-script[1131]: + status=0
<13>Jun 28 16:02:47 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Command cmd=[enable_and_start_service google-osconfig-agent] pid=3269 exited with 0'
<13>Jun 28 16:02:47 startup-script[1131]: + tee /tmp/dataproc/commands/3269.done
<13>Jun 28 16:02:47 startup-script[1131]: Command cmd=[enable_and_start_service google-osconfig-agent] pid=3269 exited with 0
<13>Jun 28 16:02:47 startup-script[1131]: + rm /tmp/dataproc/commands/3269.exitcode /tmp/dataproc/commands/3269.running
<13>Jun 28 16:02:47 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:47 startup-script[1131]: + local pid
<13>Jun 28 16:02:47 startup-script[1131]: ++ basename /tmp/dataproc/commands/3404
<13>Jun 28 16:02:47 startup-script[1131]: + pid=3404
<13>Jun 28 16:02:47 startup-script[1131]: + local cmd
<13>Jun 28 16:02:47 startup-script[1131]: + cmd='setup_service google-fluentd'
<13>Jun 28 16:02:47 startup-script[1131]: + loginfo 'Waiting on pid=3404 cmd=[setup_service google-fluentd]'
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'Waiting on pid=3404 cmd=[setup_service google-fluentd]'
<13>Jun 28 16:02:47 startup-script[1131]: Waiting on pid=3404 cmd=[setup_service google-fluentd]
<13>Jun 28 16:02:47 startup-script[1131]: + echo 'setup_service google-fluentd'
<13>Jun 28 16:02:47 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3404.exitcode
<13>Jun 28 16:02:47 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3404.exitcode ]]
<13>Jun 28 16:02:47 startup-script[1131]: + sleep 1
<13>Jun 28 16:02:48 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3404.exitcode ]]
<13>Jun 28 16:02:48 startup-script[1131]: + sleep 1
<13>Jun 28 16:02:49 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3404.exitcode ]]
<13>Jun 28 16:02:49 startup-script[1131]: + sleep 1
<13>Jun 28 16:02:50 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3404.exitcode ]]
<13>Jun 28 16:02:50 startup-script[1131]: + sleep 1
<13>Jun 28 16:02:50 startup-script[1131]: <13>Jun 28 16:02:50 setup-google-fluentd[3404]: 'systemctl start google-fluentd' succeeded after 1 execution(s).
<13>Jun 28 16:02:50 startup-script[1131]: <13>Jun 28 16:02:50 setup-google-fluentd[3404]: + return 0
<13>Jun 28 16:02:50 startup-script[1131]: ++ echo 0
<13>Jun 28 16:02:51 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3404.exitcode ]]
<13>Jun 28 16:02:51 startup-script[1131]: + local status
<13>Jun 28 16:02:51 startup-script[1131]: + status=0
<13>Jun 28 16:02:51 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:51 startup-script[1131]: + tee /tmp/dataproc/commands/3404.done
<13>Jun 28 16:02:51 startup-script[1131]: + echo 'Command cmd=[setup_service google-fluentd] pid=3404 exited with 0'
<13>Jun 28 16:02:51 startup-script[1131]: Command cmd=[setup_service google-fluentd] pid=3404 exited with 0
<13>Jun 28 16:02:51 startup-script[1131]: + rm /tmp/dataproc/commands/3404.exitcode /tmp/dataproc/commands/3404.running
<13>Jun 28 16:02:51 startup-script[1131]: + for running_file in "${COMMANDS_TMP_DIR}/"*'.running'
<13>Jun 28 16:02:51 startup-script[1131]: + local pid
<13>Jun 28 16:02:51 startup-script[1131]: ++ basename /tmp/dataproc/commands/3956
<13>Jun 28 16:02:51 startup-script[1131]: + pid=3956
<13>Jun 28 16:02:51 startup-script[1131]: + local cmd
<13>Jun 28 16:02:51 startup-script[1131]: + cmd=backup_original_configs
<13>Jun 28 16:02:51 startup-script[1131]: + loginfo 'Waiting on pid=3956 cmd=[backup_original_configs]'
<13>Jun 28 16:02:51 startup-script[1131]: + echo 'Waiting on pid=3956 cmd=[backup_original_configs]'
<13>Jun 28 16:02:51 startup-script[1131]: Waiting on pid=3956 cmd=[backup_original_configs]
<13>Jun 28 16:02:51 startup-script[1131]: + echo backup_original_configs
<13>Jun 28 16:02:51 startup-script[1131]: + local exitcode_file=/tmp/dataproc/commands/3956.exitcode
<13>Jun 28 16:02:51 startup-script[1131]: + [[ ! -f /tmp/dataproc/commands/3956.exitcode ]]
<13>Jun 28 16:02:51 startup-script[1131]: + local status
<13>Jun 28 16:02:51 startup-script[1131]: + status=0
<13>Jun 28 16:02:51 startup-script[1131]: + (( status != 0 ))
<13>Jun 28 16:02:51 startup-script[1131]: + tee /tmp/dataproc/commands/3956.done
<13>Jun 28 16:02:51 startup-script[1131]: + echo 'Command cmd=[backup_original_configs] pid=3956 exited with 0'
<13>Jun 28 16:02:51 startup-script[1131]: Command cmd=[backup_original_configs] pid=3956 exited with 0
<13>Jun 28 16:02:51 startup-script[1131]: + rm /tmp/dataproc/commands/3956.exitcode /tmp/dataproc/commands/3956.running
<13>Jun 28 16:02:51 startup-script[1131]: + is_ubuntu
<13>Jun 28 16:02:51 startup-script[1131]: ++ os_id
<13>Jun 28 16:02:51 startup-script[1131]: ++ cut -d= -f2
<13>Jun 28 16:02:51 startup-script[1131]: ++ xargs
<13>Jun 28 16:02:51 startup-script[1131]: ++ grep '^ID=' /etc/os-release
<13>Jun 28 16:02:51 startup-script[1131]: + [[ debian == \u\b\u\n\t\u ]]
<13>Jun 28 16:02:51 startup-script[1131]: + loginfo 'All done'
<13>Jun 28 16:02:51 startup-script[1131]: + echo 'All done'
<13>Jun 28 16:02:51 startup-script[1131]: All done
